{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ACKNOWLEDGEMENTS Special thanks to Callum Jackson and Dennis Woo for adaptation of the Using event automation to create Kafka streams from IBM MQ platinum demo, part of the IBM Cloud Pak for Integration collection for IBM Technology Zone. Christopher Bienko (Principal, Learning Content Development, Application Modernization) introduces the objectives and curriculum for IBM Event Automation for Technical Sales Level 3 . Additional ways to watch: ( 2 minute runtime ). Integration that weaves together both applications and systems is essential to the success of a modern, digital enterprise. In 2024 and beyond, the enterprise organizations that will outpace and outcompete their marketplace will be those who are the most responsive, agile, and adaptable to an ever-shifting and increasingly-connected IT landscape. The success of these organizations will depend on their ability to securely connect, automate, and transform their business at scale \u2014 made possible through the integration of real-time messaging, events, and processes. Conceptually, it's useful to examine these requirements across three domain areas (which so happen to map to the three pillars of the IBM Integration solution portfolio): Event-led integration : For enterprise to discover what is happening within in their business and quickly respond to emergent events. API-led integration : For enterprise that needs to access and share information (both securely and rapidly.) Messaging and connectivity : For enterprise to securely and reliable access or update Systems of Record (SoR), at scale. Organizations of every size, across all industries and verticals, face a shared set of challenges in their adoption of event-driven architectures and enterprise-wide integration. Enterprise organizations (and many IBM clients) are at different stages in their event-driven journey. For some, they have only recently embarked on scoping and putting into place an event messaging distribution layer. Others will have already cemented the the foundations for an Apache Kafka-based event distribution layer, and are looking to enrich that with capabilities that boost security, enable governed self-service access, and incorporate event processing. Despite the range of industries and use cases that must confront the challenges of event-driven architectures and integration, there are four common themes that regularly emerge in conversations with prospective clients. Real-time access for meeting customer demands and taking action in the moment. Event-driven enterprises must be optimized, timely, adaptable, and proactive. Event-driven integration and automation provides continuous awareness for a business about the state of their systems and services. It drives automation that responds to events in any given situation and enables a more adaptable approach to situations evolving in real-time. EVENT-LED INTEGRATION SOLUTIONS FROM IBM IBM Event Automation puts business events to work by making it possible for users to detect unfolding situations and making automated decisions in response in real-time. The offering consists of three core, composable capabilities: Event Streams : \"Event distribution.\" Collect real-time business events data streams using an enterprise-grade Apache Kafka distribution. Event sources can include Kafka Connect (MQ, database), REST APIs, and direct integrations (IBM App Connect, Change Data Capture, and more). Event Endpoint Management : \"Event discovery.\" Build a self-service catalog of event sources for other users within the business to securely browse and leverage. Event Processing : \"Event processing.\" Define business situations requiring automated responses ahead of time, using intuitive authoring canvas, that can be invoked in real-time to automate decision making and take decisive actions. To take raw events and make them contextually relevant to business users, Event Processing can: filter and transform event properties; combine events to identify novel patterns over continuous time windows; and aggregate events to analyze trends or detect anomalies. Once clients have set up a distribution layer and they\u2019ve started to share events around the enterprise, they want to work with those events to understand their relevance. Identifying and acting on these situations in the moment. Multiplicity of data sources, all of which a business may need access to. The ever-increasing number of data sources that an enterprise organization connects to provides an expanding surface area for malicious agents to exploit. Moreover, maintaining and managing these data sources becomes increasingly complex as more endpoints are connected to an enterprise's IT estate. + Secure access across the organization. Every enterprise has a plethora of unique APIs that are continuously updating. APIs are a specific vector that attackers can exploit for gaining unauthorized access to systems and data. Two-thirds of cloud incidents are due to misconfigured API endpoints. API-LED INTEGRATION SOLUTIONS FROM IBM The combined challenges of managing across a multiplicity of data sources and securing access to that data is addressed by the core competencies of IBM App Connect and IBM App Connect Enterprise . IBM App Connect allows clients to quickly build and test integration flows with frequently-used authoring tools. It securely connects with hundreds of cloud and on-premises applications via pre-built smart connectors and templates. App Connect's high performance transformation engine virtually supports any-to-any data formats (augmented with AI tooling to assist non-specialists.) It is used to easily deploy integrations with visibility into flow health and performance, for managing across hybrid cloud environments. IBM App Connect supports over 200 OOTB connectors and templates for jumpstarting integration. Automation Explorer provides a community for connectors and templates (explorer.automation.ibm.com). Clients can build their own bespoke systems connectors using an accelerated Connector Development Kit . Once connections have been established to the endpoints across an enterprise, that API-fed data needs to be shared (externally or internally) in order to be acted upon. From the moment these APIs are created, they must be managed and secured by a governance and control framework; it is here that API management plays a critical role. API-led integration is the cutting edge way to integrate applications and data by leveraging reusable REST APIs. Via IBM App Connect, the API-led integration approach brings together the disciplines of API management and API integration, across on-premises and cloud. First, clients build an integration flow for their APIs using an OpenAPI editor for authoring gateway policies that will govern OpenAPI documents. IBM's all-in-one flow editor provides a unified API authoring experience that brings together \"application integration\" and \"API management\" into a single tool. By contrast, other integration products require different tools, with differing experiences, to be performed by different users. IBM App Connect makes it possible to define an OpenAPI, build an integration flow, secure it with gateway policies, as well as manage and publish it to a Developer Portal for re-use \u2014 all within a unified editor. IBM App Connect's partnership with Noname Security helps identify undiscovered API exposure and API Connect's integration with IBM DataPower Gateway provides real-time threat and anomaly detection using AI. Clients can utilize StepZen to rapidly federate a variety of back-end data sources using highly responsive GraphQL APIs (declaratively defined, developer-friendly). Access to a diverse range of development and runtime execution platforms. Endpoints requiring access may include cloud-native application frameworks (containers and microservices), public cloud infrastructure, SaaS applications, software-defined or hyperconverged infrastructure configuration, on-premises implementations of public cloud services, or consumption-billed infrastructure resources. This is not an exhaustive list. MESSAGING AND CONNECTIVITY SOLUTIONS FROM IBM The challenges of integrating across a diverse range of messaging and connectivity endpoints is addressed by a range of IBM technologies: IBM Cloud Pak for Integration , IBM App Connect + API Connect software, and IBM App Connect + API Connect as a Service (on IBM Cloud.) IBM Cloud Pak for Integration runs atop Red Hat OpenShift: a vendor-agnostic, containerization platform that is truly hybrid cloud \u2014 deployable on nearly every on-premises and cloud infrastructure topology. The offering packages together: API management, application integration, end-to-end security, enterprise messaging, event streaming, and high speed data transfers. It provides a unified place for developers, lines of business, and IT operators to catalog, document, and manage services + events through a single portal. Likewise, all of these services are bundled and licensed together; there is no need to decide what capabilities are required up-front and only pay for how much integration is deployed. \"Integration Assemblies\" simplify and automate the deployment of new integrations using the IBM Cloud Pak for Integration. Users can create and manage multiple integration instances as a single object. These instances can be immediately granted high availability (HA) if required. Declarative deployments reduce the complexity of manual integration tasks and rapidly increase the rate at which a business can make new API deployments. \"Operator-deployed container images\" take advantage of Red Hat OpenShift's powerful Operators, reducing the complexity of deploying containerized applications, updating images without disruption, and increasing the resiliency of live containers. Using IBM Event Automation to capitalize on time-sensitive revenue opportunities. Focus Corporation , a digital retail company engaged with IBM teams, is highly motivated to derive more revenue from first-time customers to their online storefronts. Focus' marketing department believes that by sending high-value promotions to new customers immediately after making a large purchase can significantly boost such revenue streams. The challenge will be finding a way to capitalize on these time-sensitive revenue opportunities using the real-time transaction data already available (but underutilized) to Focus' IT teams. Focus is currently using IBM MQ to exchange customer orders (placed online) between two endpoints: an order management system and the online payment gateway. As part of the hands-on work to be performed, IBM and Focus integration teams will need to tap into IBM MQ communications and publish each order as a Kafka-based \"event stream\". This will enable business-critical communications originating over MQ to be captured as events within Kafka topics, which can later be shared across the enterprise for fueling more responsive applications. Ultimately, Focus' marketing department will be able to use a self-service catalog for inspecting Order and New Customer topics as needed. From there, they will leverage no-code editors to build automated process flows that detect new customer orders totaling over $100 and issue high-value promotions to the customer's inbox immediately following the purchase. Next Steps In the following module, you will receive the necessary pre-requisites and setup instructions for getting started with IBM Event Automation and IBM Cloud Pak for Integration environments.","title":"Introduction"},{"location":"#_1","text":"","title":""},{"location":"#integration-that-weaves-together-both-applications-and-systems-is-essential-to-the-success-of-a-modern-digital-enterprise","text":"In 2024 and beyond, the enterprise organizations that will outpace and outcompete their marketplace will be those who are the most responsive, agile, and adaptable to an ever-shifting and increasingly-connected IT landscape. The success of these organizations will depend on their ability to securely connect, automate, and transform their business at scale \u2014 made possible through the integration of real-time messaging, events, and processes. Conceptually, it's useful to examine these requirements across three domain areas (which so happen to map to the three pillars of the IBM Integration solution portfolio): Event-led integration : For enterprise to discover what is happening within in their business and quickly respond to emergent events. API-led integration : For enterprise that needs to access and share information (both securely and rapidly.) Messaging and connectivity : For enterprise to securely and reliable access or update Systems of Record (SoR), at scale.","title":"Integration that weaves together both applications and systems is essential to the success of a modern, digital enterprise."},{"location":"#_2","text":"","title":""},{"location":"#organizations-of-every-size-across-all-industries-and-verticals-face-a-shared-set-of-challenges-in-their-adoption-of-event-driven-architectures-and-enterprise-wide-integration","text":"Enterprise organizations (and many IBM clients) are at different stages in their event-driven journey. For some, they have only recently embarked on scoping and putting into place an event messaging distribution layer. Others will have already cemented the the foundations for an Apache Kafka-based event distribution layer, and are looking to enrich that with capabilities that boost security, enable governed self-service access, and incorporate event processing. Despite the range of industries and use cases that must confront the challenges of event-driven architectures and integration, there are four common themes that regularly emerge in conversations with prospective clients. Real-time access for meeting customer demands and taking action in the moment. Event-driven enterprises must be optimized, timely, adaptable, and proactive. Event-driven integration and automation provides continuous awareness for a business about the state of their systems and services. It drives automation that responds to events in any given situation and enables a more adaptable approach to situations evolving in real-time. EVENT-LED INTEGRATION SOLUTIONS FROM IBM IBM Event Automation puts business events to work by making it possible for users to detect unfolding situations and making automated decisions in response in real-time. The offering consists of three core, composable capabilities: Event Streams : \"Event distribution.\" Collect real-time business events data streams using an enterprise-grade Apache Kafka distribution. Event sources can include Kafka Connect (MQ, database), REST APIs, and direct integrations (IBM App Connect, Change Data Capture, and more). Event Endpoint Management : \"Event discovery.\" Build a self-service catalog of event sources for other users within the business to securely browse and leverage. Event Processing : \"Event processing.\" Define business situations requiring automated responses ahead of time, using intuitive authoring canvas, that can be invoked in real-time to automate decision making and take decisive actions. To take raw events and make them contextually relevant to business users, Event Processing can: filter and transform event properties; combine events to identify novel patterns over continuous time windows; and aggregate events to analyze trends or detect anomalies. Once clients have set up a distribution layer and they\u2019ve started to share events around the enterprise, they want to work with those events to understand their relevance. Identifying and acting on these situations in the moment. Multiplicity of data sources, all of which a business may need access to. The ever-increasing number of data sources that an enterprise organization connects to provides an expanding surface area for malicious agents to exploit. Moreover, maintaining and managing these data sources becomes increasingly complex as more endpoints are connected to an enterprise's IT estate. + Secure access across the organization. Every enterprise has a plethora of unique APIs that are continuously updating. APIs are a specific vector that attackers can exploit for gaining unauthorized access to systems and data. Two-thirds of cloud incidents are due to misconfigured API endpoints. API-LED INTEGRATION SOLUTIONS FROM IBM The combined challenges of managing across a multiplicity of data sources and securing access to that data is addressed by the core competencies of IBM App Connect and IBM App Connect Enterprise . IBM App Connect allows clients to quickly build and test integration flows with frequently-used authoring tools. It securely connects with hundreds of cloud and on-premises applications via pre-built smart connectors and templates. App Connect's high performance transformation engine virtually supports any-to-any data formats (augmented with AI tooling to assist non-specialists.) It is used to easily deploy integrations with visibility into flow health and performance, for managing across hybrid cloud environments. IBM App Connect supports over 200 OOTB connectors and templates for jumpstarting integration. Automation Explorer provides a community for connectors and templates (explorer.automation.ibm.com). Clients can build their own bespoke systems connectors using an accelerated Connector Development Kit . Once connections have been established to the endpoints across an enterprise, that API-fed data needs to be shared (externally or internally) in order to be acted upon. From the moment these APIs are created, they must be managed and secured by a governance and control framework; it is here that API management plays a critical role. API-led integration is the cutting edge way to integrate applications and data by leveraging reusable REST APIs. Via IBM App Connect, the API-led integration approach brings together the disciplines of API management and API integration, across on-premises and cloud. First, clients build an integration flow for their APIs using an OpenAPI editor for authoring gateway policies that will govern OpenAPI documents. IBM's all-in-one flow editor provides a unified API authoring experience that brings together \"application integration\" and \"API management\" into a single tool. By contrast, other integration products require different tools, with differing experiences, to be performed by different users. IBM App Connect makes it possible to define an OpenAPI, build an integration flow, secure it with gateway policies, as well as manage and publish it to a Developer Portal for re-use \u2014 all within a unified editor. IBM App Connect's partnership with Noname Security helps identify undiscovered API exposure and API Connect's integration with IBM DataPower Gateway provides real-time threat and anomaly detection using AI. Clients can utilize StepZen to rapidly federate a variety of back-end data sources using highly responsive GraphQL APIs (declaratively defined, developer-friendly). Access to a diverse range of development and runtime execution platforms. Endpoints requiring access may include cloud-native application frameworks (containers and microservices), public cloud infrastructure, SaaS applications, software-defined or hyperconverged infrastructure configuration, on-premises implementations of public cloud services, or consumption-billed infrastructure resources. This is not an exhaustive list. MESSAGING AND CONNECTIVITY SOLUTIONS FROM IBM The challenges of integrating across a diverse range of messaging and connectivity endpoints is addressed by a range of IBM technologies: IBM Cloud Pak for Integration , IBM App Connect + API Connect software, and IBM App Connect + API Connect as a Service (on IBM Cloud.) IBM Cloud Pak for Integration runs atop Red Hat OpenShift: a vendor-agnostic, containerization platform that is truly hybrid cloud \u2014 deployable on nearly every on-premises and cloud infrastructure topology. The offering packages together: API management, application integration, end-to-end security, enterprise messaging, event streaming, and high speed data transfers. It provides a unified place for developers, lines of business, and IT operators to catalog, document, and manage services + events through a single portal. Likewise, all of these services are bundled and licensed together; there is no need to decide what capabilities are required up-front and only pay for how much integration is deployed. \"Integration Assemblies\" simplify and automate the deployment of new integrations using the IBM Cloud Pak for Integration. Users can create and manage multiple integration instances as a single object. These instances can be immediately granted high availability (HA) if required. Declarative deployments reduce the complexity of manual integration tasks and rapidly increase the rate at which a business can make new API deployments. \"Operator-deployed container images\" take advantage of Red Hat OpenShift's powerful Operators, reducing the complexity of deploying containerized applications, updating images without disruption, and increasing the resiliency of live containers.","title":"Organizations of every size, across all industries and verticals, face a shared set of challenges in their adoption of event-driven architectures and enterprise-wide integration."},{"location":"#_3","text":"","title":""},{"location":"#using-ibm-event-automation-to-capitalize-on-time-sensitive-revenue-opportunities","text":"Focus Corporation , a digital retail company engaged with IBM teams, is highly motivated to derive more revenue from first-time customers to their online storefronts. Focus' marketing department believes that by sending high-value promotions to new customers immediately after making a large purchase can significantly boost such revenue streams. The challenge will be finding a way to capitalize on these time-sensitive revenue opportunities using the real-time transaction data already available (but underutilized) to Focus' IT teams. Focus is currently using IBM MQ to exchange customer orders (placed online) between two endpoints: an order management system and the online payment gateway. As part of the hands-on work to be performed, IBM and Focus integration teams will need to tap into IBM MQ communications and publish each order as a Kafka-based \"event stream\". This will enable business-critical communications originating over MQ to be captured as events within Kafka topics, which can later be shared across the enterprise for fueling more responsive applications. Ultimately, Focus' marketing department will be able to use a self-service catalog for inspecting Order and New Customer topics as needed. From there, they will leverage no-code editors to build automated process flows that detect new customer orders totaling over $100 and issue high-value promotions to the customer's inbox immediately following the purchase.","title":"Using IBM Event Automation to capitalize on time-sensitive revenue opportunities."},{"location":"#_4","text":"","title":""},{"location":"#next-steps","text":"In the following module, you will receive the necessary pre-requisites and setup instructions for getting started with IBM Event Automation and IBM Cloud Pak for Integration environments.","title":"Next Steps"},{"location":"1/","text":"Creating an event stream from an IBM MQ message stream To take advantage of the message queue data available from IBM MQ, Focus Corporation's integration team will need to make IBM MQ's enterprise data available to a broader set of APIs. This will allow Focus' application developers to subscribe to the data without any risk or impact to the core back-end systems supporting the data: risks can be lowered and the development process can be decoupled from data retention processes (accelerating application development.) To do so, Focus' integration team will need to expose the IBM MQ enterprise data using \"event streams.\" Specifically, the integration team (and application developers) will need access to customer order information. This data will be vital for the marketing team's ambitions of rapidly offering high value promotions to newly-acquired customers. Both the order management system and its payment gateway system are currently using IBM MQ message queues for exchanging customer order data. The integration team (and you) will begin the hands-on work by tapping into this message exchange, clone each of the orders that is handled across IBM MQ, and then publish those messages into a new event stream. Configuring IBM MQ to clone customer order data Open the IBM MQ dashboard with a web browser. From the left-hand side interface, drill down into the Manage tab. Select the Queues tab along the top of the page. Scroll down the page until you reach a table, which at this time should show several queues that are already a part of the environment. Create a new queue by clicking the blue Create + icon in the top-right corner of the table. Wait for the Queue creation wizard to load, then select Local from the Choose queue type options. The Quick create option should be selected by default. Under the Queue name field, enter TO.KAFKA (all uppercase). When ready, click Create . The web browser will refresh back to the Manage > Queues perspective. From the table at the bottom of the page, confirm that the TO.KAFKA queue is now available. Next, you need to configure IBM MQ to clone all customer order data that it is exchanging between Focus Corporation's order management system and payment gateway. Messages must be reviewed to verify that they contain the expected payload information. From the Manage > Queues table, locate the queue named PAYMENT.REQ and click the Name to inspect it in more detail. The queue will already have been populated with multiple Messages . Click on any of the Timestamp names available in the table (something similar to Feb 21, 2024 at 5:12:00 PM ) to pull open a Message Details panel. Scroll down within the Message Details panel until you reach the Application Data section. Inspect the contents of the packet, which is a series of key-value pairs. You should see details about the order id , customer , customerid , description , price , quantity , region , and ordertime . Close the Message Details panel by clicking the X in the top-right corner or the grey Close button. Back on the PAYMENT.REQ orders summary table, locate the Actions button in the top-right corner of the page. Click to open a drop-down menu and then select View configuration . Multiple attributes of the PAYMENT.REQ order queue can be configured from this page. Click the grey Edit button to the right side of the General page. From the tabs on the left side of the page, drill down into Storage . Scroll down until you reach the Streaming queue name field and change the value to TO.KAFKA . This will direct IBM MQ to clone messages from the PAYMENT.QUE queue into the TO.KAFKA streaming queue created in Step 5. When satisfied, click the blue Save button in the top-right of the page to confirm the configuration changes. Once confirmed, the new configuration will immediately take effect and cloned order messages will promptly begin filling the TO.KAFKA queue. Confirm this activity is ongoing by returning to the Manage page. From the table of queues, drill down into the TO.KAFKA queue. Take note of the customer order messages that are now populating the TO.KAFKA message queue table. You can click the circular refresh icon (to the left of the blue Create button) to reload the queue contents. Creating an Orders event stream using IBM Event Streams Open the IBM Event Streams tab using your web browser. Focus Corporation's integration team will now need to create an event stream called Orders using IBM Event Streams . This will serve as the repository where messages, cloned from IBM MQ, are published and made available to other parts of the organization. The team will need to make decisions about how to replicate the data (with or without modification) and also determine the appropriate retention settings for this data. Given the governance policies in place at Focus Corporation, they will need to retain data for up to 1 week and replicate entries for high availability. From the IBM Event Streams dashboard, click the Create a topic tile. The team must first decide on a Topic Name . Set the value to ORDERS and then click the blue Next button to continue. Under the Patitions tab, accept the default recommendation of 1 by clicking Next . Under the Message Retention tab, accept the default recommendation of A week by clicking Next . Under the Replicas tab, accept the default recommendation of Replication factor: 3 . Confirm your selections by clicking the Create Topic button. Configuring a bridge between IBM MQ and IBM Event Streams","title":"1. Creating an event stream from IBM MQ message queues"},{"location":"1/#creating-an-event-stream-from-an-ibm-mq-message-stream","text":"To take advantage of the message queue data available from IBM MQ, Focus Corporation's integration team will need to make IBM MQ's enterprise data available to a broader set of APIs. This will allow Focus' application developers to subscribe to the data without any risk or impact to the core back-end systems supporting the data: risks can be lowered and the development process can be decoupled from data retention processes (accelerating application development.) To do so, Focus' integration team will need to expose the IBM MQ enterprise data using \"event streams.\" Specifically, the integration team (and application developers) will need access to customer order information. This data will be vital for the marketing team's ambitions of rapidly offering high value promotions to newly-acquired customers. Both the order management system and its payment gateway system are currently using IBM MQ message queues for exchanging customer order data. The integration team (and you) will begin the hands-on work by tapping into this message exchange, clone each of the orders that is handled across IBM MQ, and then publish those messages into a new event stream.","title":"Creating an event stream from an IBM MQ message stream"},{"location":"1/#_1","text":"","title":""},{"location":"1/#configuring-ibm-mq-to-clone-customer-order-data","text":"Open the IBM MQ dashboard with a web browser. From the left-hand side interface, drill down into the Manage tab. Select the Queues tab along the top of the page. Scroll down the page until you reach a table, which at this time should show several queues that are already a part of the environment. Create a new queue by clicking the blue Create + icon in the top-right corner of the table. Wait for the Queue creation wizard to load, then select Local from the Choose queue type options. The Quick create option should be selected by default. Under the Queue name field, enter TO.KAFKA (all uppercase). When ready, click Create . The web browser will refresh back to the Manage > Queues perspective. From the table at the bottom of the page, confirm that the TO.KAFKA queue is now available. Next, you need to configure IBM MQ to clone all customer order data that it is exchanging between Focus Corporation's order management system and payment gateway. Messages must be reviewed to verify that they contain the expected payload information. From the Manage > Queues table, locate the queue named PAYMENT.REQ and click the Name to inspect it in more detail. The queue will already have been populated with multiple Messages . Click on any of the Timestamp names available in the table (something similar to Feb 21, 2024 at 5:12:00 PM ) to pull open a Message Details panel. Scroll down within the Message Details panel until you reach the Application Data section. Inspect the contents of the packet, which is a series of key-value pairs. You should see details about the order id , customer , customerid , description , price , quantity , region , and ordertime . Close the Message Details panel by clicking the X in the top-right corner or the grey Close button. Back on the PAYMENT.REQ orders summary table, locate the Actions button in the top-right corner of the page. Click to open a drop-down menu and then select View configuration . Multiple attributes of the PAYMENT.REQ order queue can be configured from this page. Click the grey Edit button to the right side of the General page. From the tabs on the left side of the page, drill down into Storage . Scroll down until you reach the Streaming queue name field and change the value to TO.KAFKA . This will direct IBM MQ to clone messages from the PAYMENT.QUE queue into the TO.KAFKA streaming queue created in Step 5. When satisfied, click the blue Save button in the top-right of the page to confirm the configuration changes. Once confirmed, the new configuration will immediately take effect and cloned order messages will promptly begin filling the TO.KAFKA queue. Confirm this activity is ongoing by returning to the Manage page. From the table of queues, drill down into the TO.KAFKA queue. Take note of the customer order messages that are now populating the TO.KAFKA message queue table. You can click the circular refresh icon (to the left of the blue Create button) to reload the queue contents.","title":"Configuring IBM MQ to clone customer order data"},{"location":"1/#_2","text":"","title":""},{"location":"1/#creating-an-orders-event-stream-using-ibm-event-streams","text":"Open the IBM Event Streams tab using your web browser. Focus Corporation's integration team will now need to create an event stream called Orders using IBM Event Streams . This will serve as the repository where messages, cloned from IBM MQ, are published and made available to other parts of the organization. The team will need to make decisions about how to replicate the data (with or without modification) and also determine the appropriate retention settings for this data. Given the governance policies in place at Focus Corporation, they will need to retain data for up to 1 week and replicate entries for high availability. From the IBM Event Streams dashboard, click the Create a topic tile. The team must first decide on a Topic Name . Set the value to ORDERS and then click the blue Next button to continue. Under the Patitions tab, accept the default recommendation of 1 by clicking Next . Under the Message Retention tab, accept the default recommendation of A week by clicking Next . Under the Replicas tab, accept the default recommendation of Replication factor: 3 . Confirm your selections by clicking the Create Topic button.","title":"Creating an Orders event stream using IBM Event Streams"},{"location":"1/#_3","text":"","title":""},{"location":"1/#configuring-a-bridge-between-ibm-mq-and-ibm-event-streams","text":"","title":"Configuring a bridge between IBM MQ and IBM Event Streams"},{"location":"2/","text":"Browsing the self-service IBM Event Streams catalog","title":"2. Browsing the IBM Event Streams self-service catalog"},{"location":"2/#browsing-the-self-service-ibm-event-streams-catalog","text":"","title":"Browsing the self-service IBM Event Streams catalog"},{"location":"3/","text":"Using the IBM Event Processing no-code editor to configure the solution","title":"3. Configuring the solution with IBM Event Processing editor"},{"location":"3/#using-the-ibm-event-processing-no-code-editor-to-configure-the-solution","text":"","title":"Using the IBM Event Processing no-code editor to configure the solution"},{"location":"4/","text":"Connecting the event stream output to the marketing application","title":"4. Connecting event streams to the marketing application"},{"location":"4/#connecting-the-event-stream-output-to-the-marketing-application","text":"","title":"Connecting the event stream output to the marketing application"},{"location":"evaluation/","text":"Evaluation Criteria for IBM Technical Sellers and Business Partners The series of hands-on tutorials and learning modules embedded in this Level 3 course are designed to provide IBM Technical Sellers and Business Partners with the fluency to gain trusted advisor status with clients and the expertise to tailor live technological demonstrations for customers. The hands-on lab will examine a day in the life of Amy, a Chief Innovation Officer (CIO), and her team as they embark on a digital transformation of Stacked Corporation 's business. Their work and expertise encompass everything from application developers, to IT administrators, to data scientists. As such, DevOps and IT Ops will be a heavy focus of this course\u2014 with security, observability, and automation topics to follow in future hands-on learning. Through the lens of a Chief Innovation Officer, you will: Module A : Enable continuous deployment via Red Hat OpenShift S2I (Source-2-Image) and GitHub webhooks Module B : Graphically (using the OpenShift web dashboard) create, scale, upgrade, and rollback an application with Red Hat OpenShift Module C : Programmatically (using a built-in command line interface) create, scale, upgrade, and rollback an application with Red Hat OpenShift Each module of this course and hands-on lab work should take an estimated 60 minutes to complete (or less depending on your technical expediency). In total, the lab work should take you no more than 5 hours . To receive a Level 3 accreditation, IBMers and Business Partners must demonstrate mastery of the skills learned throughout the various modules of these hands-on labs and coursework. Level 3 accreditation requirements\u2014 and the way participants will be evaluated before receiving accreditation \u2014differs depending on job role. IBM TECHNICAL SELLERS IBM Sales and Tech Sales must develop and record a Stand & Deliver presentation, which will be uploaded to the IBM Stand & Deliver platform for evaluation. This video is intended to simulate your delivery of a \u201clive\u201d demo in front of a client \u2014 on camera. IBMers will have flexibility in defining a hypothetical client, the pain points that customer has, and the goals they aspire to. The recording will then cover the seller\u2019s hands-on demonstration and pitch to the client of the value of the IBM solution using the environments and techniques of this lab. BUSINESS PARTNERS Business Partners must pass an accreditation quiz after completing the hands-on portion of the course. The quiz consists of multiple choice questions, with four possible responses (and only one correct answer) for each question. The quiz questions will ask you about on-screen text or descriptions that come up as you work through the lab guide. Answers to quiz questions can only be determined by carefully following the instructions of the hands-on lab. Sections of the lab marked as OPTIONAL will not be tested on as part of the quiz. You will have three attempts in total to obtain a passing grade. IBMer Stand & Deliver Assessment IBMers \u2014 SUBMIT RECORDINGS HERE Submit your Stand & Deliver recording online using IBM YourLearning: https://yourlearning.ibm.com/activity/ISND-SALESENABLEMENT-111810?planId=PLAN-66BD32D205CA&sectionId=SECTION-A&planIdFromParentTab=PLAN-66BD32D205CA&sectionIdFromParentTab=SECTION-A&planIdForChildTab=PLAN-66BD32D205CA The evaluation criteria described below only applies to IBMers , who must record a Stand & Deliver to receive accreditation for this Level 3 course. By default, IBMers will be evaluated by their First Line Manager (FLM) \u2014 although they may request another manager to evaluate their Stand & Deliver, if appropriate. Instructions on how to submit a Stand & Deliver are included on this page. Sellers must include each of the following in their recording: Seller articulated client's pain point(s) and the value proposition of using OpenShift on IBM Power Systems. Seller highlighted use cases for OpenShift on Power. Seller demonstrated and discussed several of the key differentiated capabilities of OpenShift on Power that deliver on the value proposition on point one. Seller highlighted benefits to the client (this is the why the client can\u2019t live without these benefits section). Seller highlighted benefits to the client's customers (what will the client be able to deliver to their customers that they could not without this product). Seller closed the demo with a call to action for the client that could include: a workshop, a deeper dive into the product meeting, MVP engagements, and so on. Business Partner Quiz Assessment PARTNERS \u2014 COMPLETE ASSESSMENT HERE Complete your assessment online using IBM Training: https://learn.ibm.com/mod/subcourse/view.php?id=217396 The accreditation quiz consists of multiple choice questions, with four possible responses (and only one correct answer) for each question. The quiz questions will ask you about on-screen text or descriptions that come up as you work through the lab guide. Answers to the quiz can only be determined by carefully following the instructions of the hands-on lab. Sections of the lab marked as OPTIONAL will not be tested on as part of the quiz. You will have three attempts in total to obtain a passing grade. Next Steps In the following section, you will prepare a hands-on lab environment with the necessary services and configurations.","title":"Evaluation Criteria for IBM Technical Sellers and Business Partners"},{"location":"evaluation/#evaluation-criteria-for-ibm-technical-sellers-and-business-partners","text":"The series of hands-on tutorials and learning modules embedded in this Level 3 course are designed to provide IBM Technical Sellers and Business Partners with the fluency to gain trusted advisor status with clients and the expertise to tailor live technological demonstrations for customers. The hands-on lab will examine a day in the life of Amy, a Chief Innovation Officer (CIO), and her team as they embark on a digital transformation of Stacked Corporation 's business. Their work and expertise encompass everything from application developers, to IT administrators, to data scientists. As such, DevOps and IT Ops will be a heavy focus of this course\u2014 with security, observability, and automation topics to follow in future hands-on learning. Through the lens of a Chief Innovation Officer, you will: Module A : Enable continuous deployment via Red Hat OpenShift S2I (Source-2-Image) and GitHub webhooks Module B : Graphically (using the OpenShift web dashboard) create, scale, upgrade, and rollback an application with Red Hat OpenShift Module C : Programmatically (using a built-in command line interface) create, scale, upgrade, and rollback an application with Red Hat OpenShift Each module of this course and hands-on lab work should take an estimated 60 minutes to complete (or less depending on your technical expediency). In total, the lab work should take you no more than 5 hours . To receive a Level 3 accreditation, IBMers and Business Partners must demonstrate mastery of the skills learned throughout the various modules of these hands-on labs and coursework. Level 3 accreditation requirements\u2014 and the way participants will be evaluated before receiving accreditation \u2014differs depending on job role. IBM TECHNICAL SELLERS IBM Sales and Tech Sales must develop and record a Stand & Deliver presentation, which will be uploaded to the IBM Stand & Deliver platform for evaluation. This video is intended to simulate your delivery of a \u201clive\u201d demo in front of a client \u2014 on camera. IBMers will have flexibility in defining a hypothetical client, the pain points that customer has, and the goals they aspire to. The recording will then cover the seller\u2019s hands-on demonstration and pitch to the client of the value of the IBM solution using the environments and techniques of this lab. BUSINESS PARTNERS Business Partners must pass an accreditation quiz after completing the hands-on portion of the course. The quiz consists of multiple choice questions, with four possible responses (and only one correct answer) for each question. The quiz questions will ask you about on-screen text or descriptions that come up as you work through the lab guide. Answers to quiz questions can only be determined by carefully following the instructions of the hands-on lab. Sections of the lab marked as OPTIONAL will not be tested on as part of the quiz. You will have three attempts in total to obtain a passing grade.","title":"Evaluation Criteria for IBM Technical Sellers and Business Partners"},{"location":"evaluation/#_1","text":"","title":""},{"location":"evaluation/#ibmer-stand-deliver-assessment","text":"IBMers \u2014 SUBMIT RECORDINGS HERE Submit your Stand & Deliver recording online using IBM YourLearning: https://yourlearning.ibm.com/activity/ISND-SALESENABLEMENT-111810?planId=PLAN-66BD32D205CA&sectionId=SECTION-A&planIdFromParentTab=PLAN-66BD32D205CA&sectionIdFromParentTab=SECTION-A&planIdForChildTab=PLAN-66BD32D205CA The evaluation criteria described below only applies to IBMers , who must record a Stand & Deliver to receive accreditation for this Level 3 course. By default, IBMers will be evaluated by their First Line Manager (FLM) \u2014 although they may request another manager to evaluate their Stand & Deliver, if appropriate. Instructions on how to submit a Stand & Deliver are included on this page. Sellers must include each of the following in their recording: Seller articulated client's pain point(s) and the value proposition of using OpenShift on IBM Power Systems. Seller highlighted use cases for OpenShift on Power. Seller demonstrated and discussed several of the key differentiated capabilities of OpenShift on Power that deliver on the value proposition on point one. Seller highlighted benefits to the client (this is the why the client can\u2019t live without these benefits section). Seller highlighted benefits to the client's customers (what will the client be able to deliver to their customers that they could not without this product). Seller closed the demo with a call to action for the client that could include: a workshop, a deeper dive into the product meeting, MVP engagements, and so on.","title":"IBMer Stand &amp; Deliver Assessment"},{"location":"evaluation/#_2","text":"","title":""},{"location":"evaluation/#business-partner-quiz-assessment","text":"PARTNERS \u2014 COMPLETE ASSESSMENT HERE Complete your assessment online using IBM Training: https://learn.ibm.com/mod/subcourse/view.php?id=217396 The accreditation quiz consists of multiple choice questions, with four possible responses (and only one correct answer) for each question. The quiz questions will ask you about on-screen text or descriptions that come up as you work through the lab guide. Answers to the quiz can only be determined by carefully following the instructions of the hands-on lab. Sections of the lab marked as OPTIONAL will not be tested on as part of the quiz. You will have three attempts in total to obtain a passing grade.","title":"Business Partner Quiz Assessment"},{"location":"evaluation/#_3","text":"","title":""},{"location":"evaluation/#next-steps","text":"In the following section, you will prepare a hands-on lab environment with the necessary services and configurations.","title":"Next Steps"},{"location":"setup/","text":"Prerequisites and Setup SETUP TIME ESTIMATES The following section will take approximately 90-120 minutes to complete. Much of the setup work is fully automated and will allow you to perform other tasks or errands while it is underway. Budget your time accordingly. SUPPORT AND TROUBLESHOOTING If you require assistance or run into issues with the hands-on lab, help is available. Environment issues : The lab environment is managed by IBM Technology Zone. Opening a support case ticket is recommended for issues related to the hands-on environment (provisioning, running, and so on.) Documentation issues : If there is an error in the lab documentation, or if you require additional support in completing the material, open a thread on the #platinumdemos-automation-support Slack channel. The hands-on material covered within this lab will focus on event-led integration solutions, specifically the IBM Event Automation suite \u2014 as well as elements of the IBM Cloud Pak for Integration (IBM MQ-Kafka connector.) IBM Event Automation provides an open, composable set of capabilities that interoperate with the services and tools that are already part of a client's ecosystem. A fundamental component to IBM Event Automation's architecture is composability, which will be highlighted through the lab as you incrementally add more capabilities and elements to the demo environment. Event Endpoint Management : to facilitate secured exposure of event streams and unite all event streams (including MQ and Kafka) into a single, unified view. Focus' marketing department can discover, subscribe, and generate security credentials for colleagues using the self-service catalog. Manual processes will be automated and onboarding of users can be expedited. Event Processing : a no-code editor for allowing developers (of any skill level) to construct processing flows, detect situations across multiple events inside a configurable time-window, and validate flow behavior prior to deploying into production (using historical data.) Event Streams + Kafka-IBM MQ connector : part of the IBM Cloud Pak for Integration suite, this connector allows MQ messages to be published into a Kafka-compatible event stream. It provides assurances that all MQ messages are delivered once-and-only-once, without requiring complex or custom code. Environment templates (images) have been defined ahead of time with the assistance of IBM Technology Zone (ITZ), which can be reserved for learning and client demonstration purposes at no-charge. Some further configuration of these environments will be required after deployment. Request an IBM Technology Zone Environment You will require access to the ITZ in order to reserve your environment and complete the hands-on training. If you do not yet have access or an account with the ITZ, you will need to register for one . The hands-on environment\u2014 a combination of services from IBM Cloud Pak for Integration (CPI) and IBM Event Automation (EA) \u2014will be running entirely atop of Red Hat OpenShift , which will orchestrate containerized deployments of CPI and EA across the OpenShift cluster. Reserve a Red Hat OpenShift cluster via the IBM Technology Zone pre-defined template. URL: https://techzone.ibm.com/my/reservations/create/63a3a25a3a4689001740dbb3 From the Single environment reservation options list, select Reserve now . Supply additional details about your ITZ reservation request: Name : Give your reservation a unique name. Purpose : Set to Practice / Self-Education and affirm that customer data will not be used with the environment. If you are replicating this hands-on demonstration with a client, you must select Customer Demo and supply a sales opportunity number. Purpose Description : Provide a brief summary of how the environment will be used. Preferred Geography : Select the data center region that is closest to your location. End Date & Time : Select a time and date for when the reservation will expire. The recommended amount is 2 days, although it is possible to finish the hands-on demonstration within a few hours. Additional time extensions are available. OCP/Kubernetes Cluster Network : Default value of 10.128.0.0/14 Enable FIPS Security : Default value of No OpenShift Version : Set to 4.14 Storage : Set to ODF - 2 TB OCP/Kubernetes Service Network : Default value of 172.30.0.0/16 Worker Node Count : Default value of 3 Worker Node Flavor : Set to 16 vCPU x 64 GB - 100 GB ephemeral storage When satisfied, verify that you agree to the Terms and Conditions for the environment and finalize your reservation request by clicking Submit . PROVISIONING TIMES Reservations take approximately 60 minutes to complete from the time that you click submit. If you navigate to the My Reservations tab of the ITZ, you can monitor the progress of your reservation. Wait for the ITZ reservation to be marked as \" Ready \" before attempting to start the lab \u2014 accessing it too soon will lead to issues (or an outright failure) when connecting to the OpenShift cluster. When the ITZ environment is ready to go, you will receive an email to your inbox (similar to the screenshot below). Click the blue View My Reservations at the bottom of the email to open a page with more details about the environment. Alternatively, you can click the My Reservations tab from the ITZ home page to drill down into your environment's details. Details about your OpenShift cluster, including login and authentication details, are summarized on the page. Record the Username ( kubeadmin ) and Password (unique to your environment), as well as the Desktop URL just above those two items, to a notepad for reference later. Click the blue Open your IBM Cloud environment icon to access the OpenShift web dashboard. You will be prompted to provide the Username and Password recorded in Step 5 . Click Log in to proceed. THIS CONNECTION IS NOT PRIVATE Depending on your web browser, you may receive a warning about This connection is not private or a similarly phrased message. Disregard these warnings and continue to the intended destination by clicking \"visit this website.\" Once authenticated, the web browser will redirect to the main dashboard for the OpenShift Container Platform web interface. You are now ready to proceed with the lab environment customization for IBM Event Automation and IBM Cloud Pak for Integration . Customization of the Lab Environment COMMAND LINE ACCESS For parts of the hands-on lab, you will be required to establish connections over SSH to remote infrastructure endpoints. It is recommended that users do so via Terminal (Mac) or PuTTY (Windows). For detailed instructions on how to connect with PuTTY, or if you are using a Windows machine, reference the linked material . In order to deploy containerized instances of IBM Event Automation and IBM Cloud Pak for Integration on top of OpenShift, you must first download and set up your local machine with the OpenShift Command Line Interface (CLI) . In the top-right corner of the OpenShift dashboard, click the question mark ( ? ) icon as shown in the screenshot below. This will open a drop-down menu with several options. Click the Command line tools option from the drop-down menu. From the oc - OpenShift Command Line Interface (CLI) options, click the download link that matches your local machine's operating system (Mac, Windows, or Linux). Once downloaded, execute the installer and proceed with the lab guide once oc is successfully installed. Return to the OpenShift dashboard landing page. In the far right-hand corner of the interface, click the kube:admin button to open another drop-down menu. From the options, select Copy login command to open a new tab or window with details about remotely accessing the OpenShift cluster. A mostly-blank page will load, with Display Token written in blue text in the top-left of the page. Click this link to proceed. Details about a newly-generated API token will load on screen. Record the full command listed under Log in with this token (resembling something similar to oc login --token=sha256... ) and save this to your notepad for later reference. Open your command line tool of preference on your local machine, likely either Terminal or PuTTY . At this stage, the OpenShift CLI ( oc ) should be available on your machine and ready for remotely connecting to the OpenShift cluster. Within the Terminal, paste the full login command recorded in Step 11 and hit Return to commence the login. A prompt similar to the screenshot below will be displayed momentarily, indicating that the Terminal is now successfully connected (remotely) to the OpenShift cluster. To deploy containerized instances of IBM Event Automation and IBM Cloud Pak for Integration to the OpenShift cluster, you must first download those container images locally and then deploy them remotely on the OpenShift cluster. Execute the following command within your Terminal console to clone the necessary scripts and images to your local machine : git clone https://github.com/IBM/platinum-demo-code-event-automation.git Navigate to the newly cloned platinum-demo-code-event-automation repository with the following command: cd platinum-demo-code-event-automation DEPLOYMENT TIMES The deployment of IBM Event Automation and IBM Cloud Pak for Integration containers to the OpenShift cluster is entirely automated using Red Hat Ansible scripts. Full deployment and configuration should take approximately 20-45 minutes to complete. Execute the following command to kick off the process of deploying and configuring the containerized services to the OpenShift cluster : ./deploy.sh Wait for the full deployment to complete before continuing with the lab documentation. When ready, each of the lab services\u2014 IBM Cloud Pak for Integration (\"Platform Navigator\"), Event Streams, Event Endpoint Management, Event Processing, and IBM MQ \u2014will be listed to the Terminal window. Each listing will be accompanied by a URL , username , and password . Record this information to a notepad for reference later. The Platform Navigator and IBM MQ services utilize the same login details. The password supplied for these two services by the Terminal console in Step 15 is a temporary , one-time-use password. Try logging into the IBM MQ environment environment by pasting the URL into a web browser. Then supply the username and temporary password recorded within your notepad. THIS CONNECTION IS NOT PRIVATE Ignore any \" This connection is not private \" warnings you may receive. If the page does not load, verify that you are connected to the IBM intranet with an active VPN. When prompted, create your own password for logging into the environment. Confirm your selection with Submit and then update your password records for both the Platform Navigator and IBM MQ services. At this stage, the lab environment is fully configured and ready for hands-on work. With your web browser, open and authenticate with (using the credentials captured in Step 15) each of the services needed for the hands-on lab: IBM Cloud Pak for Integration (CPI) Event Streams Event Endpoint Management Event Processing IBM MQ Keep these tabs open. You will be switching between them frequently. MQ & CPI AUTHENTICATION Both services utilize the same username and password credentials. However, the two URLs generated by the Terminal console in Step 15 are unique . Ensure that you are logging in to the correct dashboard (IBM MQ or CPI) when asked to do so in the subsequent chapters. Next Steps In the following module, you will create an event stream from a message queue using IBM MQ .","title":"Setup"},{"location":"setup/#prerequisites-and-setup","text":"SETUP TIME ESTIMATES The following section will take approximately 90-120 minutes to complete. Much of the setup work is fully automated and will allow you to perform other tasks or errands while it is underway. Budget your time accordingly. SUPPORT AND TROUBLESHOOTING If you require assistance or run into issues with the hands-on lab, help is available. Environment issues : The lab environment is managed by IBM Technology Zone. Opening a support case ticket is recommended for issues related to the hands-on environment (provisioning, running, and so on.) Documentation issues : If there is an error in the lab documentation, or if you require additional support in completing the material, open a thread on the #platinumdemos-automation-support Slack channel. The hands-on material covered within this lab will focus on event-led integration solutions, specifically the IBM Event Automation suite \u2014 as well as elements of the IBM Cloud Pak for Integration (IBM MQ-Kafka connector.) IBM Event Automation provides an open, composable set of capabilities that interoperate with the services and tools that are already part of a client's ecosystem. A fundamental component to IBM Event Automation's architecture is composability, which will be highlighted through the lab as you incrementally add more capabilities and elements to the demo environment. Event Endpoint Management : to facilitate secured exposure of event streams and unite all event streams (including MQ and Kafka) into a single, unified view. Focus' marketing department can discover, subscribe, and generate security credentials for colleagues using the self-service catalog. Manual processes will be automated and onboarding of users can be expedited. Event Processing : a no-code editor for allowing developers (of any skill level) to construct processing flows, detect situations across multiple events inside a configurable time-window, and validate flow behavior prior to deploying into production (using historical data.) Event Streams + Kafka-IBM MQ connector : part of the IBM Cloud Pak for Integration suite, this connector allows MQ messages to be published into a Kafka-compatible event stream. It provides assurances that all MQ messages are delivered once-and-only-once, without requiring complex or custom code. Environment templates (images) have been defined ahead of time with the assistance of IBM Technology Zone (ITZ), which can be reserved for learning and client demonstration purposes at no-charge. Some further configuration of these environments will be required after deployment.","title":"Prerequisites and Setup"},{"location":"setup/#_1","text":"","title":""},{"location":"setup/#request-an-ibm-technology-zone-environment","text":"You will require access to the ITZ in order to reserve your environment and complete the hands-on training. If you do not yet have access or an account with the ITZ, you will need to register for one . The hands-on environment\u2014 a combination of services from IBM Cloud Pak for Integration (CPI) and IBM Event Automation (EA) \u2014will be running entirely atop of Red Hat OpenShift , which will orchestrate containerized deployments of CPI and EA across the OpenShift cluster. Reserve a Red Hat OpenShift cluster via the IBM Technology Zone pre-defined template. URL: https://techzone.ibm.com/my/reservations/create/63a3a25a3a4689001740dbb3 From the Single environment reservation options list, select Reserve now . Supply additional details about your ITZ reservation request: Name : Give your reservation a unique name. Purpose : Set to Practice / Self-Education and affirm that customer data will not be used with the environment. If you are replicating this hands-on demonstration with a client, you must select Customer Demo and supply a sales opportunity number. Purpose Description : Provide a brief summary of how the environment will be used. Preferred Geography : Select the data center region that is closest to your location. End Date & Time : Select a time and date for when the reservation will expire. The recommended amount is 2 days, although it is possible to finish the hands-on demonstration within a few hours. Additional time extensions are available. OCP/Kubernetes Cluster Network : Default value of 10.128.0.0/14 Enable FIPS Security : Default value of No OpenShift Version : Set to 4.14 Storage : Set to ODF - 2 TB OCP/Kubernetes Service Network : Default value of 172.30.0.0/16 Worker Node Count : Default value of 3 Worker Node Flavor : Set to 16 vCPU x 64 GB - 100 GB ephemeral storage When satisfied, verify that you agree to the Terms and Conditions for the environment and finalize your reservation request by clicking Submit . PROVISIONING TIMES Reservations take approximately 60 minutes to complete from the time that you click submit. If you navigate to the My Reservations tab of the ITZ, you can monitor the progress of your reservation. Wait for the ITZ reservation to be marked as \" Ready \" before attempting to start the lab \u2014 accessing it too soon will lead to issues (or an outright failure) when connecting to the OpenShift cluster. When the ITZ environment is ready to go, you will receive an email to your inbox (similar to the screenshot below). Click the blue View My Reservations at the bottom of the email to open a page with more details about the environment. Alternatively, you can click the My Reservations tab from the ITZ home page to drill down into your environment's details. Details about your OpenShift cluster, including login and authentication details, are summarized on the page. Record the Username ( kubeadmin ) and Password (unique to your environment), as well as the Desktop URL just above those two items, to a notepad for reference later. Click the blue Open your IBM Cloud environment icon to access the OpenShift web dashboard. You will be prompted to provide the Username and Password recorded in Step 5 . Click Log in to proceed. THIS CONNECTION IS NOT PRIVATE Depending on your web browser, you may receive a warning about This connection is not private or a similarly phrased message. Disregard these warnings and continue to the intended destination by clicking \"visit this website.\" Once authenticated, the web browser will redirect to the main dashboard for the OpenShift Container Platform web interface. You are now ready to proceed with the lab environment customization for IBM Event Automation and IBM Cloud Pak for Integration .","title":"Request an IBM Technology Zone Environment"},{"location":"setup/#_2","text":"","title":""},{"location":"setup/#customization-of-the-lab-environment","text":"COMMAND LINE ACCESS For parts of the hands-on lab, you will be required to establish connections over SSH to remote infrastructure endpoints. It is recommended that users do so via Terminal (Mac) or PuTTY (Windows). For detailed instructions on how to connect with PuTTY, or if you are using a Windows machine, reference the linked material . In order to deploy containerized instances of IBM Event Automation and IBM Cloud Pak for Integration on top of OpenShift, you must first download and set up your local machine with the OpenShift Command Line Interface (CLI) . In the top-right corner of the OpenShift dashboard, click the question mark ( ? ) icon as shown in the screenshot below. This will open a drop-down menu with several options. Click the Command line tools option from the drop-down menu. From the oc - OpenShift Command Line Interface (CLI) options, click the download link that matches your local machine's operating system (Mac, Windows, or Linux). Once downloaded, execute the installer and proceed with the lab guide once oc is successfully installed. Return to the OpenShift dashboard landing page. In the far right-hand corner of the interface, click the kube:admin button to open another drop-down menu. From the options, select Copy login command to open a new tab or window with details about remotely accessing the OpenShift cluster. A mostly-blank page will load, with Display Token written in blue text in the top-left of the page. Click this link to proceed. Details about a newly-generated API token will load on screen. Record the full command listed under Log in with this token (resembling something similar to oc login --token=sha256... ) and save this to your notepad for later reference. Open your command line tool of preference on your local machine, likely either Terminal or PuTTY . At this stage, the OpenShift CLI ( oc ) should be available on your machine and ready for remotely connecting to the OpenShift cluster. Within the Terminal, paste the full login command recorded in Step 11 and hit Return to commence the login. A prompt similar to the screenshot below will be displayed momentarily, indicating that the Terminal is now successfully connected (remotely) to the OpenShift cluster. To deploy containerized instances of IBM Event Automation and IBM Cloud Pak for Integration to the OpenShift cluster, you must first download those container images locally and then deploy them remotely on the OpenShift cluster. Execute the following command within your Terminal console to clone the necessary scripts and images to your local machine : git clone https://github.com/IBM/platinum-demo-code-event-automation.git Navigate to the newly cloned platinum-demo-code-event-automation repository with the following command: cd platinum-demo-code-event-automation DEPLOYMENT TIMES The deployment of IBM Event Automation and IBM Cloud Pak for Integration containers to the OpenShift cluster is entirely automated using Red Hat Ansible scripts. Full deployment and configuration should take approximately 20-45 minutes to complete. Execute the following command to kick off the process of deploying and configuring the containerized services to the OpenShift cluster : ./deploy.sh Wait for the full deployment to complete before continuing with the lab documentation. When ready, each of the lab services\u2014 IBM Cloud Pak for Integration (\"Platform Navigator\"), Event Streams, Event Endpoint Management, Event Processing, and IBM MQ \u2014will be listed to the Terminal window. Each listing will be accompanied by a URL , username , and password . Record this information to a notepad for reference later. The Platform Navigator and IBM MQ services utilize the same login details. The password supplied for these two services by the Terminal console in Step 15 is a temporary , one-time-use password. Try logging into the IBM MQ environment environment by pasting the URL into a web browser. Then supply the username and temporary password recorded within your notepad. THIS CONNECTION IS NOT PRIVATE Ignore any \" This connection is not private \" warnings you may receive. If the page does not load, verify that you are connected to the IBM intranet with an active VPN. When prompted, create your own password for logging into the environment. Confirm your selection with Submit and then update your password records for both the Platform Navigator and IBM MQ services. At this stage, the lab environment is fully configured and ready for hands-on work. With your web browser, open and authenticate with (using the credentials captured in Step 15) each of the services needed for the hands-on lab: IBM Cloud Pak for Integration (CPI) Event Streams Event Endpoint Management Event Processing IBM MQ Keep these tabs open. You will be switching between them frequently. MQ & CPI AUTHENTICATION Both services utilize the same username and password credentials. However, the two URLs generated by the Terminal console in Step 15 are unique . Ensure that you are logging in to the correct dashboard (IBM MQ or CPI) when asked to do so in the subsequent chapters.","title":"Customization of the Lab Environment"},{"location":"setup/#_3","text":"","title":""},{"location":"setup/#next-steps","text":"In the following module, you will create an event stream from a message queue using IBM MQ .","title":"Next Steps"}]}