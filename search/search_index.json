{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Christopher Bienko (Principal, IBM Global Sales Enablement) introduces the objectives and curriculum for IBM Event Automation for Technical Sales Level 3 . Additional ways to watch: Seismic replay available for download. [4 minutes] Integration that weaves together both applications and systems is essential to the success of a modern, digital enterprise. In 2024 and beyond, the enterprise organizations that outpace and outcompete in their marketplace will be those that are the most responsive, agile, and adaptable to an ever-shifting and increasingly-connected IT landscape. The success of these organizations will depend on their ability to securely connect, automate, and transform their business at scale \u2014 made possible through the integration of real-time messaging, events, and processes. Conceptually, it's useful to examine these requirements across three domain areas (which so happen to map to the three pillars of the IBM Integration solution portfolio): Event-led integration : For enterprise to discover what is happening within their business and quickly respond to emergent events. API-led integration : For enterprise that needs to access and share information, both securely and rapidly. Messaging and connectivity : For enterprise to securely and reliable access or update Systems of Record, at scale. Organizations of every size, across all industries and verticals, face a shared set of challenges in their adoption of event-driven architectures and enterprise-wide integration. Enterprise organizations (and many IBM clients) are at different stages in their event-driven journey. For some, they have only recently embarked on scoping and putting into place an event messaging distribution layer. Others already have in place the foundations for an Apache Kafka-based event distribution layer and are looking to add capabilities that boost security, enable governed self-service access, and incorporate event processing. Despite the range of industries that must confront the challenges of event-driven architectures and integration, there are four common themes that regularly emerge in conversations with prospective clients: Real-time access for meeting customer demands and taking action in the moment. Event-driven enterprises must be optimized, timely, adaptable, and proactive. Event-driven integration and automation provides continuous awareness for a business about the state of their systems and services. It drives automation that responds to events for any situation and enables a more adaptable approach to situations evolving in real-time. EVENT-LED INTEGRATION SOLUTIONS FROM IBM IBM Event Automation puts business events to work by making it possible for users to detect unfolding situations and make automated decisions in real-time. The offering consists of three core, composable capabilities: Event Streams : \"Event distribution.\" Collect real-time business events data streams using an enterprise-grade Apache Kafka distribution. Event sources can include Kafka Connect (IBM MQ or databases), REST APIs, and direct integrations (IBM App Connect, Change Data Capture, and more). Event Endpoint Management : \"Event discovery.\" Build a self-service catalog of event sources for other users within the business to securely browse and leverage. Event Processing : \"Event processing.\" Define business situations requiring automated responses ahead of time, using an intuitive authoring canvas that can be invoked in real-time to automate decision making and take decisive actions. To take raw events and make them contextually relevant to business users, Event Processing can: filter and transform event properties; combine events to identify novel patterns over continuous time windows; and aggregate events to analyze trends or detect anomalies. Multiplicity of data sources, all of which a business may need access to. The ever-increasing number of data sources that an enterprise organization connects to provides an expanding surface area for malicious agents to exploit. Moreover, maintaining and managing these data sources becomes increasingly complex as more endpoints are connected to an enterprise's IT estate. + Secure access across the organization. Every enterprise has a plethora of unique APIs that are continuously updating. APIs are a specific vector that attackers can exploit for gaining unauthorized access to systems and data. Two-thirds of cloud incidents are due to misconfigured API endpoints. API-LED INTEGRATION SOLUTIONS FROM IBM The combined challenges of managing across a multiplicity of data sources and securing access to that data is addressed by the core competencies of IBM App Connect and IBM App Connect Enterprise . IBM App Connect allows clients to quickly build and test integration flows with frequently-used authoring tools. It securely connects with hundreds of cloud and on-premises applications via pre-built smart connectors and templates. App Connect's high performance transformation engine virtually supports any-to-any data formats (augmented with AI tooling to assist non-specialists.) It is used to easily deploy integrations with visibility into flow health and performance, for managing across hybrid cloud environments. IBM App Connect supports over 200 OOTB connectors and templates for jumpstarting integration. Automation Explorer provides a community for connectors and templates . Clients can build their own bespoke systems connectors using an accelerated Connector Development Kit . Once connections have been established to the endpoints across an enterprise, API-fed data must be shared (externally or internally) in order to be acted upon. From the moment these APIs are created, they must be managed and secured by a governance and control framework; it is here that API management plays a critical role. API-led integration is the cutting edge way to integrate applications and data by leveraging reusable REST APIs. Via IBM App Connect, the API-led integration approach brings together the disciplines of API management and API integration, across on-premises and cloud. First, clients build an integration flow for their APIs using an OpenAPI editor for authoring gateway policies that will govern OpenAPI documents. IBM's all-in-one flow editor provides a unified API authoring experience that brings together \"application integration\" and \"API management\" into a single tool. By contrast, other integration products require different tools, with differing experiences, to be performed by different users. IBM App Connect makes it possible to define an OpenAPI, build an integration flow, secure it with gateway policies, as well as manage and publish it to a developer portal for re-use \u2014 all within a unified editor. IBM App Connect's partnership with Noname Security helps identify undiscovered API exposure and API Connect's integration with IBM DataPower Gateway provides real-time threat and anomaly detection using AI. Clients can utilize StepZen to rapidly federate a variety of back-end data sources using highly responsive GraphQL APIs (declaratively defined, developer-friendly). Access to a diverse range of development and runtime execution platforms. Endpoints requiring access may include cloud-native application frameworks (containers and microservices), public cloud infrastructure, software-as-a-service (SaaS) applications, software-defined or hyperconverged infrastructure configuration, on-premises implementations of public cloud services, or consumption-billed infrastructure resources. This is not an exhaustive list. MESSAGING AND CONNECTIVITY SOLUTIONS FROM IBM The challenges of integrating across a diverse range of messaging and connectivity endpoints is addressed by a range of IBM technologies: IBM Cloud Pak for Integration , IBM App Connect + API Connect software, and IBM App Connect + API Connect as a Service (on IBM Cloud.) IBM Cloud Pak for Integration runs atop Red Hat OpenShift: a vendor-agnostic, containerization platform that is truly hybrid cloud \u2014 deployable on nearly every on-premises and cloud infrastructure topology. The offering packages together: API management, application integration, end-to-end security, enterprise messaging, event streaming, and high speed data transfers. It provides a unified place for developers, lines of business, and IT operators to catalog, document, and manage services through a single portal. Likewise, all of these services are bundled and licensed together; there is no need to decide what capabilities are required up-front and clients only pay for how much integration is deployed. \"Integration Assemblies\" simplify and automate the deployment of new integrations using the IBM Cloud Pak for Integration. Users can create and manage multiple integration instances as a single object. These instances can be immediately granted high availability (HA) if required. Declarative deployments reduce the complexity of manual integration tasks and rapidly increase the rate at which a business can make new API deployments. \"Operator-deployed container images\" take advantage of Red Hat OpenShift's powerful Operators, reducing the complexity of deploying containerized applications, updating images without disruption, and increasing the resiliency of live containers. Using IBM Event Automation to capitalize on time-sensitive revenue opportunities. Focus Corporation , a hypothetical retail company engaged with IBM, is highly motivated to derive more revenue from first-time customers to their online storefronts. Focus' marketing department believes that sending high-value promotions to new customers immediately after they make a large purchase can significantly boost the company's revenue streams. The challenge is finding a way to capitalize on these time-sensitive revenue opportunities using the real-time transaction data already available (but underutilized) to Focus' IT department. Focus is currently using IBM MQ to exchange customer orders (placed online) between two endpoints: an order management system and the online payment gateway. As part of the hands-on work to be performed, IBM and Focus integration teams will need to tap into IBM MQ communications and publish each order as a Kafka-based \"event stream\". This will enable business-critical communications originating over MQ to be captured as events within Kafka topics, which can later be shared across the enterprise for fueling more responsive applications. Ultimately, Focus' marketing department will be able to use a self-service catalog for inspecting Order and New Customer topics as needed. From there, they will leverage no-code editors to build automated process flows that detect new customer orders totaling over $100 and issue high-value promotions to the customer's inbox immediately following the purchase. Next Steps In the following module, review the necessary pre-requisites and setup instructions for getting started with IBM Event Automation and IBM Cloud Pak for Integration environments.","title":"Introduction"},{"location":"#_1","text":"","title":""},{"location":"#integration-that-weaves-together-both-applications-and-systems-is-essential-to-the-success-of-a-modern-digital-enterprise","text":"In 2024 and beyond, the enterprise organizations that outpace and outcompete in their marketplace will be those that are the most responsive, agile, and adaptable to an ever-shifting and increasingly-connected IT landscape. The success of these organizations will depend on their ability to securely connect, automate, and transform their business at scale \u2014 made possible through the integration of real-time messaging, events, and processes. Conceptually, it's useful to examine these requirements across three domain areas (which so happen to map to the three pillars of the IBM Integration solution portfolio): Event-led integration : For enterprise to discover what is happening within their business and quickly respond to emergent events. API-led integration : For enterprise that needs to access and share information, both securely and rapidly. Messaging and connectivity : For enterprise to securely and reliable access or update Systems of Record, at scale.","title":"Integration that weaves together both applications and systems is essential to the success of a modern, digital enterprise."},{"location":"#_2","text":"","title":""},{"location":"#organizations-of-every-size-across-all-industries-and-verticals-face-a-shared-set-of-challenges-in-their-adoption-of-event-driven-architectures-and-enterprise-wide-integration","text":"Enterprise organizations (and many IBM clients) are at different stages in their event-driven journey. For some, they have only recently embarked on scoping and putting into place an event messaging distribution layer. Others already have in place the foundations for an Apache Kafka-based event distribution layer and are looking to add capabilities that boost security, enable governed self-service access, and incorporate event processing. Despite the range of industries that must confront the challenges of event-driven architectures and integration, there are four common themes that regularly emerge in conversations with prospective clients: Real-time access for meeting customer demands and taking action in the moment. Event-driven enterprises must be optimized, timely, adaptable, and proactive. Event-driven integration and automation provides continuous awareness for a business about the state of their systems and services. It drives automation that responds to events for any situation and enables a more adaptable approach to situations evolving in real-time. EVENT-LED INTEGRATION SOLUTIONS FROM IBM IBM Event Automation puts business events to work by making it possible for users to detect unfolding situations and make automated decisions in real-time. The offering consists of three core, composable capabilities: Event Streams : \"Event distribution.\" Collect real-time business events data streams using an enterprise-grade Apache Kafka distribution. Event sources can include Kafka Connect (IBM MQ or databases), REST APIs, and direct integrations (IBM App Connect, Change Data Capture, and more). Event Endpoint Management : \"Event discovery.\" Build a self-service catalog of event sources for other users within the business to securely browse and leverage. Event Processing : \"Event processing.\" Define business situations requiring automated responses ahead of time, using an intuitive authoring canvas that can be invoked in real-time to automate decision making and take decisive actions. To take raw events and make them contextually relevant to business users, Event Processing can: filter and transform event properties; combine events to identify novel patterns over continuous time windows; and aggregate events to analyze trends or detect anomalies. Multiplicity of data sources, all of which a business may need access to. The ever-increasing number of data sources that an enterprise organization connects to provides an expanding surface area for malicious agents to exploit. Moreover, maintaining and managing these data sources becomes increasingly complex as more endpoints are connected to an enterprise's IT estate. + Secure access across the organization. Every enterprise has a plethora of unique APIs that are continuously updating. APIs are a specific vector that attackers can exploit for gaining unauthorized access to systems and data. Two-thirds of cloud incidents are due to misconfigured API endpoints. API-LED INTEGRATION SOLUTIONS FROM IBM The combined challenges of managing across a multiplicity of data sources and securing access to that data is addressed by the core competencies of IBM App Connect and IBM App Connect Enterprise . IBM App Connect allows clients to quickly build and test integration flows with frequently-used authoring tools. It securely connects with hundreds of cloud and on-premises applications via pre-built smart connectors and templates. App Connect's high performance transformation engine virtually supports any-to-any data formats (augmented with AI tooling to assist non-specialists.) It is used to easily deploy integrations with visibility into flow health and performance, for managing across hybrid cloud environments. IBM App Connect supports over 200 OOTB connectors and templates for jumpstarting integration. Automation Explorer provides a community for connectors and templates . Clients can build their own bespoke systems connectors using an accelerated Connector Development Kit . Once connections have been established to the endpoints across an enterprise, API-fed data must be shared (externally or internally) in order to be acted upon. From the moment these APIs are created, they must be managed and secured by a governance and control framework; it is here that API management plays a critical role. API-led integration is the cutting edge way to integrate applications and data by leveraging reusable REST APIs. Via IBM App Connect, the API-led integration approach brings together the disciplines of API management and API integration, across on-premises and cloud. First, clients build an integration flow for their APIs using an OpenAPI editor for authoring gateway policies that will govern OpenAPI documents. IBM's all-in-one flow editor provides a unified API authoring experience that brings together \"application integration\" and \"API management\" into a single tool. By contrast, other integration products require different tools, with differing experiences, to be performed by different users. IBM App Connect makes it possible to define an OpenAPI, build an integration flow, secure it with gateway policies, as well as manage and publish it to a developer portal for re-use \u2014 all within a unified editor. IBM App Connect's partnership with Noname Security helps identify undiscovered API exposure and API Connect's integration with IBM DataPower Gateway provides real-time threat and anomaly detection using AI. Clients can utilize StepZen to rapidly federate a variety of back-end data sources using highly responsive GraphQL APIs (declaratively defined, developer-friendly). Access to a diverse range of development and runtime execution platforms. Endpoints requiring access may include cloud-native application frameworks (containers and microservices), public cloud infrastructure, software-as-a-service (SaaS) applications, software-defined or hyperconverged infrastructure configuration, on-premises implementations of public cloud services, or consumption-billed infrastructure resources. This is not an exhaustive list. MESSAGING AND CONNECTIVITY SOLUTIONS FROM IBM The challenges of integrating across a diverse range of messaging and connectivity endpoints is addressed by a range of IBM technologies: IBM Cloud Pak for Integration , IBM App Connect + API Connect software, and IBM App Connect + API Connect as a Service (on IBM Cloud.) IBM Cloud Pak for Integration runs atop Red Hat OpenShift: a vendor-agnostic, containerization platform that is truly hybrid cloud \u2014 deployable on nearly every on-premises and cloud infrastructure topology. The offering packages together: API management, application integration, end-to-end security, enterprise messaging, event streaming, and high speed data transfers. It provides a unified place for developers, lines of business, and IT operators to catalog, document, and manage services through a single portal. Likewise, all of these services are bundled and licensed together; there is no need to decide what capabilities are required up-front and clients only pay for how much integration is deployed. \"Integration Assemblies\" simplify and automate the deployment of new integrations using the IBM Cloud Pak for Integration. Users can create and manage multiple integration instances as a single object. These instances can be immediately granted high availability (HA) if required. Declarative deployments reduce the complexity of manual integration tasks and rapidly increase the rate at which a business can make new API deployments. \"Operator-deployed container images\" take advantage of Red Hat OpenShift's powerful Operators, reducing the complexity of deploying containerized applications, updating images without disruption, and increasing the resiliency of live containers.","title":"Organizations of every size, across all industries and verticals, face a shared set of challenges in their adoption of event-driven architectures and enterprise-wide integration."},{"location":"#_3","text":"","title":""},{"location":"#using-ibm-event-automation-to-capitalize-on-time-sensitive-revenue-opportunities","text":"Focus Corporation , a hypothetical retail company engaged with IBM, is highly motivated to derive more revenue from first-time customers to their online storefronts. Focus' marketing department believes that sending high-value promotions to new customers immediately after they make a large purchase can significantly boost the company's revenue streams. The challenge is finding a way to capitalize on these time-sensitive revenue opportunities using the real-time transaction data already available (but underutilized) to Focus' IT department. Focus is currently using IBM MQ to exchange customer orders (placed online) between two endpoints: an order management system and the online payment gateway. As part of the hands-on work to be performed, IBM and Focus integration teams will need to tap into IBM MQ communications and publish each order as a Kafka-based \"event stream\". This will enable business-critical communications originating over MQ to be captured as events within Kafka topics, which can later be shared across the enterprise for fueling more responsive applications. Ultimately, Focus' marketing department will be able to use a self-service catalog for inspecting Order and New Customer topics as needed. From there, they will leverage no-code editors to build automated process flows that detect new customer orders totaling over $100 and issue high-value promotions to the customer's inbox immediately following the purchase.","title":"Using IBM Event Automation to capitalize on time-sensitive revenue opportunities."},{"location":"#_4","text":"","title":""},{"location":"#next-steps","text":"In the following module, review the necessary pre-requisites and setup instructions for getting started with IBM Event Automation and IBM Cloud Pak for Integration environments.","title":"Next Steps"},{"location":"1/","text":"Christopher Bienko (Principal, IBM Global Sales Enablement) provides a hands-on demonstration of Module 1's contents. Additional ways to watch: Seismic replay available for download. [8 minutes] Creating an event stream from an IBM MQ message queue To take advantage of the message queue data available from IBM MQ, Focus Corporation's integration team will need to make IBM MQ's enterprise data available to a broader set of APIs. This will allow Focus' application developers to subscribe to the data without any risk or impact to the core back-end systems that support the data. Risks will be lowered and the application development process can be decoupled from data retention processes. To do so, Focus' integration team will need to expose the IBM MQ enterprise data using \"event streams.\" Specifically, the integration team (and application developers) will need access to the customer order information contained within these streams. This data will be vital for the marketing team's plans to offer high value promotions for newly-acquired customers in a timely manner. Configuring IBM MQ to clone customer order data Both the order management system and its payment gateway system are currently using IBM MQ message queues for exchanging customer order data. The integration team will need to tap into this message exchange, clone each of the orders being handled across IBM MQ, and then publish those messages into a new event stream. Open the IBM Cloud Pak for Integration Platform Navigator dashboard with a web browser. From the home page, open the Orders queue manager From the left-hand side interface, drill down into the Manage [A] tab Select the Queues [A] tab along the top of the page. Scroll down the page until you reach a table, which at this time should show several queues that are already a part of the environment. Create a new queue by clicking the blue Create [A] icon in the top-right corner of the table. Wait for the Queue creation wizard to load, then select Local [A] from Choose queue type . The Quick create [A] option should be selected by default. Under the Queue name [B] field, enter TO.KAFKA (all uppercase) Leave all other settings configured to their default values When ready, click Create [C] The web browser will refresh back to the Manage > Queues perspective From the Queues table, confirm that the TO.KAFKA queue is now available Next, you need to configure IBM MQ to clone all customer order data that it is exchanging between Focus Corporation's order management system and payment gateway. Messages must be reviewed to verify that they contain the expected payload information. From the Manage > Queues table, locate the queue named PAYMENT.REQ and click the name [A] to inspect it in more detail. The queue will already have been populated with multiple Messages . Click on any of the Timestamp [A] names available in the table (something similar to Feb 21, 2024 at 5:12:00 PM ) to pull open a Message Details panel. Scroll down the Message Details panel until you reach Application Data [B] Inspect the contents of the packet, which is a series of key-value pairs: you should see details about the order id , customer , customerid , description , price , quantity , region , and ordertime Close [C] the Message Details panel by clicking the X in the top-right corner or the grey Close button Back on the PAYMENT.REQ orders summary table, locate the Actions button in the top-right corner of the page. Click to open a drop-down menu and then select View configuration [A] . Multiple attributes of the PAYMENT.REQ order queue can be configured from this page. Click the grey Edit [A] button to the right side of the General page. From the tabs on the left side of the page, drill down into Storage [A] . Scroll down until you reach the Streaming queue name [B] field and change the value to TO.KAFKA This will direct IBM MQ to clone messages from the PAYMENT.REQ queue into the TO.KAFKA streaming queue created in Step 5 When satisfied, click the blue Save [C] button in the top-right of the page to confirm the configuration changes Once confirmed, the new configuration will immediately take effect and cloned order messages will promptly begin filling the TO.KAFKA queue. Scroll back up to the top of the page and locate the blue Manage [A] text in the top-left corner of the screen Click the text to return back to the Manage page for Orders From the tabs along the top of the page, click the Queues tab From the table of queues, drill down into the TO.KAFKA queue Take note of the customer order messages that are now populating the TO.KAFKA message queue table. You can click the circular refresh icon (to the left of the blue Create button) to reload the queue contents. Cloning order queues with IBM Event Streams Focus Corporation's integration team will now need to create an event stream called Orders using IBM Event Streams . This will serve as the repository where messages, cloned from IBM MQ, are published and made available to other parts of the organization. The integration team will need to make decisions about how to replicate the data (with or without modification) and also determine the appropriate retention settings for this data. Given the governance policies in place at Focus Corporation, they will need to retain data for up to 1 week and replicate entries for high availability. MQ-KAFKA CONNECTOR IBM MQ allows applications, systems, services, and files to request and coordinate processing tasks \u2014 sending and receiving message data via messaging queues. IBM Event Automation 's Kafka integrations makes it possible to capture a continuous stream of events, representing state changes across one or multiple environments, and makes those events persistently available for retrieval. In combination, IBM MQ and Event Automation enable business-critical communications originating over MQ to be captured as events within Kafka topics, which can later be shared across the enterprise for fueling more responsive applications. IBM MQ-Kafka connectors support bi-directional connectivity between these two architectures. Clients of IBM Event Automation are fully supported for using MQ-Kafka connectors with the Event Streams capability of the platform. Support for the MQ-Kafka connector is also available for IBM clients with MQ Advanced entitlements. Open the IBM Event Streams tab using your web browser. From the IBM Event Streams dashboard, click the Create a topic [A] tile. The team must first decide on a Topic Name [A] . Set the value to ORDERS and then click the blue Next [B] button to continue. Under the Patitions tab, accept the default recommendation of 1 by clicking Next [A] . Under the Message Retention tab, accept the default recommendation [A] of A week and click Next [B] to continue. Under the Replicas tab, accept the default recommendation [A] of Replication factor: 3 and confirm your selections by clicking the Create Topic [B] button. Configuring a message bridge between IBM MQ and IBM Event Streams Using the Apache Kafka connector framework, Focus Corporation's integration team will now need to configure an \"event bridge\" using Red Hat OpenShift. The task can be performed programmatically via the OpenShift console. The bridge configuration will include connectivity details for accessing both IBM MQ and IBM Event Streams. Once configured and deployed, the bridge will utilize the Apache Kafka connector framework to read messages from the TO.KAFKA message queue and then publish those to the newly-created Orders event stream . Return to the OpenShift container platform dashboard. From the Home page, click the + [A] icon located in the top-right corner of the interface. The interface will load an Import YAML configuration tool, with a black canvas awaiting input. Here you can supply YAML (Yet Another Markup Language) or JSON files to define new deployments on the OpenShift cluster. The YAML definition of the Apache Kafka connector \"bridge\" has been prepared ahead of time. Copy and paste the following YAML exactly as written into the Import YAML canvas: apiVersion : eventstreams.ibm.com/v1beta2 kind : KafkaConnector metadata : name : mq-connector namespace : cp4i labels : eventstreams.ibm.com/cluster : kafka-connect-cluster spec : class : com.ibm.eventstreams.connect.mqsource.MQSourceConnector tasksMax : 1 config : # the Kafka topic to produce to topic : ORDERS # the MQ queue to get messages from mq.queue : TO.KAFKA # connection details for the queue manager mq.queue.manager : orders mq.connection.name.list : orders-ibm-mq(1414) mq.channel.name : SYSTEM.DEF.SVRCONN # format of the messages to transfer mq.message.body.jms : true mq.record.builder : com.ibm.eventstreams.connect.mqsource.builders.JsonRecordBuilder key.converter : org.apache.kafka.connect.storage.StringConverter value.converter : org.apache.kafka.connect.json.JsonConverter # whether to send the schema with the messages key.converter.schemas.enable : false value.converter.schemas.enable : false When ready, click Create [A] . Full deployment should only take a moment. Switch over to the IBM Event Streams tab with your web browser. Having configured the streaming queue in IBM MQ (earlier in Step 18), Focus Corporation's integration team now wants to view the orders that have been generated so far. From the home dashboard of the IBM Event Streams service, click the Topic [A] tab (left-hand side) and then click on the name ORDERS from the table. Granular details about the ORDERS topic will be loaded within the browser. From this page, you can inspect all of the messages (orders) generated from the time you set up the IBM MQ streaming queue configuration earlier. Click any one of the orders to pull up additional details on the payload and its contents Note that you can inspect details about the quantity , price , customerid , description , id , region , ordertime , and customer These fields will be valuable later for the marketing team as they look to perform outreach on customers meeting certain criteria. Switch back to the IBM Event Streams dashboard [A] and click Connect to this cluster [B] . Details about your Kafka cluster, including the URL are authentication details, are summarized on this page. Open the Internal tab [A] Record the Kafka SCRAM URL to a notepad for reference later [B] Click Generate SCRAM credentials [C] To connect securely to IBM Event Streams, your application needs credentials with permissions set appropriately for accessing cluster resources (such as Topics). Credential name [A] should be set to es-demo What do you want your application to do? [B] set to Produce messages, consume messages and create topics and schemas Next [C] to continue Select All Topics and click Next . Select All consumer groups and click Next . Select No transactional IDs [A] and click Generate credentials [B] to continue. Record the SCRAM username [A] and SCRAM password [B] values to a notepad for future reference. Next steps In the following module, the integration team will use IBM Event Endpoint Management to import the event stream data and curate a self-service catalog for the marketing department.","title":"1. Creating an event stream from MQ data using Event Streams"},{"location":"1/#_1","text":"","title":""},{"location":"1/#creating-an-event-stream-from-an-ibm-mq-message-queue","text":"To take advantage of the message queue data available from IBM MQ, Focus Corporation's integration team will need to make IBM MQ's enterprise data available to a broader set of APIs. This will allow Focus' application developers to subscribe to the data without any risk or impact to the core back-end systems that support the data. Risks will be lowered and the application development process can be decoupled from data retention processes. To do so, Focus' integration team will need to expose the IBM MQ enterprise data using \"event streams.\" Specifically, the integration team (and application developers) will need access to the customer order information contained within these streams. This data will be vital for the marketing team's plans to offer high value promotions for newly-acquired customers in a timely manner.","title":"Creating an event stream from an IBM MQ message queue"},{"location":"1/#configuring-ibm-mq-to-clone-customer-order-data","text":"Both the order management system and its payment gateway system are currently using IBM MQ message queues for exchanging customer order data. The integration team will need to tap into this message exchange, clone each of the orders being handled across IBM MQ, and then publish those messages into a new event stream. Open the IBM Cloud Pak for Integration Platform Navigator dashboard with a web browser. From the home page, open the Orders queue manager From the left-hand side interface, drill down into the Manage [A] tab Select the Queues [A] tab along the top of the page. Scroll down the page until you reach a table, which at this time should show several queues that are already a part of the environment. Create a new queue by clicking the blue Create [A] icon in the top-right corner of the table. Wait for the Queue creation wizard to load, then select Local [A] from Choose queue type . The Quick create [A] option should be selected by default. Under the Queue name [B] field, enter TO.KAFKA (all uppercase) Leave all other settings configured to their default values When ready, click Create [C] The web browser will refresh back to the Manage > Queues perspective From the Queues table, confirm that the TO.KAFKA queue is now available Next, you need to configure IBM MQ to clone all customer order data that it is exchanging between Focus Corporation's order management system and payment gateway. Messages must be reviewed to verify that they contain the expected payload information. From the Manage > Queues table, locate the queue named PAYMENT.REQ and click the name [A] to inspect it in more detail. The queue will already have been populated with multiple Messages . Click on any of the Timestamp [A] names available in the table (something similar to Feb 21, 2024 at 5:12:00 PM ) to pull open a Message Details panel. Scroll down the Message Details panel until you reach Application Data [B] Inspect the contents of the packet, which is a series of key-value pairs: you should see details about the order id , customer , customerid , description , price , quantity , region , and ordertime Close [C] the Message Details panel by clicking the X in the top-right corner or the grey Close button Back on the PAYMENT.REQ orders summary table, locate the Actions button in the top-right corner of the page. Click to open a drop-down menu and then select View configuration [A] . Multiple attributes of the PAYMENT.REQ order queue can be configured from this page. Click the grey Edit [A] button to the right side of the General page. From the tabs on the left side of the page, drill down into Storage [A] . Scroll down until you reach the Streaming queue name [B] field and change the value to TO.KAFKA This will direct IBM MQ to clone messages from the PAYMENT.REQ queue into the TO.KAFKA streaming queue created in Step 5 When satisfied, click the blue Save [C] button in the top-right of the page to confirm the configuration changes Once confirmed, the new configuration will immediately take effect and cloned order messages will promptly begin filling the TO.KAFKA queue. Scroll back up to the top of the page and locate the blue Manage [A] text in the top-left corner of the screen Click the text to return back to the Manage page for Orders From the tabs along the top of the page, click the Queues tab From the table of queues, drill down into the TO.KAFKA queue Take note of the customer order messages that are now populating the TO.KAFKA message queue table. You can click the circular refresh icon (to the left of the blue Create button) to reload the queue contents.","title":"Configuring IBM MQ to clone customer order data"},{"location":"1/#cloning-order-queues-with-ibm-event-streams","text":"Focus Corporation's integration team will now need to create an event stream called Orders using IBM Event Streams . This will serve as the repository where messages, cloned from IBM MQ, are published and made available to other parts of the organization. The integration team will need to make decisions about how to replicate the data (with or without modification) and also determine the appropriate retention settings for this data. Given the governance policies in place at Focus Corporation, they will need to retain data for up to 1 week and replicate entries for high availability. MQ-KAFKA CONNECTOR IBM MQ allows applications, systems, services, and files to request and coordinate processing tasks \u2014 sending and receiving message data via messaging queues. IBM Event Automation 's Kafka integrations makes it possible to capture a continuous stream of events, representing state changes across one or multiple environments, and makes those events persistently available for retrieval. In combination, IBM MQ and Event Automation enable business-critical communications originating over MQ to be captured as events within Kafka topics, which can later be shared across the enterprise for fueling more responsive applications. IBM MQ-Kafka connectors support bi-directional connectivity between these two architectures. Clients of IBM Event Automation are fully supported for using MQ-Kafka connectors with the Event Streams capability of the platform. Support for the MQ-Kafka connector is also available for IBM clients with MQ Advanced entitlements. Open the IBM Event Streams tab using your web browser. From the IBM Event Streams dashboard, click the Create a topic [A] tile. The team must first decide on a Topic Name [A] . Set the value to ORDERS and then click the blue Next [B] button to continue. Under the Patitions tab, accept the default recommendation of 1 by clicking Next [A] . Under the Message Retention tab, accept the default recommendation [A] of A week and click Next [B] to continue. Under the Replicas tab, accept the default recommendation [A] of Replication factor: 3 and confirm your selections by clicking the Create Topic [B] button.","title":"Cloning order queues with IBM Event Streams"},{"location":"1/#configuring-a-message-bridge-between-ibm-mq-and-ibm-event-streams","text":"Using the Apache Kafka connector framework, Focus Corporation's integration team will now need to configure an \"event bridge\" using Red Hat OpenShift. The task can be performed programmatically via the OpenShift console. The bridge configuration will include connectivity details for accessing both IBM MQ and IBM Event Streams. Once configured and deployed, the bridge will utilize the Apache Kafka connector framework to read messages from the TO.KAFKA message queue and then publish those to the newly-created Orders event stream . Return to the OpenShift container platform dashboard. From the Home page, click the + [A] icon located in the top-right corner of the interface. The interface will load an Import YAML configuration tool, with a black canvas awaiting input. Here you can supply YAML (Yet Another Markup Language) or JSON files to define new deployments on the OpenShift cluster. The YAML definition of the Apache Kafka connector \"bridge\" has been prepared ahead of time. Copy and paste the following YAML exactly as written into the Import YAML canvas: apiVersion : eventstreams.ibm.com/v1beta2 kind : KafkaConnector metadata : name : mq-connector namespace : cp4i labels : eventstreams.ibm.com/cluster : kafka-connect-cluster spec : class : com.ibm.eventstreams.connect.mqsource.MQSourceConnector tasksMax : 1 config : # the Kafka topic to produce to topic : ORDERS # the MQ queue to get messages from mq.queue : TO.KAFKA # connection details for the queue manager mq.queue.manager : orders mq.connection.name.list : orders-ibm-mq(1414) mq.channel.name : SYSTEM.DEF.SVRCONN # format of the messages to transfer mq.message.body.jms : true mq.record.builder : com.ibm.eventstreams.connect.mqsource.builders.JsonRecordBuilder key.converter : org.apache.kafka.connect.storage.StringConverter value.converter : org.apache.kafka.connect.json.JsonConverter # whether to send the schema with the messages key.converter.schemas.enable : false value.converter.schemas.enable : false When ready, click Create [A] . Full deployment should only take a moment. Switch over to the IBM Event Streams tab with your web browser. Having configured the streaming queue in IBM MQ (earlier in Step 18), Focus Corporation's integration team now wants to view the orders that have been generated so far. From the home dashboard of the IBM Event Streams service, click the Topic [A] tab (left-hand side) and then click on the name ORDERS from the table. Granular details about the ORDERS topic will be loaded within the browser. From this page, you can inspect all of the messages (orders) generated from the time you set up the IBM MQ streaming queue configuration earlier. Click any one of the orders to pull up additional details on the payload and its contents Note that you can inspect details about the quantity , price , customerid , description , id , region , ordertime , and customer These fields will be valuable later for the marketing team as they look to perform outreach on customers meeting certain criteria. Switch back to the IBM Event Streams dashboard [A] and click Connect to this cluster [B] . Details about your Kafka cluster, including the URL are authentication details, are summarized on this page. Open the Internal tab [A] Record the Kafka SCRAM URL to a notepad for reference later [B] Click Generate SCRAM credentials [C] To connect securely to IBM Event Streams, your application needs credentials with permissions set appropriately for accessing cluster resources (such as Topics). Credential name [A] should be set to es-demo What do you want your application to do? [B] set to Produce messages, consume messages and create topics and schemas Next [C] to continue Select All Topics and click Next . Select All consumer groups and click Next . Select No transactional IDs [A] and click Generate credentials [B] to continue. Record the SCRAM username [A] and SCRAM password [B] values to a notepad for future reference.","title":"Configuring a message bridge between IBM MQ and IBM Event Streams"},{"location":"1/#_2","text":"","title":""},{"location":"1/#next-steps","text":"In the following module, the integration team will use IBM Event Endpoint Management to import the event stream data and curate a self-service catalog for the marketing department.","title":"Next steps"},{"location":"2/","text":"Christopher Bienko (Principal, IBM Global Sales Enablement) provides a hands-on demonstration of Module 2's contents. Additional ways to watch: Seismic replay available for download. [9 minutes] Building a self-service event catalog with IBM Event Endpoint Management Having successfully deployed the IBM MQ-Apacha Kafka connector framework, Focus Corporation's integration team have created a bridge between MQ and Event Streams. As new customer order data is processed by the IBM MQ message queue, it is simultaneously cloned into the Orders event stream \u2014 awaiting further analysis and action by other users downstream. In this module, Focus' integration team will utilize IBM Event Endpoint Management (EEM) to import the newly-created event stream into a self-service catalog. EEM supports two use cases: the first is allowing teams (like the integration squad) to publish new event streams; the second is for consumers (such as marketing) to browse the contents of those event streams, without requiring direct intervention from the integration team. WHAT IS THE DIFFERENCE BETWEEN OpenAPI and AsyncAPI? OpenAPI and AsyncAPI both represent formalized specifications for describing APIs, but the data source feeding these APIs and the style in which that data is communicated differs between the two. This is important in the context of IBM Event Endpoint Management. OpenAPI is designed for describing RESTful API endpoints, focusing on synchronous client-server communication: a client sends a request and waits for a direct response from a server. Therefore, OpenAPI is well-suited for standard web services where a client interacts with a server in a request-response model. AsyncAPI is built for working with event-driven, asynchronous APIs that rely on asynchronous communication patterns: publish and subscribe, event streaming, and message-driven APIs. It is well-suited for Internet of Things (IoT) applications, real-time data processing use cases, or other scenarios where communication doesn't follow the typical direct request-response (synchronous and RESTful) API pattern. Put simply, the key difference between the two is their differing communication patterns. OpenAPI is designed for synchronous messaging (following the request-response pattern) and AsyncAPI is intended for asynchronous communication (event-driven) patterns. IBM Event Endpoint Management\u2014 a composable service within IBM Event Automation \u2014uses AsyncAPI as the standardized way for describing event-driven messages. It allows clients to quickly \"discover\" events across their ecosystem, publish those events across the organization according to governance rules that ensure compliance, and create enforceable policies through event gateways. Like enterprises do today with API gateways, an event gateway provides isolation, abstraction, policy enforcement, and decoupling to asynchronous (event-driven) data sources. Defining new Topics within IBM Event Streams Before the event stream data can be browsed as part of a self-service catalog, the integration team must first import into EEM the ORDER and CUSTOMER streams via Topics created earlier within IBM Event Streams. Since this is the first time that the integration team has needed to import Topics from IBM Event Streams, they must first register the cluster within IBM Event Endpoint Manager (EEM): Switch over to the EEM tab within your web browser Click the Hide Welcome [A] text to collapse the tutorials at the top of the page From the left-hand side of the interface, click Topic [B] From the right side of the interface, click the blue Add Topic button. A configuration tool for importing a Topic into EEM will load. Interaction settings: Click the Consume events [A] and then Next to proceed Cluster connection settings: Click the Add new cluster [B] button Set the Cluster Name [A] variable to IBM Event Streams and click Next [B] to continue. Additional details about the connection are required: the endpoint address, certificates required for secure communication with the cluster, and authentication credentials. Set the Servers [A] variable to the Kafka SCRAM URL that you copied in Module 1 - Step 24 (likely es-demo-kafka-bootstrap.tools.svc:9095 ) Click Next [B] to continue Click (to add a checkmark) on the Accept all certificates [A] toggle, then click Next [B] . Next, configure details regarding Credentials . The Security Protocol field should remain at the default value of SCRAM-SHA-512 Username [A] should be set to es-demo (set in the previous section) Password [B] must match the IBM Event Streams password generated in the Setup module. This password is unique to your particular environment. When ready, click Add Cluster [C] The web browser will return to the Add Topic configuration page from Step 2. Under the Cluster connection [A] header, select the IBM Event Streams connection defined earlier When ready, click Next [B] Now select the Topics that will be made available for browsing within the Event Endpoint Management catalog. Toggle (select) both the CUSTOMERS [A] and ORDERS [B] topics Keep the alias values set to their recommended defaults When ready, click Add Topic [C] Defining a data schema for CUSTOMERS With the connections to IBM Event Streams defined and the Kafka Topics selected for replication, Focus' integration team is ready to create self-service event stream catalogs within EEM. The team must provide descriptions of the event streams, including an example of how the data is structured and formatted. This description will be beneficial to other individuals within the company (such as the marketing department), who will want to know at-a-glance whether a particular Topic catalog fits their needs. From the EEM dashboad, drill down into Topics [A] as before and click the name of the CUSTOMERS [B] topic. Click the blue Edit information [A] on the right-hand side of the page. Within the Overview information tab: Scroll down until you reach the Description [A] field and enter the following text: Events generated by the customer management system. A new event is created for each new user registration. Scroll further down until you reach the Tags [B] field and enter customer For the Contact Email [C] field, enter customerservice@focus.corp Click the Event Information [D] tab from the left side of the interface DO NOT CLICK SAVE \u2014 if you did click save, return to the previous screen by clicking the Edit information button again Within the Event information [A] tab, scroll down until you locate the Sample message [B] field. Here you will provide a representation of a typical message on the CUSTOMERS event stream. Copy and paste the following JSON into the Sample message field: { \"customerid\" : \"acb3eb65-98a1-45c2-84d4-f5df157862b4\" , \"customername\" : \"Emilio Quitzon\" , \"registered\" : \"2023-10-24 19:20:35.638\" } When ready, click Save [C] to finalize both the Overview Information and Event Information configuration settings. The web browser will return to the CUSTOMERS Topic overview. From the three tabs at the top of the screen, click Options [A] Click the blue Create Options [B] button Within the Details tab, provide the following customizations: Order name [A] : Customer Access Alias [B] : CUSTOMERS Description [C] : Self-service access to the customer event stream When ready, click Next [D] . Leave the Controls tab empty ( No data ) and click Next [A] to proceed. The Publish option tab will give you a final opportunity to review the CUSTOMERS topic before publishing for other users of EEM. Within the blue Option Status field, click the Publish [A] button Further down the panel, under the Available gateways [B] field, verify that the production option is pre-selected When ready, click Publish [C] to finalize publication of the CUSTOMERS topic Defining a data schema for ORDERS The process of publishing an event streams Topic within EEM (Steps 9\u201316) must now be repeated\u2014 with some slight modifications \u2014for the ORDERS topic. From the EEM dashboard, drill down into Topics [A] and then click the name ORDERS [B] . Click on the Edit information button. Within the Overview information tab: Scroll down until you reach the Description [A] field and enter the following text: Events generated by the customer management system. An event will be emitted for every new order that is made. Scroll further down until you reach the Tags [B] field and enter orders For the Contact Email [C] field, enter orders@focus.corp Click the Event Information [D] tab from the left side of the interface DO NOT CLICK SAVE \u2014 if you did click save, return to the previous screen by clicking the Edit information button again Within the Event information tab, scroll down until you locate the Sample message [A] field. Here you will provide a representation of a typical message on the ORDERS event stream. Copy and paste the following JSON into the field: { \"quantity\" : 9 , \"price\" : 197.09 , \"customerid\" : \"a7d1586b-ced1-462f-9e44-14e9e5013540\" , \"description\" : \"Composite Oversize 28in Tennis Racket\" , \"id\" : \"1eba7af9-b748-4754-b750-3459e589dccf\" , \"region\" : \"EMEA\" , \"ordertime\" : \"2023-10-24 19:26:04.839\" , \"customer\" : \"Reed McKenzie DDS\" } When ready, click Save [B] to finalize both the Overview Information and Event Information configuration settings. The web browser will return to the ORDERS Topic overview. From the three tabs at the top of the screen, click Options [A] Click the blue Create Options [B] button Within the Details tab, provide the following customizations: Order name [A] : Order Access Alias [B] : ORDERS Description [C] : Self-service access to the order event stream When ready, click Next [D] . Leave the Controls tab empty ( No data ) and click Save to proceed. The Publish option tab will give you a final opportunity to review the ORDERS topic before publishing for other users of EEM. Within the blue Option Status field, click the Publish [A] button Further down the panel, under the Available gateways [B] field, verify that the production option is pre-selected When ready, click Publish [C] to finalize publication of the ORDERS topic Validating the configuration changes and generating access credentials Before handing access over to Focus Corporation's marketing team, the integration squad must first validate that the catalogued event streams are performing as expected. If no issues are detected, they can generate access credentials for users to securely access the catalog. Return to the EEM dashboard and drill down into Catalog [A] , then click the name CUSTOMERS [B] from the table. Click the Generate access credentials [A] button in the top-right corner of the page. Provide an email address for the Contact details field: marketing@focus.corp When ready, click Generate . Access credentials for CUSTOMERS will be generated immediately and displayed within the web browser. Record the Username [A] and Password [B] values to a notepad for reference later Alternatively, save the credentials as a JSON file (using the Download as JSON button) on your local machine RECORD THE ACCESS CREDENTIALS Don't proceed with the lab guide instructions before recording the CUSTOMERS access credentials. These values cannot be referenced again once this window is closed. You can always generate new credentials again later, but it will require repeating Steps 25-27. Generate access credentials for the ORDERS catalog in the same way done for CUSTOMERS : Return to the EEM dashboard Drill down into Catalog and then click ORDERS Click the Generate access credentials button. Provide an email address for the Contact details field: marketing@focus.corp When ready, click Generate . Access credentials for ORDERS will be generated immediately within the web browser: Record the Username and Password values to a notepad for reference later Alternatively, save the credentials as a JSON file (using the Download as JSON button) on your local machine Next steps At this stage, responsibility can shift towards Focus Corporation's marketing team. Thanks to the hard work of the integration squad, the marketing department now has a fully configured, self-service catalog of event stream data available within EEM. In the following module, Focus' marketing team will use IBM Event Processing to correlate newly-created customer accounts with orders totaling over $100 within a 24-hour time window.","title":"2. Building a self-service catalog of events with Event Endpoint Management"},{"location":"2/#_1","text":"","title":""},{"location":"2/#building-a-self-service-event-catalog-with-ibm-event-endpoint-management","text":"Having successfully deployed the IBM MQ-Apacha Kafka connector framework, Focus Corporation's integration team have created a bridge between MQ and Event Streams. As new customer order data is processed by the IBM MQ message queue, it is simultaneously cloned into the Orders event stream \u2014 awaiting further analysis and action by other users downstream. In this module, Focus' integration team will utilize IBM Event Endpoint Management (EEM) to import the newly-created event stream into a self-service catalog. EEM supports two use cases: the first is allowing teams (like the integration squad) to publish new event streams; the second is for consumers (such as marketing) to browse the contents of those event streams, without requiring direct intervention from the integration team. WHAT IS THE DIFFERENCE BETWEEN OpenAPI and AsyncAPI? OpenAPI and AsyncAPI both represent formalized specifications for describing APIs, but the data source feeding these APIs and the style in which that data is communicated differs between the two. This is important in the context of IBM Event Endpoint Management. OpenAPI is designed for describing RESTful API endpoints, focusing on synchronous client-server communication: a client sends a request and waits for a direct response from a server. Therefore, OpenAPI is well-suited for standard web services where a client interacts with a server in a request-response model. AsyncAPI is built for working with event-driven, asynchronous APIs that rely on asynchronous communication patterns: publish and subscribe, event streaming, and message-driven APIs. It is well-suited for Internet of Things (IoT) applications, real-time data processing use cases, or other scenarios where communication doesn't follow the typical direct request-response (synchronous and RESTful) API pattern. Put simply, the key difference between the two is their differing communication patterns. OpenAPI is designed for synchronous messaging (following the request-response pattern) and AsyncAPI is intended for asynchronous communication (event-driven) patterns. IBM Event Endpoint Management\u2014 a composable service within IBM Event Automation \u2014uses AsyncAPI as the standardized way for describing event-driven messages. It allows clients to quickly \"discover\" events across their ecosystem, publish those events across the organization according to governance rules that ensure compliance, and create enforceable policies through event gateways. Like enterprises do today with API gateways, an event gateway provides isolation, abstraction, policy enforcement, and decoupling to asynchronous (event-driven) data sources.","title":"Building a self-service event catalog with IBM Event Endpoint Management"},{"location":"2/#defining-new-topics-within-ibm-event-streams","text":"Before the event stream data can be browsed as part of a self-service catalog, the integration team must first import into EEM the ORDER and CUSTOMER streams via Topics created earlier within IBM Event Streams. Since this is the first time that the integration team has needed to import Topics from IBM Event Streams, they must first register the cluster within IBM Event Endpoint Manager (EEM): Switch over to the EEM tab within your web browser Click the Hide Welcome [A] text to collapse the tutorials at the top of the page From the left-hand side of the interface, click Topic [B] From the right side of the interface, click the blue Add Topic button. A configuration tool for importing a Topic into EEM will load. Interaction settings: Click the Consume events [A] and then Next to proceed Cluster connection settings: Click the Add new cluster [B] button Set the Cluster Name [A] variable to IBM Event Streams and click Next [B] to continue. Additional details about the connection are required: the endpoint address, certificates required for secure communication with the cluster, and authentication credentials. Set the Servers [A] variable to the Kafka SCRAM URL that you copied in Module 1 - Step 24 (likely es-demo-kafka-bootstrap.tools.svc:9095 ) Click Next [B] to continue Click (to add a checkmark) on the Accept all certificates [A] toggle, then click Next [B] . Next, configure details regarding Credentials . The Security Protocol field should remain at the default value of SCRAM-SHA-512 Username [A] should be set to es-demo (set in the previous section) Password [B] must match the IBM Event Streams password generated in the Setup module. This password is unique to your particular environment. When ready, click Add Cluster [C] The web browser will return to the Add Topic configuration page from Step 2. Under the Cluster connection [A] header, select the IBM Event Streams connection defined earlier When ready, click Next [B] Now select the Topics that will be made available for browsing within the Event Endpoint Management catalog. Toggle (select) both the CUSTOMERS [A] and ORDERS [B] topics Keep the alias values set to their recommended defaults When ready, click Add Topic [C]","title":"Defining new Topics within IBM Event Streams"},{"location":"2/#_2","text":"","title":""},{"location":"2/#defining-a-data-schema-for-customers","text":"With the connections to IBM Event Streams defined and the Kafka Topics selected for replication, Focus' integration team is ready to create self-service event stream catalogs within EEM. The team must provide descriptions of the event streams, including an example of how the data is structured and formatted. This description will be beneficial to other individuals within the company (such as the marketing department), who will want to know at-a-glance whether a particular Topic catalog fits their needs. From the EEM dashboad, drill down into Topics [A] as before and click the name of the CUSTOMERS [B] topic. Click the blue Edit information [A] on the right-hand side of the page. Within the Overview information tab: Scroll down until you reach the Description [A] field and enter the following text: Events generated by the customer management system. A new event is created for each new user registration. Scroll further down until you reach the Tags [B] field and enter customer For the Contact Email [C] field, enter customerservice@focus.corp Click the Event Information [D] tab from the left side of the interface DO NOT CLICK SAVE \u2014 if you did click save, return to the previous screen by clicking the Edit information button again Within the Event information [A] tab, scroll down until you locate the Sample message [B] field. Here you will provide a representation of a typical message on the CUSTOMERS event stream. Copy and paste the following JSON into the Sample message field: { \"customerid\" : \"acb3eb65-98a1-45c2-84d4-f5df157862b4\" , \"customername\" : \"Emilio Quitzon\" , \"registered\" : \"2023-10-24 19:20:35.638\" } When ready, click Save [C] to finalize both the Overview Information and Event Information configuration settings. The web browser will return to the CUSTOMERS Topic overview. From the three tabs at the top of the screen, click Options [A] Click the blue Create Options [B] button Within the Details tab, provide the following customizations: Order name [A] : Customer Access Alias [B] : CUSTOMERS Description [C] : Self-service access to the customer event stream When ready, click Next [D] . Leave the Controls tab empty ( No data ) and click Next [A] to proceed. The Publish option tab will give you a final opportunity to review the CUSTOMERS topic before publishing for other users of EEM. Within the blue Option Status field, click the Publish [A] button Further down the panel, under the Available gateways [B] field, verify that the production option is pre-selected When ready, click Publish [C] to finalize publication of the CUSTOMERS topic","title":"Defining a data schema for CUSTOMERS"},{"location":"2/#defining-a-data-schema-for-orders","text":"The process of publishing an event streams Topic within EEM (Steps 9\u201316) must now be repeated\u2014 with some slight modifications \u2014for the ORDERS topic. From the EEM dashboard, drill down into Topics [A] and then click the name ORDERS [B] . Click on the Edit information button. Within the Overview information tab: Scroll down until you reach the Description [A] field and enter the following text: Events generated by the customer management system. An event will be emitted for every new order that is made. Scroll further down until you reach the Tags [B] field and enter orders For the Contact Email [C] field, enter orders@focus.corp Click the Event Information [D] tab from the left side of the interface DO NOT CLICK SAVE \u2014 if you did click save, return to the previous screen by clicking the Edit information button again Within the Event information tab, scroll down until you locate the Sample message [A] field. Here you will provide a representation of a typical message on the ORDERS event stream. Copy and paste the following JSON into the field: { \"quantity\" : 9 , \"price\" : 197.09 , \"customerid\" : \"a7d1586b-ced1-462f-9e44-14e9e5013540\" , \"description\" : \"Composite Oversize 28in Tennis Racket\" , \"id\" : \"1eba7af9-b748-4754-b750-3459e589dccf\" , \"region\" : \"EMEA\" , \"ordertime\" : \"2023-10-24 19:26:04.839\" , \"customer\" : \"Reed McKenzie DDS\" } When ready, click Save [B] to finalize both the Overview Information and Event Information configuration settings. The web browser will return to the ORDERS Topic overview. From the three tabs at the top of the screen, click Options [A] Click the blue Create Options [B] button Within the Details tab, provide the following customizations: Order name [A] : Order Access Alias [B] : ORDERS Description [C] : Self-service access to the order event stream When ready, click Next [D] . Leave the Controls tab empty ( No data ) and click Save to proceed. The Publish option tab will give you a final opportunity to review the ORDERS topic before publishing for other users of EEM. Within the blue Option Status field, click the Publish [A] button Further down the panel, under the Available gateways [B] field, verify that the production option is pre-selected When ready, click Publish [C] to finalize publication of the ORDERS topic","title":"Defining a data schema for ORDERS"},{"location":"2/#validating-the-configuration-changes-and-generating-access-credentials","text":"Before handing access over to Focus Corporation's marketing team, the integration squad must first validate that the catalogued event streams are performing as expected. If no issues are detected, they can generate access credentials for users to securely access the catalog. Return to the EEM dashboard and drill down into Catalog [A] , then click the name CUSTOMERS [B] from the table. Click the Generate access credentials [A] button in the top-right corner of the page. Provide an email address for the Contact details field: marketing@focus.corp When ready, click Generate . Access credentials for CUSTOMERS will be generated immediately and displayed within the web browser. Record the Username [A] and Password [B] values to a notepad for reference later Alternatively, save the credentials as a JSON file (using the Download as JSON button) on your local machine RECORD THE ACCESS CREDENTIALS Don't proceed with the lab guide instructions before recording the CUSTOMERS access credentials. These values cannot be referenced again once this window is closed. You can always generate new credentials again later, but it will require repeating Steps 25-27. Generate access credentials for the ORDERS catalog in the same way done for CUSTOMERS : Return to the EEM dashboard Drill down into Catalog and then click ORDERS Click the Generate access credentials button. Provide an email address for the Contact details field: marketing@focus.corp When ready, click Generate . Access credentials for ORDERS will be generated immediately within the web browser: Record the Username and Password values to a notepad for reference later Alternatively, save the credentials as a JSON file (using the Download as JSON button) on your local machine","title":"Validating the configuration changes and generating access credentials"},{"location":"2/#_3","text":"","title":""},{"location":"2/#next-steps","text":"At this stage, responsibility can shift towards Focus Corporation's marketing team. Thanks to the hard work of the integration squad, the marketing department now has a fully configured, self-service catalog of event stream data available within EEM. In the following module, Focus' marketing team will use IBM Event Processing to correlate newly-created customer accounts with orders totaling over $100 within a 24-hour time window.","title":"Next steps"},{"location":"3/","text":"Christopher Bienko (Principal, IBM Global Sales Enablement) provides a hands-on demonstration of Module 3's contents. Additional ways to watch: Seismic replay available for download. [14 minutes] Using IBM Event Processing no-code editors to configure the solution Focus Corporation's marketing department now has a fully configured, self-service catalog of event stream data available within IBM Event Endpoint Management (EEM) . At this stage, work will transition from the integration team to the domain of the marketing team. Focus' marketing team will utilize IBM Event Processing (EP) to correlate any newly-created customer accounts with first-time purchase orders that total over $100 USD in value. Furthermore, these correlated transactions will need to be identified within a 24-hour window of when they occur. If these conditions can be met, the marketing team will be able to create an actionable plan for putting high-value promotional offers in front of first-time customers in a timely manner \u2014 which should translate to more sales revenue through Focus' digital storefront. With your web browser, access the IBM Event Processing dashboard. Dismiss any welcome screens or tutorials that are prompted after first logging in by clicking the Hide Welcome [A] text in the bottom-left corner of the interface. In the bottom-right corner of the page, click the blue Create [B] to begin defining a new flow . EVENT PROCESSING FLOWS An event processing flow ingests a stream of events, analyzes that input, and takes automatic actions (defined by the user) in response to those conditions. A flow begins with one or multiple event sources which represent inbound events. For the purposes of this demonstration, you will leverage the ORDERS event stream that was defined in the previous module. You will do so using EP's intuitive, no-code authoring canvas. For Name [A] , specify NewCustomerLargeOrder and then click Create [B] . Configuring an event source for Orders Before the inbound events can be captured, connectivity details for the event source node ( ORDERS ) must be configured within the authoring canvas. Wait until the page has refreshed to display the no-code environment. Along the left side are tools and actions that can be dragged into the authoring canvas on the right Within the canvas, you will see an Event source [A] node with default name of source_1 With your cursor, hover over the source_1 node and then click the Edit [B] (pencil) icon in the top-right corner of the tile to begin modifying the node The Configure event source wizard will load within your browser. Under the Add an event source or pick a recent one header, select Add event source [A] Click Next [B] Switch back to the Event Endpoint Management tab and click Catalog . Drill down into the ORDERS [A] topic by clicking on the name Scroll down the topic summary page until you reach the Access Information section Just below, within the Servers table, is the full address of the ORDERS event topic Copy the full address ( ademo-event-gw-ibm... ) [B] to your notepad for reference later When ready, switch back to the IBM Event Processing tab with your web browser Back within EP: the first tab, Details , needs additional information to configure the node. Scroll down to the Node name field and enter Orders Further down, under the Connect to Kafka cluster header, paste the full address copied in Step 6 into the Server [A] field Click Next The event source must be configured to allow certificates used by the event stream. Accept certificates [B] (by clicking the toggle button as shown in the screenshot below) Click Next [C] The second tab, Access credentials , requires authentication details for the EEM event source. Keep the Select security mechanism variable as its default value: PLAIN For Username [A] and Password [B] , supply the credentials for the ORDERS topic (Step 32 of Module 2) Click Next [C] The third tab, Topic selection , asks for which topic(s) to ingest from the event source Available topics will already be displayed within the configuration tool Under the Topic selection header, click the button next to the ORDERS [A] topic to select it Click Next [B] The fourth tab, Define event structure , will automatically recognize the expected JSON data structure of events ingested from the event source. Message format may be left as the JSON value Compare the JSON sample message field, ingested from one of the captured Kafka event messages, with the message structure format recorded in the previous module (below) When ready, click Next to proceed From the Key and Headers tab, you will NOT need to map key and headers to event fields: simply click Next to continue Note that this is the same sample schema you supplied when configuring the event source within EEM in Module 2. { \"quantity\" : 9 , \"price\" : 197.09 , \"customerid\" : \"a7d1586b-ced1-462f-9e44-14e9e5013540\" , \"description\" : \"Composite Oversize 28in Tennis Racket\" , \"id\" : \"1eba7af9-b748-4754-b750-3459e589dccf\" , \"region\" : \"EMEA\" , \"ordertime\" : \"2023-10-24 19:26:04.839\" , \"customer\" : \"Reed McKenzie DDS\" } You will have a final opportunity to review and confirm the event source configuration. Set the Node name [A] to Orders Click Configure [B] to finalize the configuration of the Orders event source Configuring an event source for New Customers The configurations performed for the ORDERS topic now (Steps 4 \u2013 12) must now be repeated for the CUSTOMERS topic. Return to the NewCustomerLargeOrder [A] canvas and drag a new Event Source [B] (from the left-hand interface) into the canvas, placing it just below the Orders source. With your cursor, hover over the new source_1 node and then click the Edit (pencil) icon in the top-right corner of the tile to begin modifying the node. The Configure event source wizard will load within your browser. Under the Add an event source or pick a recent one header, select Add event source [A] Click Next [B] Switch back to the Event Endpoint Management tab and click Catalog . Drill down into the CUSTOMERS topic by clicking on the name Scroll down the topic summary page until you reach the Access Information section Just below, within the Servers table, is the full address of the CUSTOMERS event topic Copy the full address ( ademo-event-gw-ibm... ) to your notepad for reference later When ready, switch back to the IBM Event Processing tab with your web browser Back within EP: the first tab, Details , needs additional information to configure the node. Scroll down to the Node name [A] field and enter New Customers Further down, under the Connect to Kafka cluster header, paste the full address copied in Step 15 into the Server [B] field Click Next [C] The event source must be configured to allow certificates used by the event stream. Accept certificates (by clicking the toggle button as shown in the screenshot below) Click Next The second tab, Access credentials , requires authentication details for the EEM event source. Keep the Select security mechanism variable as its default value: PLAIN For Username and Password , supply the credentials for the CUSTOMERS topic (Step 28 of Module 2) Click Next The third tab, Topic selection , will ask which topic(s) to ingest from the event source. Available topics will already be displayed within the configuration tool Under the Topic selection header, click the button next to the CUSTOMERS [A] topic to select it Click Next [B] The fourth tab, Define event structure , will automatically recognize the expected JSON data structure of events ingested from the event source. Message format may be left as the JSON value Compare the JSON sample message field, ingested from one of the captured Kafka event messages, with the message structure format recorded in the previous module (below) When ready, click Next to proceed From the Key and Headers tab, you will NOT need to map key and headers to event fields: simply click Next to continue Note that this is the same sample schema you supplied when configuring the event source within EEM in Module 2 { \"customerid\" : \"acb3eb65-98a1-45c2-84d4-f5df157862b4\" , \"customername\" : \"Emilio Quitzon\" , \"registered\" : \"2023-10-24 19:20:35.638\" } You will have a final opportunity to review and confirm the event source configuration. Set the Node name [A] field to New Customers Click Configure [A] to finalize the configuration of the New Customers event source Filtering event sources to match specific criteria With the event source fully configured within EP, you can now begin tailoring the event processing flow to filter on orders where the value exceeds $100 USD. Scroll down within the list of nodes and actions on the left-side interface until you locate the Filter node. Click and hold the Filter [A] node, then drag it to the authoring canvas on the right Position it to the right side of the Orders node The output terminal for Orders needs to be connected as input to the filter_1 node. With your mouse pointer, hover over the Orders node and take note of the grey dot (labelled Output Port [B] ) at the right edge of the tile Click and hold the Output Port (right-most) of Orders and drag it to the Input Port [C] (left-most edge) of filter_1 , then release your hold to complete the circuit With the Orders event source node connected to the filter_1 operation node, you must utilize the Filter node's expression builder to remove orders that are less than $100 USD in value. This will allow the marketing team's application to target only first-time customers with spending patterns of $100 USD or more. Hover over the filter_1 node with your mouse pointer and click the Edit [D] (pencil) icon. Within the Details tab, specify the value of Node name as FilterLargeOrders When ready, click Next . The Define filter tab will load next. To the right of the Filter expression field, click the Assistant [A] drop-down menu to expose additional options. Event Processing's expression builder will guide you in defining the filter. Property to filter on: price Condition: Is greater than Value: 100 Click the Add to expression button The expression builder will automatically translate your statements into a syntactically-correct filter statement: 'price' > 100 Click the Configure [B] button at the bottom of the panel to finalize the filter configuration Performing a JOIN across two event sources The marketing team needs to \"JOIN\" the filtered Order stream data to records of New Customers in order to correctly identify first-time purchasers that placed an order for over $100 USD in the last 24 hours. Use EP's authoring canvas to carry out the necessary JOIN operation. Back on the canvas for NewCustomerLargeOrders , scroll down along the left-hand interface until you locate the Interval Join [A] node. Drag and drop this node [B] to the far right-hand edge of the authoring canvas. With your cursor, hover over the New Customers output terminal and locate the Output Port (grey) button. Click and hold the New Customers Output Port [A] edge and drag this to the Input Port [B] edge of the newly-created intervalJoin_1 node Release your cursor to finalize the circuit Repeat the process, this time connecting the FilterLargeOrders node's Output Port [A] to the intervalJoin_1 node's Input Port [B] . At this stage, the input of intervalJoin_1 should be connected to the outputs of both the FilterLargeOrders and New Customers nodes. The JOIN node must now be configured to correlate events based on a shared customerid field within the two event streams ( New Customers and pre-filtered Orders ). With your cursor, hover over the intervalJoin_1 node and click the Edit (pencil) icon. Within the Details tab, set Node name [A] equal to DetectNewCustomerLargeOrders When ready, click Next [B] . Within the Join conditions tab, activate the expression builder by click the Assistant [A] drop-down menu. Set the following properties: Specify property from 'New Customers': customerid Specify property from 'FilterLargeOrders': customerid When ready, click Add to expression [B] The syntactically-correct JOIN expression will be expressed under the Define events field: 'New Customers'.'customerid' = 'FilterLargeOrders'.'customerid' After you have reviewed the JOIN condition, click Next [C] TRIGGERING EVENTS AND DETECTED EVENTS Two concepts are important to understand at this time: Triggering events and Detected events . Since the New Customer sign-up event must logically occur before a purchase, it is considered a triggering event Therefore, when a signal for an Order purchase of over $100 (the detected event ) is received, the JOIN logic condition will be triggered for the two event streams Under the Time window condition , you can define the time interval \"window\" where detected events are considered viable for a promotional offer from the marketing team. To meet the criteria, a purchase of over $100 USD must be made by a first-time customer within a 24 hour window of creating an account. Look for the Event to detect [A] field and select the FilterLargeOrders (event_time) option from the drop-down menu A Preview visualization of the time window interval is rendered within the panel \u2014 for now, with no adjustments made, this will appear as a vertical bar at 0 (on the X-axis) Scroll down further to reveal additional attributes to edit Event to set the time window : New Customers (event-time) Offset from event to start the time window : 0 HOUR(S) Offset from event to end the time window [B] : 24 HOUR(S) When ready, click Next [C] The JOIN node's output will be a combination of fields from both the filtered Orders and New Customers events streams. Before finalizing the interval join, you need to clean up the output so that duplicate fields (like customerid and event_time ) are not included from the JOIN operation. You have the option of renaming or removing the duplicate fields \u2014 but for the sake of this demonstration, you will be removing the fields. Within the Output properties tab, locate the customerid [A] field. There are two (duplicate) copies of the same field at this time. Your objective will be to delete ONE of the two duplicate records, where the Source is labelled as New Customers . Preserve the record where Source is labelled as FilterLargeOrders . Click the round \u2014 sign to the left of the first customerid row, where the Source is labelled as \"New Customers\", to remove the duplicate The result will be only one copy of the customerid field in the table The red text ( Must be unique ) underlining the field will disappear from the table ONLY REMOVE DUPLICATES WHERE SOURCE == NEW CUSTOMERS When reviewing Output properties for duplicate fields, take note of the table's Source column. Duplicates will be produced by both the New Customers and FilterLargeOrders nodes. Only remove duplicates where the Source is listed as New Customers . Repeat the procedure for the event_time field, which is also duplicated by the JOIN: Delete the event_time row where the Source column is labelled as \"New Customers\" With all duplicates removed from the table, the Configure [A] button will now be highlighted in blue and selectable Click the Configure button to finalize the interval join configuration Next steps With filtering and processing steps now in place for incoming event streams, Focus Corporation's marketing team is able to detect valid promotional opportunities and emit those detected events to their customer loyalty application. In the following section, you will create those linkages and validate its effectiveness using \"live\" data feeds.","title":"3. Configuring the solution with Event Processing no-code editors"},{"location":"3/#_1","text":"","title":""},{"location":"3/#using-ibm-event-processing-no-code-editors-to-configure-the-solution","text":"Focus Corporation's marketing department now has a fully configured, self-service catalog of event stream data available within IBM Event Endpoint Management (EEM) . At this stage, work will transition from the integration team to the domain of the marketing team. Focus' marketing team will utilize IBM Event Processing (EP) to correlate any newly-created customer accounts with first-time purchase orders that total over $100 USD in value. Furthermore, these correlated transactions will need to be identified within a 24-hour window of when they occur. If these conditions can be met, the marketing team will be able to create an actionable plan for putting high-value promotional offers in front of first-time customers in a timely manner \u2014 which should translate to more sales revenue through Focus' digital storefront. With your web browser, access the IBM Event Processing dashboard. Dismiss any welcome screens or tutorials that are prompted after first logging in by clicking the Hide Welcome [A] text in the bottom-left corner of the interface. In the bottom-right corner of the page, click the blue Create [B] to begin defining a new flow . EVENT PROCESSING FLOWS An event processing flow ingests a stream of events, analyzes that input, and takes automatic actions (defined by the user) in response to those conditions. A flow begins with one or multiple event sources which represent inbound events. For the purposes of this demonstration, you will leverage the ORDERS event stream that was defined in the previous module. You will do so using EP's intuitive, no-code authoring canvas. For Name [A] , specify NewCustomerLargeOrder and then click Create [B] .","title":"Using IBM Event Processing no-code editors to configure the solution"},{"location":"3/#configuring-an-event-source-for-orders","text":"Before the inbound events can be captured, connectivity details for the event source node ( ORDERS ) must be configured within the authoring canvas. Wait until the page has refreshed to display the no-code environment. Along the left side are tools and actions that can be dragged into the authoring canvas on the right Within the canvas, you will see an Event source [A] node with default name of source_1 With your cursor, hover over the source_1 node and then click the Edit [B] (pencil) icon in the top-right corner of the tile to begin modifying the node The Configure event source wizard will load within your browser. Under the Add an event source or pick a recent one header, select Add event source [A] Click Next [B] Switch back to the Event Endpoint Management tab and click Catalog . Drill down into the ORDERS [A] topic by clicking on the name Scroll down the topic summary page until you reach the Access Information section Just below, within the Servers table, is the full address of the ORDERS event topic Copy the full address ( ademo-event-gw-ibm... ) [B] to your notepad for reference later When ready, switch back to the IBM Event Processing tab with your web browser Back within EP: the first tab, Details , needs additional information to configure the node. Scroll down to the Node name field and enter Orders Further down, under the Connect to Kafka cluster header, paste the full address copied in Step 6 into the Server [A] field Click Next The event source must be configured to allow certificates used by the event stream. Accept certificates [B] (by clicking the toggle button as shown in the screenshot below) Click Next [C] The second tab, Access credentials , requires authentication details for the EEM event source. Keep the Select security mechanism variable as its default value: PLAIN For Username [A] and Password [B] , supply the credentials for the ORDERS topic (Step 32 of Module 2) Click Next [C] The third tab, Topic selection , asks for which topic(s) to ingest from the event source Available topics will already be displayed within the configuration tool Under the Topic selection header, click the button next to the ORDERS [A] topic to select it Click Next [B] The fourth tab, Define event structure , will automatically recognize the expected JSON data structure of events ingested from the event source. Message format may be left as the JSON value Compare the JSON sample message field, ingested from one of the captured Kafka event messages, with the message structure format recorded in the previous module (below) When ready, click Next to proceed From the Key and Headers tab, you will NOT need to map key and headers to event fields: simply click Next to continue Note that this is the same sample schema you supplied when configuring the event source within EEM in Module 2. { \"quantity\" : 9 , \"price\" : 197.09 , \"customerid\" : \"a7d1586b-ced1-462f-9e44-14e9e5013540\" , \"description\" : \"Composite Oversize 28in Tennis Racket\" , \"id\" : \"1eba7af9-b748-4754-b750-3459e589dccf\" , \"region\" : \"EMEA\" , \"ordertime\" : \"2023-10-24 19:26:04.839\" , \"customer\" : \"Reed McKenzie DDS\" } You will have a final opportunity to review and confirm the event source configuration. Set the Node name [A] to Orders Click Configure [B] to finalize the configuration of the Orders event source","title":"Configuring an event source for Orders"},{"location":"3/#configuring-an-event-source-for-new-customers","text":"The configurations performed for the ORDERS topic now (Steps 4 \u2013 12) must now be repeated for the CUSTOMERS topic. Return to the NewCustomerLargeOrder [A] canvas and drag a new Event Source [B] (from the left-hand interface) into the canvas, placing it just below the Orders source. With your cursor, hover over the new source_1 node and then click the Edit (pencil) icon in the top-right corner of the tile to begin modifying the node. The Configure event source wizard will load within your browser. Under the Add an event source or pick a recent one header, select Add event source [A] Click Next [B] Switch back to the Event Endpoint Management tab and click Catalog . Drill down into the CUSTOMERS topic by clicking on the name Scroll down the topic summary page until you reach the Access Information section Just below, within the Servers table, is the full address of the CUSTOMERS event topic Copy the full address ( ademo-event-gw-ibm... ) to your notepad for reference later When ready, switch back to the IBM Event Processing tab with your web browser Back within EP: the first tab, Details , needs additional information to configure the node. Scroll down to the Node name [A] field and enter New Customers Further down, under the Connect to Kafka cluster header, paste the full address copied in Step 15 into the Server [B] field Click Next [C] The event source must be configured to allow certificates used by the event stream. Accept certificates (by clicking the toggle button as shown in the screenshot below) Click Next The second tab, Access credentials , requires authentication details for the EEM event source. Keep the Select security mechanism variable as its default value: PLAIN For Username and Password , supply the credentials for the CUSTOMERS topic (Step 28 of Module 2) Click Next The third tab, Topic selection , will ask which topic(s) to ingest from the event source. Available topics will already be displayed within the configuration tool Under the Topic selection header, click the button next to the CUSTOMERS [A] topic to select it Click Next [B] The fourth tab, Define event structure , will automatically recognize the expected JSON data structure of events ingested from the event source. Message format may be left as the JSON value Compare the JSON sample message field, ingested from one of the captured Kafka event messages, with the message structure format recorded in the previous module (below) When ready, click Next to proceed From the Key and Headers tab, you will NOT need to map key and headers to event fields: simply click Next to continue Note that this is the same sample schema you supplied when configuring the event source within EEM in Module 2 { \"customerid\" : \"acb3eb65-98a1-45c2-84d4-f5df157862b4\" , \"customername\" : \"Emilio Quitzon\" , \"registered\" : \"2023-10-24 19:20:35.638\" } You will have a final opportunity to review and confirm the event source configuration. Set the Node name [A] field to New Customers Click Configure [A] to finalize the configuration of the New Customers event source","title":"Configuring an event source for New Customers"},{"location":"3/#filtering-event-sources-to-match-specific-criteria","text":"With the event source fully configured within EP, you can now begin tailoring the event processing flow to filter on orders where the value exceeds $100 USD. Scroll down within the list of nodes and actions on the left-side interface until you locate the Filter node. Click and hold the Filter [A] node, then drag it to the authoring canvas on the right Position it to the right side of the Orders node The output terminal for Orders needs to be connected as input to the filter_1 node. With your mouse pointer, hover over the Orders node and take note of the grey dot (labelled Output Port [B] ) at the right edge of the tile Click and hold the Output Port (right-most) of Orders and drag it to the Input Port [C] (left-most edge) of filter_1 , then release your hold to complete the circuit With the Orders event source node connected to the filter_1 operation node, you must utilize the Filter node's expression builder to remove orders that are less than $100 USD in value. This will allow the marketing team's application to target only first-time customers with spending patterns of $100 USD or more. Hover over the filter_1 node with your mouse pointer and click the Edit [D] (pencil) icon. Within the Details tab, specify the value of Node name as FilterLargeOrders When ready, click Next . The Define filter tab will load next. To the right of the Filter expression field, click the Assistant [A] drop-down menu to expose additional options. Event Processing's expression builder will guide you in defining the filter. Property to filter on: price Condition: Is greater than Value: 100 Click the Add to expression button The expression builder will automatically translate your statements into a syntactically-correct filter statement: 'price' > 100 Click the Configure [B] button at the bottom of the panel to finalize the filter configuration","title":"Filtering event sources to match specific criteria"},{"location":"3/#performing-a-join-across-two-event-sources","text":"The marketing team needs to \"JOIN\" the filtered Order stream data to records of New Customers in order to correctly identify first-time purchasers that placed an order for over $100 USD in the last 24 hours. Use EP's authoring canvas to carry out the necessary JOIN operation. Back on the canvas for NewCustomerLargeOrders , scroll down along the left-hand interface until you locate the Interval Join [A] node. Drag and drop this node [B] to the far right-hand edge of the authoring canvas. With your cursor, hover over the New Customers output terminal and locate the Output Port (grey) button. Click and hold the New Customers Output Port [A] edge and drag this to the Input Port [B] edge of the newly-created intervalJoin_1 node Release your cursor to finalize the circuit Repeat the process, this time connecting the FilterLargeOrders node's Output Port [A] to the intervalJoin_1 node's Input Port [B] . At this stage, the input of intervalJoin_1 should be connected to the outputs of both the FilterLargeOrders and New Customers nodes. The JOIN node must now be configured to correlate events based on a shared customerid field within the two event streams ( New Customers and pre-filtered Orders ). With your cursor, hover over the intervalJoin_1 node and click the Edit (pencil) icon. Within the Details tab, set Node name [A] equal to DetectNewCustomerLargeOrders When ready, click Next [B] . Within the Join conditions tab, activate the expression builder by click the Assistant [A] drop-down menu. Set the following properties: Specify property from 'New Customers': customerid Specify property from 'FilterLargeOrders': customerid When ready, click Add to expression [B] The syntactically-correct JOIN expression will be expressed under the Define events field: 'New Customers'.'customerid' = 'FilterLargeOrders'.'customerid' After you have reviewed the JOIN condition, click Next [C] TRIGGERING EVENTS AND DETECTED EVENTS Two concepts are important to understand at this time: Triggering events and Detected events . Since the New Customer sign-up event must logically occur before a purchase, it is considered a triggering event Therefore, when a signal for an Order purchase of over $100 (the detected event ) is received, the JOIN logic condition will be triggered for the two event streams Under the Time window condition , you can define the time interval \"window\" where detected events are considered viable for a promotional offer from the marketing team. To meet the criteria, a purchase of over $100 USD must be made by a first-time customer within a 24 hour window of creating an account. Look for the Event to detect [A] field and select the FilterLargeOrders (event_time) option from the drop-down menu A Preview visualization of the time window interval is rendered within the panel \u2014 for now, with no adjustments made, this will appear as a vertical bar at 0 (on the X-axis) Scroll down further to reveal additional attributes to edit Event to set the time window : New Customers (event-time) Offset from event to start the time window : 0 HOUR(S) Offset from event to end the time window [B] : 24 HOUR(S) When ready, click Next [C] The JOIN node's output will be a combination of fields from both the filtered Orders and New Customers events streams. Before finalizing the interval join, you need to clean up the output so that duplicate fields (like customerid and event_time ) are not included from the JOIN operation. You have the option of renaming or removing the duplicate fields \u2014 but for the sake of this demonstration, you will be removing the fields. Within the Output properties tab, locate the customerid [A] field. There are two (duplicate) copies of the same field at this time. Your objective will be to delete ONE of the two duplicate records, where the Source is labelled as New Customers . Preserve the record where Source is labelled as FilterLargeOrders . Click the round \u2014 sign to the left of the first customerid row, where the Source is labelled as \"New Customers\", to remove the duplicate The result will be only one copy of the customerid field in the table The red text ( Must be unique ) underlining the field will disappear from the table ONLY REMOVE DUPLICATES WHERE SOURCE == NEW CUSTOMERS When reviewing Output properties for duplicate fields, take note of the table's Source column. Duplicates will be produced by both the New Customers and FilterLargeOrders nodes. Only remove duplicates where the Source is listed as New Customers . Repeat the procedure for the event_time field, which is also duplicated by the JOIN: Delete the event_time row where the Source column is labelled as \"New Customers\" With all duplicates removed from the table, the Configure [A] button will now be highlighted in blue and selectable Click the Configure button to finalize the interval join configuration","title":"Performing a JOIN across two event sources"},{"location":"3/#_2","text":"","title":""},{"location":"3/#next-steps","text":"With filtering and processing steps now in place for incoming event streams, Focus Corporation's marketing team is able to detect valid promotional opportunities and emit those detected events to their customer loyalty application. In the following section, you will create those linkages and validate its effectiveness using \"live\" data feeds.","title":"Next steps"},{"location":"4/","text":"Connecting real-time events to a promotional marketing application Christopher Bienko (Principal, IBM Global Sales Enablement) provides a hands-on demonstration of Module 4's contents. Additional ways to watch: Seismic replay available for download. [4 minutes] It's finally time to unify the hard work of Focus Corporation's integration and marketing teams. With a rich feed of filtered and processed event streams, which joins together New Customers and Orders data feeds, the marketing department can begin supplying their customer loyalty application with input. When new events are detected which match the marketing team's promotional criteria, the application will automatically trigger an upsell activity for the new customer. Return to the authoring canvas dashboard for NewCustomerLargeOrders within the IBM Event Processing (EP) tab. From the list of nodes along the left-side interface, drag and drop the Event destination [A] node into the canvas. Place it far to the right of the other nodes within the table [B] . You can use your mouse wheel to resize the table if necessary. The new node sink_1 represents a \"resource\", a concept from Apache Kafka. These resources can receive incoming events \u2014 thereby serving as a destination . With your cursor, hover over the DetectNewCustomerLargeOrders node that was created in the previous module. Click and drag the node's Output Port edge, connecting it to the Input Port edge of the sink_1 node Release the mouse button to establish the connection between the two nodes Hover over the sink_1 node and click the Edit [A] (pencil) icon to configure the destination. Within the Details tab, adjust the following variables: Node name [A] : OutputToMarketingApp Server [B] : ademo-es-kafka-bootstrap.cp4i.svc:9095 When ready, click Next [C] The server has been pre-configured as part of the hands-on lab Setup module, generating the Apache Kafka events that will simulate both \"historical\" and \"live\" customer interactions with Focus' online store and payment gateway. Select the Accept certificates [B] toggle when asked if you wish to trust all security certificates issued by the event stream source. When ready, click Next [C] . Within the Access credentials tab, modify the following variables: Username [A] : es-admin Password [B] : the password unique to your Event Streams instance When ready, click Next [C] Within the Topic selection tab, you will be asked to specify which of the available endpoints topics should serve as a destination \"sink.\" In this situation, the goal is to send all valid customer interactions (which meet the purchasing criteria) to the marketing department's customer loyalty application. Select the LOYALTY.APP topic [A] - you may need to tab through the pages (3 total), or search for the Topic Name within the search bar provided at the top of the panel Click Configure [B] to finalize the event destination You are now ready to begin testing the end-to-end flow using historical event data. This will allow the marketing and integration teams to see the results of the flow and validate the results. If successful, several new customers who are eligible for the promotional discount should be detected and flagged within the loyalty application. Return to the authoring canvas dashboard for NewCustomerLargeOrders within EP. In the top-right corner of the interface, click the Run drop-down menu From the list of options, select Include historical [A] Detected events will begin automatically populating the Output Events panel, along the bottom of the page. It will take a few moments for the events to begin populating the table After several hours, the table will be populated with pages of entries (cycle through these using the arrows or drop-down page selector) Continue to verify the detected events until satisfied with the historical event data results When ready, stop the flow by clicking the Stop button in the top-right corner of the page TROUBLESHOOTING \u2014 UNABLE TO CONNECT TO RESULTS You may experience a pop-up window in the top-right corner of the authoring canvas after initiating a Run execution. This is expected given that not all of the Kafka topic endpoints were configured \u2014 you only configured the LOYALTY.APP endpoint, as well as the ORDERS and CUSTOMERS event streams. You can safely ignore this warning and click the X icon to dismiss it. Conclusion Having successfully tested the flow, Focus Corporation is ready to reconfigure the event destination sink and begin feeding live event stream data into their production environment. Afterwards, it will immediately begin generating actionable promotions within Focus Corporation's customer loyalty application. Congratulations on a successful demonstration! This concludes the hands-on material. At this time, you should be ready to record a Stand & Deliver presentation or complete the business partner skills evaluation quiz \u2014 follow the Evaluation steps appropriate to your job role.","title":"4. Connecting real-time events to a promotional marketing app"},{"location":"4/#connecting-real-time-events-to-a-promotional-marketing-application","text":"Christopher Bienko (Principal, IBM Global Sales Enablement) provides a hands-on demonstration of Module 4's contents. Additional ways to watch: Seismic replay available for download. [4 minutes] It's finally time to unify the hard work of Focus Corporation's integration and marketing teams. With a rich feed of filtered and processed event streams, which joins together New Customers and Orders data feeds, the marketing department can begin supplying their customer loyalty application with input. When new events are detected which match the marketing team's promotional criteria, the application will automatically trigger an upsell activity for the new customer. Return to the authoring canvas dashboard for NewCustomerLargeOrders within the IBM Event Processing (EP) tab. From the list of nodes along the left-side interface, drag and drop the Event destination [A] node into the canvas. Place it far to the right of the other nodes within the table [B] . You can use your mouse wheel to resize the table if necessary. The new node sink_1 represents a \"resource\", a concept from Apache Kafka. These resources can receive incoming events \u2014 thereby serving as a destination . With your cursor, hover over the DetectNewCustomerLargeOrders node that was created in the previous module. Click and drag the node's Output Port edge, connecting it to the Input Port edge of the sink_1 node Release the mouse button to establish the connection between the two nodes Hover over the sink_1 node and click the Edit [A] (pencil) icon to configure the destination. Within the Details tab, adjust the following variables: Node name [A] : OutputToMarketingApp Server [B] : ademo-es-kafka-bootstrap.cp4i.svc:9095 When ready, click Next [C] The server has been pre-configured as part of the hands-on lab Setup module, generating the Apache Kafka events that will simulate both \"historical\" and \"live\" customer interactions with Focus' online store and payment gateway. Select the Accept certificates [B] toggle when asked if you wish to trust all security certificates issued by the event stream source. When ready, click Next [C] . Within the Access credentials tab, modify the following variables: Username [A] : es-admin Password [B] : the password unique to your Event Streams instance When ready, click Next [C] Within the Topic selection tab, you will be asked to specify which of the available endpoints topics should serve as a destination \"sink.\" In this situation, the goal is to send all valid customer interactions (which meet the purchasing criteria) to the marketing department's customer loyalty application. Select the LOYALTY.APP topic [A] - you may need to tab through the pages (3 total), or search for the Topic Name within the search bar provided at the top of the panel Click Configure [B] to finalize the event destination You are now ready to begin testing the end-to-end flow using historical event data. This will allow the marketing and integration teams to see the results of the flow and validate the results. If successful, several new customers who are eligible for the promotional discount should be detected and flagged within the loyalty application. Return to the authoring canvas dashboard for NewCustomerLargeOrders within EP. In the top-right corner of the interface, click the Run drop-down menu From the list of options, select Include historical [A] Detected events will begin automatically populating the Output Events panel, along the bottom of the page. It will take a few moments for the events to begin populating the table After several hours, the table will be populated with pages of entries (cycle through these using the arrows or drop-down page selector) Continue to verify the detected events until satisfied with the historical event data results When ready, stop the flow by clicking the Stop button in the top-right corner of the page TROUBLESHOOTING \u2014 UNABLE TO CONNECT TO RESULTS You may experience a pop-up window in the top-right corner of the authoring canvas after initiating a Run execution. This is expected given that not all of the Kafka topic endpoints were configured \u2014 you only configured the LOYALTY.APP endpoint, as well as the ORDERS and CUSTOMERS event streams. You can safely ignore this warning and click the X icon to dismiss it.","title":"Connecting real-time events to a promotional marketing application"},{"location":"4/#_1","text":"","title":""},{"location":"4/#conclusion","text":"Having successfully tested the flow, Focus Corporation is ready to reconfigure the event destination sink and begin feeding live event stream data into their production environment. Afterwards, it will immediately begin generating actionable promotions within Focus Corporation's customer loyalty application. Congratulations on a successful demonstration! This concludes the hands-on material. At this time, you should be ready to record a Stand & Deliver presentation or complete the business partner skills evaluation quiz \u2014 follow the Evaluation steps appropriate to your job role.","title":"Conclusion"},{"location":"evaluation/","text":"Evaluation Criteria for IBM Technical Sellers and Business Partners To receive a Level 3 badge ( IBM Event Automation for Technical Sales Intermediate ), IBMers and Business Partners must demonstrate mastery of the skills learned throughout the various modules of these hands-on labs and coursework. Level 3 skills requirements\u2014 and the way participants will be evaluated \u2014differs depending on job role. IBM TECHNICAL SELLERS IBM technical sellers must develop and record a Stand & Deliver presentation, which will be uploaded to the IBM Stand & Deliver platform for evaluation. This video is intended to simulate your delivery of a \u201clive\u201d demo in front of a client \u2014 on camera. IBMers will have flexibility in defining a hypothetical client, the pain points that customer has, and the goals they aspire to. The recording will then cover the seller\u2019s hands-on demonstration and pitch to the client of the value of the IBM solution using the environments and techniques of this lab. BUSINESS PARTNERS Business partners must pass a skills evaluation quiz after completing the hands-on portion of the course. The quiz consists of multiple choice questions, with four possible responses (and only one correct answer) for each question. Participants must pass the quiz with a grade of 80% or higher. The quiz questions will ask you about on-screen text or descriptions that come up as you work through the lab guide. Answers to quiz questions can only be determined by carefully following the instructions of the hands-on lab. Take note that any materials marked as OPTIONAL are not evaluated for Level 3 badges. IBM technical sellers are not required to include optional sections in their Stand & Deliver Business partners are not tested on the contents of optional sections IBMer Stand & Deliver Assessment IBMers \u2014 SUBMIT RECORDINGS HERE Submit your Stand & Deliver recording online using IBM YourLearning: https://yourlearning.ibm.com/activity/ISND-SALESENABLEMENT-2403071100 The evaluation criteria described below only applies to IBMers , who must record a Stand & Deliver to receive credit for this Level 3 course. By default, IBMers will be evaluated by their First Line Manager (FLM) \u2014 although they may request another manager to evaluate their Stand & Deliver, if appropriate. Instructions on how to submit a Stand & Deliver are included within the activity on YourLearning. IBM Technical Sellers need to include all seven of the following elements in their Stand & Deliver recording to receive a Level 3 badge. FLMs are to use these requirements as part of their evaluation criteria. Seller articulated their client's pain point(s) and the value proposition of using IBM Event Automation and IBM Cloud Pak for Integration . Seller highlighted use cases for IBM Event Automation . It is recommended that sellers address the challenges of real-time access for meeting client demands \u2014 and how event-led integration solutions from IBM can overcome these obstacles. Seller must demonstrate how to take existing IBM MQ message queues and publish those as Kafka-based event streams using IBM Cloud Pak for Integration and IBM Event Streams . Seller must utilize IBM Event Endpoint Management to import the newly-created event streams topics and show how the service provides a self-service catalog for managing those topics. Seller must show how IBM Event Processing can correlate any newly-created customer accounts with first-time purchase orders that total over $100 USD in value. They should include in this demonstration an explanation about how to define event sources, destinations, filters, and JOIN interval operations using the no-code authoring canvas. Seller must connect historical data to the LOYALTY.APP application endpoint to validate the end-to-end configurations made so far and simulate how \"live\" event data is being processed by the IBM Event Automation suite. Sellers should use the Run > Include Historical test execution inside IBM Event Processing to demonstrate this. Seller closed the demonstration with a call to action for their client that could include: a workshop, a deeper dive into the product meeting, or Proof of Experience (PoX) engagement. Business Partner Quiz Assessment PARTNERS \u2014 COMPLETE ASSESSMENT HERE Complete your assessment online using IBM Training: https://learn.ibm.com/course/view.php?id=16334 The skills evaluation quiz consists of multiple choice questions, with four possible responses (and only one correct answer) for each question. The quiz questions will ask you about on-screen text or descriptions that come up as you work through the lab guide. Participants must pass the quiz with a grade of 80% or higher. Answers to the quiz can only be determined by carefully following the instructions of the hands-on lab. Sections of the lab marked as OPTIONAL will not be tested on as part of the quiz. Next Steps In the following module, you will create an event stream from a message queue using IBM MQ .","title":"Evaluation"},{"location":"evaluation/#evaluation-criteria-for-ibm-technical-sellers-and-business-partners","text":"To receive a Level 3 badge ( IBM Event Automation for Technical Sales Intermediate ), IBMers and Business Partners must demonstrate mastery of the skills learned throughout the various modules of these hands-on labs and coursework. Level 3 skills requirements\u2014 and the way participants will be evaluated \u2014differs depending on job role. IBM TECHNICAL SELLERS IBM technical sellers must develop and record a Stand & Deliver presentation, which will be uploaded to the IBM Stand & Deliver platform for evaluation. This video is intended to simulate your delivery of a \u201clive\u201d demo in front of a client \u2014 on camera. IBMers will have flexibility in defining a hypothetical client, the pain points that customer has, and the goals they aspire to. The recording will then cover the seller\u2019s hands-on demonstration and pitch to the client of the value of the IBM solution using the environments and techniques of this lab. BUSINESS PARTNERS Business partners must pass a skills evaluation quiz after completing the hands-on portion of the course. The quiz consists of multiple choice questions, with four possible responses (and only one correct answer) for each question. Participants must pass the quiz with a grade of 80% or higher. The quiz questions will ask you about on-screen text or descriptions that come up as you work through the lab guide. Answers to quiz questions can only be determined by carefully following the instructions of the hands-on lab. Take note that any materials marked as OPTIONAL are not evaluated for Level 3 badges. IBM technical sellers are not required to include optional sections in their Stand & Deliver Business partners are not tested on the contents of optional sections","title":"Evaluation Criteria for IBM Technical Sellers and Business Partners"},{"location":"evaluation/#_1","text":"","title":""},{"location":"evaluation/#ibmer-stand-deliver-assessment","text":"IBMers \u2014 SUBMIT RECORDINGS HERE Submit your Stand & Deliver recording online using IBM YourLearning: https://yourlearning.ibm.com/activity/ISND-SALESENABLEMENT-2403071100 The evaluation criteria described below only applies to IBMers , who must record a Stand & Deliver to receive credit for this Level 3 course. By default, IBMers will be evaluated by their First Line Manager (FLM) \u2014 although they may request another manager to evaluate their Stand & Deliver, if appropriate. Instructions on how to submit a Stand & Deliver are included within the activity on YourLearning. IBM Technical Sellers need to include all seven of the following elements in their Stand & Deliver recording to receive a Level 3 badge. FLMs are to use these requirements as part of their evaluation criteria. Seller articulated their client's pain point(s) and the value proposition of using IBM Event Automation and IBM Cloud Pak for Integration . Seller highlighted use cases for IBM Event Automation . It is recommended that sellers address the challenges of real-time access for meeting client demands \u2014 and how event-led integration solutions from IBM can overcome these obstacles. Seller must demonstrate how to take existing IBM MQ message queues and publish those as Kafka-based event streams using IBM Cloud Pak for Integration and IBM Event Streams . Seller must utilize IBM Event Endpoint Management to import the newly-created event streams topics and show how the service provides a self-service catalog for managing those topics. Seller must show how IBM Event Processing can correlate any newly-created customer accounts with first-time purchase orders that total over $100 USD in value. They should include in this demonstration an explanation about how to define event sources, destinations, filters, and JOIN interval operations using the no-code authoring canvas. Seller must connect historical data to the LOYALTY.APP application endpoint to validate the end-to-end configurations made so far and simulate how \"live\" event data is being processed by the IBM Event Automation suite. Sellers should use the Run > Include Historical test execution inside IBM Event Processing to demonstrate this. Seller closed the demonstration with a call to action for their client that could include: a workshop, a deeper dive into the product meeting, or Proof of Experience (PoX) engagement.","title":"IBMer Stand &amp; Deliver Assessment"},{"location":"evaluation/#_2","text":"","title":""},{"location":"evaluation/#business-partner-quiz-assessment","text":"PARTNERS \u2014 COMPLETE ASSESSMENT HERE Complete your assessment online using IBM Training: https://learn.ibm.com/course/view.php?id=16334 The skills evaluation quiz consists of multiple choice questions, with four possible responses (and only one correct answer) for each question. The quiz questions will ask you about on-screen text or descriptions that come up as you work through the lab guide. Participants must pass the quiz with a grade of 80% or higher. Answers to the quiz can only be determined by carefully following the instructions of the hands-on lab. Sections of the lab marked as OPTIONAL will not be tested on as part of the quiz.","title":"Business Partner Quiz Assessment"},{"location":"evaluation/#_3","text":"","title":""},{"location":"evaluation/#next-steps","text":"In the following module, you will create an event stream from a message queue using IBM MQ .","title":"Next Steps"},{"location":"setup-2/","text":"Prerequisites and Setup SETUP TIME ESTIMATES The following section will take approximately 90-120 minutes to complete. Much of the setup work is fully automated and will allow you to perform other tasks or errands while it is underway. Budget your time accordingly. SUPPORT AND TROUBLESHOOTING If you require assistance or run into issues with the hands-on lab, help is available. Environment issues : The lab environment is managed by IBM Technology Zone. Opening a support case ticket is recommended for issues related to the hands-on environment (provisioning, running, and so on.) Documentation issues : If there is an error in the lab documentation, or if you require additional support in completing the material, open a thread on the #l3-support-app-modernization Slack channel. The hands-on material covered within this lab will focus on event-led integration solutions, specifically the IBM Event Automation suite \u2014 as well as elements of the IBM Cloud Pak for Integration (MQ-Kafka connector.) IBM Event Automation provides an open, composable set of capabilities that interoperate with the services and tools that are already part of many enterprise IT departments. A fundamental component to IBM Event Automation's architecture is composability, which will be highlighted through the lab as you incrementally add more capabilities and elements to the demo environment. Event Endpoint Management : facilitates secured exposure of event streams and unite all event streams (including MQ and Kafka) into a single, comprehensive view. Focus' marketing department can discover, subscribe to, and generate security credentials for colleagues using the self-service catalog. Manual processes will be automated and onboarding of users can be expedited. Event Processing : a no-code editor for allowing developers (of any skill level) to construct processing flows, detect situations across multiple events inside a configurable time-window, and validate flow behavior prior to deploying into production (using historical data.) Event Streams + Kafka-MQ connector : part of the IBM Cloud Pak for Integration suite, this connector allows IBM MQ messages to be published into a Kafka-compatible event stream. It provides assurances that all MQ messages are delivered once-and-only-once, without requiring complex or custom code. Environment templates (images) have been defined ahead of time with the assistance of IBM Technology Zone (ITZ), which can be reserved for learning and client demonstration purposes at no-charge. Some further configuration of these environments will be required after deployment. Request an IBM Technology Zone Environment You will require access to the ITZ in order to reserve your environment and complete the lab. If you do not yet have access or an account with the ITZ, you will need to register for one . The hands-on environment\u2014 a combination of services from IBM Cloud Pak for Integration (CPI) and IBM Event Automation (EA) \u2014will be running entirely atop of Red Hat OpenShift , which will orchestrate containerized deployments of CPI and EA across the OpenShift cluster. Reserve a Red Hat OpenShift cluster via the IBM Technology Zone pre-defined template. URL: https://techzone.ibm.com/my/reservations/create/63a3a25a3a4689001740dbb3 From the Single environment reservation options list, select Reserve now [A] . Supply additional details about your ITZ reservation request: Name [A] : Give your reservation a unique name. Purpose [B] : Set to Practice / Self-Education and affirm that customer data will not be used with the environment. If you are replicating this hands-on demonstration with a client, you must select Customer Demo and supply a sales opportunity number. Purpose Description [C] : Provide a brief summary of how the environment will be used. Preferred Geography [D] : Select the data center region that is closest to your location. End Date & Time [E] : Select a time and date for when the reservation will expire. The recommended amount is 2 days, although it is possible to finish the hands-on demonstration within a few hours. Additional time extensions are available. OCP/Kubernetes Cluster Network [F] : Default value of 10.128.0.0/14 Enable FIPS Security : Default value of No OpenShift Version [G] : Set to 4.14 Storage [H] : Set to ODF - 2 TB OCP/Kubernetes Service Network [I] : Default value of 172.30.0.0/16 Worker Node Count : Default value of 3 Worker Node Flavor [J] : Set to 16 vCPU x 64 GB - 100 GB ephemeral storage When satisfied, verify that you agree to the Terms and Conditions for the environment and finalize your reservation request by clicking Submit [K] . PROVISIONING TIMES Reservations take approximately 60 minutes to complete from the time that you click submit. If you navigate to the My Reservations tab of the ITZ, you can monitor the progress of your reservation. Wait for the ITZ reservation to be marked as \" Ready \" before attempting to start the lab \u2014 accessing it too soon will lead to issues (or an outright failure) when connecting to the OpenShift cluster. When the ITZ environment is ready to go, you will receive an email to your inbox (similar to the screenshot below). Click the blue View My Reservations [A] at the bottom of the email to open a page with more details about the environment. Alternatively, you can click the My Reservations tab from the ITZ home page to drill down into your environment's details. Details about your OpenShift cluster, including login and authentication details, are summarized on the page. Record the Username [A] ( kubeadmin ) and Password [B] (unique to your environment), as well as the Desktop URL just above those two items, to a notepad for reference later Click Open your IBM Cloud environment [C] to access OpenShift's dashboard You will be prompted to provide the Username and Password recorded in Step 5. Click Log in [A] to proceed Once authenticated, the web browser will redirect to the main dashboard for the OpenShift Container Platform web interface You are now ready to proceed with the lab environment customization for IBM Event Automation and IBM Cloud Pak for Integration THIS CONNECTION IS NOT PRIVATE Depending on your web browser, you may receive a warning about This connection is not private or a similarly phrased message. Disregard these warnings and continue to the intended destination by clicking \"visit this website.\" Customization of the Lab Environment In order to deploy containerized instances of IBM Event Automation and IBM Cloud Pak for Integration on top of OpenShift, you must first download and set up your local machine with the OpenShift Command Line Interface (CLI) . In the top-right corner of the OpenShift dashboard, click the question mark [A] ( ? ) icon as shown in the screenshot below Click the Command line tools [B] option from the drop-down menu COMMAND LINE ACCESS For parts of the hands-on lab, you will be required to establish connections over SSH to remote infrastructure endpoints. It is recommended that users do so via Terminal (Mac) or PuTTY (Windows). For detailed instructions on how to connect with PuTTY, or if you are using Windows, reference the linked material . From the oc - OpenShift Command Line Interface (CLI) options, click the download link that matches your local machine's operating system (Mac, Windows, or Linux). Once downloaded, execute the installer and proceed with the lab guide once oc is installed. Return to the OpenShift dashboard landing page. In the far right-hand corner of the interface, click the kube:admin button to open another drop-down menu From the options, select Copy login command [A] to open a new tab or window with details about remotely accessing the OpenShift cluster A mostly-blank page will load, with Display Token written in blue text in the top-left of the page. Click the link to proceed. Details about a newly-generated API token will load on screen. Record the full command listed under Log in with this token [A] (resembling something similar to oc login --token=sha256... ) and save this to your notepad for later reference. Open your command line tool of preference on your local machine, likely either Terminal or PuTTY . At this stage, the OpenShift CLI ( oc ) should be available on your machine and ready for remotely connecting to the OpenShift cluster. Within the Terminal, paste the full login command recorded in Step 11 and hit Return to commence the login A prompt similar to the screenshot below will be displayed momentarily, indicating that the Terminal is now successfully connected (remotely) to the OpenShift cluster To deploy containerized instances of IBM Event Automation and IBM Cloud Pak for Integration to the OpenShift cluster, you must first download those container images locally and then deploy them remotely on the OpenShift cluster. Execute the following command within your Terminal console to clone the necessary scripts and images to your local machine : Copy and execute: git clone https://github.com/IBM/platinum-demo-code-event-automation.git Navigate to the newly cloned platinum-demo-code-event-automation repository with the following command: cd platinum-demo-code-event-automation WINDOWS USERS Step 15 uses a shell script to deploy IBM Cloud Pak for Integration and IBM Event Automation via OpenShift containers. The shell script can be executed as-written for MacOS and Linux users. However, if you are a Windows user, follow the linked documentation to request a Linux Virtual Machine. You will only need to make use of the VM for the purposes of Step 15 (deploying the demo environment.) Afterwards you may carry on with the lab instructions on your personal Windows device via a web browser. Execute the following command to kick off the process of deploying and configuring the containerized services to the OpenShift cluster : ./deploy.sh Wait for the full deployment to complete before continuing with the lab documentation When ready, each of the lab services\u2014 IBM Cloud Pak for Integration [A] (\"Platform Navigator\"), Event Streams [B] , Event Endpoint Management [C] , Event Processing [D] , and IBM MQ [E] \u2014will be listed to the Terminal window Each listing will be accompanied by a URL , username , and password \u2014 record this information to a notepad for reference later DEPLOYMENT TIMES The deployment of IBM Event Automation and IBM Cloud Pak for Integration containers to the OpenShift cluster is entirely automated using Red Hat Ansible scripts. Full deployment and configuration should take approximately 20-45 minutes to complete. The Platform Navigator and IBM MQ services utilize the same login details. The password supplied for these two services by the Terminal console in Step 15 is a temporary , one-time-use password. Try logging into the IBM MQ environment environment by pasting the URL into a web browser, then enter the username and temporary password recorded earlier When prompted, create your own password for logging into the environment Confirm your selection with Submit [A] and then update your password records for both the Platform Navigator and IBM MQ services At this stage, the lab environment is fully configured and ready for hands-on work MQ & CPI AUTHENTICATION Both services utilize the same username and password credentials. However, the two URLs generated by the Terminal console in Step 15 are unique . Ensure that you are logging in to the correct dashboard (IBM MQ or CPI) when asked to do so. With your web browser, open a new tab for each of the services needed for the hands-on lab: IBM Cloud Pak for Integration (CPI) Event Streams Event Endpoint Management Event Processing IBM MQ Keep these tabs open. You will be switching between them frequently. Next Steps The following module will outline the Level 3 evaluation criteria for IBM technical sellers and business partners. Follow the accreditation steps appropriate to your job role.","title":"Prerequisites and Setup"},{"location":"setup-2/#prerequisites-and-setup","text":"SETUP TIME ESTIMATES The following section will take approximately 90-120 minutes to complete. Much of the setup work is fully automated and will allow you to perform other tasks or errands while it is underway. Budget your time accordingly. SUPPORT AND TROUBLESHOOTING If you require assistance or run into issues with the hands-on lab, help is available. Environment issues : The lab environment is managed by IBM Technology Zone. Opening a support case ticket is recommended for issues related to the hands-on environment (provisioning, running, and so on.) Documentation issues : If there is an error in the lab documentation, or if you require additional support in completing the material, open a thread on the #l3-support-app-modernization Slack channel. The hands-on material covered within this lab will focus on event-led integration solutions, specifically the IBM Event Automation suite \u2014 as well as elements of the IBM Cloud Pak for Integration (MQ-Kafka connector.) IBM Event Automation provides an open, composable set of capabilities that interoperate with the services and tools that are already part of many enterprise IT departments. A fundamental component to IBM Event Automation's architecture is composability, which will be highlighted through the lab as you incrementally add more capabilities and elements to the demo environment. Event Endpoint Management : facilitates secured exposure of event streams and unite all event streams (including MQ and Kafka) into a single, comprehensive view. Focus' marketing department can discover, subscribe to, and generate security credentials for colleagues using the self-service catalog. Manual processes will be automated and onboarding of users can be expedited. Event Processing : a no-code editor for allowing developers (of any skill level) to construct processing flows, detect situations across multiple events inside a configurable time-window, and validate flow behavior prior to deploying into production (using historical data.) Event Streams + Kafka-MQ connector : part of the IBM Cloud Pak for Integration suite, this connector allows IBM MQ messages to be published into a Kafka-compatible event stream. It provides assurances that all MQ messages are delivered once-and-only-once, without requiring complex or custom code. Environment templates (images) have been defined ahead of time with the assistance of IBM Technology Zone (ITZ), which can be reserved for learning and client demonstration purposes at no-charge. Some further configuration of these environments will be required after deployment.","title":"Prerequisites and Setup"},{"location":"setup-2/#_1","text":"","title":""},{"location":"setup-2/#request-an-ibm-technology-zone-environment","text":"You will require access to the ITZ in order to reserve your environment and complete the lab. If you do not yet have access or an account with the ITZ, you will need to register for one . The hands-on environment\u2014 a combination of services from IBM Cloud Pak for Integration (CPI) and IBM Event Automation (EA) \u2014will be running entirely atop of Red Hat OpenShift , which will orchestrate containerized deployments of CPI and EA across the OpenShift cluster. Reserve a Red Hat OpenShift cluster via the IBM Technology Zone pre-defined template. URL: https://techzone.ibm.com/my/reservations/create/63a3a25a3a4689001740dbb3 From the Single environment reservation options list, select Reserve now [A] . Supply additional details about your ITZ reservation request: Name [A] : Give your reservation a unique name. Purpose [B] : Set to Practice / Self-Education and affirm that customer data will not be used with the environment. If you are replicating this hands-on demonstration with a client, you must select Customer Demo and supply a sales opportunity number. Purpose Description [C] : Provide a brief summary of how the environment will be used. Preferred Geography [D] : Select the data center region that is closest to your location. End Date & Time [E] : Select a time and date for when the reservation will expire. The recommended amount is 2 days, although it is possible to finish the hands-on demonstration within a few hours. Additional time extensions are available. OCP/Kubernetes Cluster Network [F] : Default value of 10.128.0.0/14 Enable FIPS Security : Default value of No OpenShift Version [G] : Set to 4.14 Storage [H] : Set to ODF - 2 TB OCP/Kubernetes Service Network [I] : Default value of 172.30.0.0/16 Worker Node Count : Default value of 3 Worker Node Flavor [J] : Set to 16 vCPU x 64 GB - 100 GB ephemeral storage When satisfied, verify that you agree to the Terms and Conditions for the environment and finalize your reservation request by clicking Submit [K] . PROVISIONING TIMES Reservations take approximately 60 minutes to complete from the time that you click submit. If you navigate to the My Reservations tab of the ITZ, you can monitor the progress of your reservation. Wait for the ITZ reservation to be marked as \" Ready \" before attempting to start the lab \u2014 accessing it too soon will lead to issues (or an outright failure) when connecting to the OpenShift cluster. When the ITZ environment is ready to go, you will receive an email to your inbox (similar to the screenshot below). Click the blue View My Reservations [A] at the bottom of the email to open a page with more details about the environment. Alternatively, you can click the My Reservations tab from the ITZ home page to drill down into your environment's details. Details about your OpenShift cluster, including login and authentication details, are summarized on the page. Record the Username [A] ( kubeadmin ) and Password [B] (unique to your environment), as well as the Desktop URL just above those two items, to a notepad for reference later Click Open your IBM Cloud environment [C] to access OpenShift's dashboard You will be prompted to provide the Username and Password recorded in Step 5. Click Log in [A] to proceed Once authenticated, the web browser will redirect to the main dashboard for the OpenShift Container Platform web interface You are now ready to proceed with the lab environment customization for IBM Event Automation and IBM Cloud Pak for Integration THIS CONNECTION IS NOT PRIVATE Depending on your web browser, you may receive a warning about This connection is not private or a similarly phrased message. Disregard these warnings and continue to the intended destination by clicking \"visit this website.\"","title":"Request an IBM Technology Zone Environment"},{"location":"setup-2/#_2","text":"","title":""},{"location":"setup-2/#customization-of-the-lab-environment","text":"In order to deploy containerized instances of IBM Event Automation and IBM Cloud Pak for Integration on top of OpenShift, you must first download and set up your local machine with the OpenShift Command Line Interface (CLI) . In the top-right corner of the OpenShift dashboard, click the question mark [A] ( ? ) icon as shown in the screenshot below Click the Command line tools [B] option from the drop-down menu COMMAND LINE ACCESS For parts of the hands-on lab, you will be required to establish connections over SSH to remote infrastructure endpoints. It is recommended that users do so via Terminal (Mac) or PuTTY (Windows). For detailed instructions on how to connect with PuTTY, or if you are using Windows, reference the linked material . From the oc - OpenShift Command Line Interface (CLI) options, click the download link that matches your local machine's operating system (Mac, Windows, or Linux). Once downloaded, execute the installer and proceed with the lab guide once oc is installed. Return to the OpenShift dashboard landing page. In the far right-hand corner of the interface, click the kube:admin button to open another drop-down menu From the options, select Copy login command [A] to open a new tab or window with details about remotely accessing the OpenShift cluster A mostly-blank page will load, with Display Token written in blue text in the top-left of the page. Click the link to proceed. Details about a newly-generated API token will load on screen. Record the full command listed under Log in with this token [A] (resembling something similar to oc login --token=sha256... ) and save this to your notepad for later reference. Open your command line tool of preference on your local machine, likely either Terminal or PuTTY . At this stage, the OpenShift CLI ( oc ) should be available on your machine and ready for remotely connecting to the OpenShift cluster. Within the Terminal, paste the full login command recorded in Step 11 and hit Return to commence the login A prompt similar to the screenshot below will be displayed momentarily, indicating that the Terminal is now successfully connected (remotely) to the OpenShift cluster To deploy containerized instances of IBM Event Automation and IBM Cloud Pak for Integration to the OpenShift cluster, you must first download those container images locally and then deploy them remotely on the OpenShift cluster. Execute the following command within your Terminal console to clone the necessary scripts and images to your local machine : Copy and execute: git clone https://github.com/IBM/platinum-demo-code-event-automation.git Navigate to the newly cloned platinum-demo-code-event-automation repository with the following command: cd platinum-demo-code-event-automation WINDOWS USERS Step 15 uses a shell script to deploy IBM Cloud Pak for Integration and IBM Event Automation via OpenShift containers. The shell script can be executed as-written for MacOS and Linux users. However, if you are a Windows user, follow the linked documentation to request a Linux Virtual Machine. You will only need to make use of the VM for the purposes of Step 15 (deploying the demo environment.) Afterwards you may carry on with the lab instructions on your personal Windows device via a web browser. Execute the following command to kick off the process of deploying and configuring the containerized services to the OpenShift cluster : ./deploy.sh Wait for the full deployment to complete before continuing with the lab documentation When ready, each of the lab services\u2014 IBM Cloud Pak for Integration [A] (\"Platform Navigator\"), Event Streams [B] , Event Endpoint Management [C] , Event Processing [D] , and IBM MQ [E] \u2014will be listed to the Terminal window Each listing will be accompanied by a URL , username , and password \u2014 record this information to a notepad for reference later DEPLOYMENT TIMES The deployment of IBM Event Automation and IBM Cloud Pak for Integration containers to the OpenShift cluster is entirely automated using Red Hat Ansible scripts. Full deployment and configuration should take approximately 20-45 minutes to complete. The Platform Navigator and IBM MQ services utilize the same login details. The password supplied for these two services by the Terminal console in Step 15 is a temporary , one-time-use password. Try logging into the IBM MQ environment environment by pasting the URL into a web browser, then enter the username and temporary password recorded earlier When prompted, create your own password for logging into the environment Confirm your selection with Submit [A] and then update your password records for both the Platform Navigator and IBM MQ services At this stage, the lab environment is fully configured and ready for hands-on work MQ & CPI AUTHENTICATION Both services utilize the same username and password credentials. However, the two URLs generated by the Terminal console in Step 15 are unique . Ensure that you are logging in to the correct dashboard (IBM MQ or CPI) when asked to do so. With your web browser, open a new tab for each of the services needed for the hands-on lab: IBM Cloud Pak for Integration (CPI) Event Streams Event Endpoint Management Event Processing IBM MQ Keep these tabs open. You will be switching between them frequently.","title":"Customization of the Lab Environment"},{"location":"setup-2/#_3","text":"","title":""},{"location":"setup-2/#next-steps","text":"The following module will outline the Level 3 evaluation criteria for IBM technical sellers and business partners. Follow the accreditation steps appropriate to your job role.","title":"Next Steps"},{"location":"setup/","text":"TIME ESTIMATES FOR SETUP The following section will take approximately 3 to 4 hours to complete. Budget your time accordingly. Roughly 30 to 60 minutes for provisioning of the Red Hat OpenShift (OCP) cluster from IBM Technology Zone An estimated 3 hours of time to manually install and deploy IBM Event Automation (and IBM Cloud Pak for Integration) on to the OCP cluster SUPPORT AND TROUBLESHOOTING If you require assistance or run into issues with the hands-on lab, help is available. Environment issues : The lab environment is managed by IBM Technology Zone. Opening a support case ticket is recommended for issues related to the hands-on environment (provisioning, running, and so on.) Documentation issues : If there is an error in the lab documentation, or if you require additional support in completing the material, open a thread on the #l3-support-app-modernization Slack channel. The hands-on material covered within this lab will focus on event-led integration solutions, specifically the IBM Event Automation (EA) suite \u2014 as well as elements of the IBM Cloud Pak for Integration (CP4I) MQ-Kafka connector. IBM Event Automation provides an open, composable set of capabilities that interoperate with the services and tools that are already part of many enterprise IT departments. A fundamental component to IBM Event Automation's architecture is composability, which will be highlighted through the lab as you incrementally add more capabilities and elements to the demo environment. Event Endpoint Management : facilitates secured exposure of event streams and unite all event streams (including MQ and Kafka) into a single, comprehensive view. Focus' marketing department can discover, subscribe to, and generate security credentials for colleagues using the self-service catalog. Manual processes will be automated and onboarding of users can be expedited. Event Processing : a no-code editor for allowing developers (of any skill level) to construct processing flows, detect situations across multiple events inside a configurable time-window, and validate flow behavior prior to deploying into production (using historical data.) Event Streams + Kafka-MQ connector : part of the IBM Cloud Pak for Integration suite, this connector allows IBM MQ messages to be published into a Kafka-compatible event stream. It provides assurances that all MQ messages are delivered once-and-only-once, without requiring complex or custom code. Environment templates (images) have been defined ahead of time with the assistance of IBM Technology Zone (ITZ), which can be reserved for learning and client demonstration purposes at no-charge. Some further configuration of these environments will be required after deployment. 1. Prerequisites This product is being developed and continuously updated in an agile manner. In addition to adding new capabilities, upgrades to the product are likely to result in changes to the user interface over time. As such, the screenshots used in this lab may not always look exactly like what you see in the latest version of product. These differences should not affect how the lab is executed, and the authoring team will do their best to keep things up to date, but be aware that screenshots or steps may differ slightly with your environment. Before reserving an environment with ITZ, first ensure that all of the following services and frameworks are available on your local machine. These will be needed to begin deployment and installation of the EA and CP4I suite on top of the OCP cluster. oc CLI v4.6+ GitHub CLI Java v8+ keytool openssl mailtrap Instructions and resources for prerequisites setup are provided in each of the hyperlinks. 2. Deploying the lab environment You will require access to the ITZ in order to reserve your environment and complete the lab. If you do not yet have access or an account with the ITZ, you will need to register for one . The hands-on environment\u2014 a combination of services from IBM Cloud Pak for Integration (CP4I) and IBM Event Automation (EA) \u2014will be running entirely atop of Red Hat OpenShift , which will orchestrate containerized deployments of CPI and EA across the OpenShift cluster. If you require more details about the CP4I installation process, please check the product documentation . 2.1 Deploying an OpenShift Container Platform (OCP) cluster Reserve a Red Hat OpenShift cluster via the IBM Technology Zone pre-defined template. URL: https://techzone.ibm.com/my/reservations/create/63a3a25a3a4689001740dbb3 From the Single environment reservation options list, select Reserve now [A] . FULLSCREEN IMAGES Click on any of the screenshots within this documentation to enlarge the image. Configure the following fields in the reservation template to the values as described in the table below. All other fields from the ITZ template, if not listed in the table below, should be kept at their default values. Field Value Purpose If reserving for L3 or L4 training, select Education . If delivering a PoC, select Pilot and provide a Sales opportunity number. Purpose Description If reserving for L3 or L4 training, enter Event-led Integration Training . If delivering a PoC, enter the PoC and client details. Preferred Geography Select your preferred geography. OpenShift Version 4.15 Storage ODF \u2013 2TB Worker Node Count 5 Worker Node Flavor 16 vCPU x 64 GB - 100 GB ephemeral storage When satisfied, verify that you agree to the Terms and Conditions for the environment and finalize your reservation request by clicking Submit . You will receive an ITZ email confirming that the instance is provisioning. PROVISIONING TIMES Reservations take approximately 30 to 60 minutes to complete from the time that you click submit. If you navigate to the My Reservations tab of the ITZ, you can monitor the progress of the reservation. Wait for the ITZ reservation to be marked as \" Ready \" before attempting to start the lab \u2014 accessing it too soon will lead to issues (or an outright failure) when connecting to OpenShift. You will receive a second email once the environment is provisioned and the reservation status will now show as Ready . Click on Reservation ID to open the details page for the new OCP environment. Save for reference the Cluster Admin Username [A] , Cluster Admin Password [B] , and OCP Console [C] link. 2.2 Accessing the OCP cluster In this section, access the OpenShift cluster and install the OpenShift command line tool (CLI). In a web browser, open the OCP Console link and paste the Cluster Admin Username [A] and Password [B] from the previous step and click Log in [C] . THIS CONNECTION IS NOT PRIVATE Depending on your web browser, you may receive a warning about This connection is not private or a similarly phrased message. Disregard these warnings and continue to the intended destination by clicking \"visit this website.\" Copy the login command to access the cluster by CLI. In the top right, click your username [A] and select Copy login command [B] . Click Display Token . Copy the Log in with this token command and paste into a Terminal window. Now you are ready for the Cloud Pak for Integration (CP4I) installation. 2.3 Cloning repositories You will need to clone the demo repo to your workstation. Open a terminal window and run the command below: gh repo clone github.ibm.com/joel-gomez/cp4i-demo Open the cp4i-demo folder: cd cp4i-demo 2.4 Set context Based on where you have deployed your OCP cluster you will need to set up some environment variables to inform the installation script about your environment. Define an environment variable to set the CP4I version: export CP4I_VER = 16 .1.0 Set the OCP type based on the storage classes in your cluster. If using a ROKS cluster, use export OCP_TYPE=ROKS instead. export OCP_TYPE = ODF Set mailtap credentials, substituting <my-mailtrap-user> and <my-mailtrap-pwd> with the variables you set during the 1. Prerequisites section. export MAILTRAP_USER = <my-mailtrap-user> export MAILTRAP_PWD = <my-mailtrap-pwd> If you have provisioned your OCP cluster in Tech Zone you can use the following script to set the proper default storage class. scripts/99-odf-tkz-set-scs.sh Your cluster will also need access to pull the container software required to deploy the Cloud Pak. Your Entitlement Key is used to grant access. You can download your entitlement key from My IBM . Click Copy . The IBM Entitled Registry contains software images for the capabilities in IBM Cloud Pak for Integration. To allow the Cloud Pak for Integration operators to automatically pull those software images, you must first obtain your entitlement key, then add your entitlement key in a pull secret. Replace <my-key> with your IBM entitlement key and then execute: export ENT_KEY = <my-key> Now, you need to define a namespace where you will deploy the different capabilities. scripts/02a-cp4i-ns-key-config.sh 2.5 Installing the IBM Cloud Pak foundational services The IBM Cloud Pak foundational services operator is no longer installed automatically. Install this operator manually if you need to create an instance that uses identity and access management (IAM). The following steps will walk you through this process. First you need a certificate manager, otherwise the IBM Cloud Pak foundational services operator installation will not complete. oc apply -f resources/00-cert-manager-namespace.yaml oc apply -f resources/00-cert-manager-operatorgroup.yaml oc apply -f resources/00-cert-manager-subscription.yaml Confirm the subscription has been completed successfully before moving to the next step running the following command. SUB_NAME = $( oc get deployment cert-manager-operator-controller-manager -n cert-manager-operator --ignore-not-found -o jsonpath = '{.metadata.labels.olm\\.owner}' ) ; if [ ! -z \" $SUB_NAME \" ] ; then oc get csv/ $SUB_NAME -n cert-manager-operator --ignore-not-found -o jsonpath = '{.status.phase}' ; fi ; echo You should expect a response of Succeeded . Install the Postgress SQL Catalog source. oc apply -f catalog-sources/ ${ CP4I_VER } /01-postgress-sql-catalog-source.yaml Confirm the catalog source has been deployed successfully before moving to the next step running the following command. oc get catalogsources cloud-native-postgresql-catalog -n openshift-marketplace -o jsonpath = '{.status.connectionState.lastObservedState}' ; echo You should expect a response of READY . Install the Common Services Catalog source. oc apply -f catalog-sources/ ${ CP4I_VER } /02-common-services-catalog-source.yaml Confirm the catalog source has been deployed successfully before moving to the next step running the following command. oc get catalogsources opencloud-operators -n openshift-marketplace -o jsonpath = '{.status.connectionState.lastObservedState}' ; echo You should expect a response of READY . Create the Common Services namespace. oc new-project ibm-common-services Install the Foundational Services operator (former Common Services). oc apply -f subscriptions/ ${ CP4I_VER } /00-common-service-subscription.yaml Confirm the operator has been deployed successfully before moving to the next step running the following command. SUB_NAME = $( oc get deployment/ibm-common-service-operator -n openshift-operators --ignore-not-found -o jsonpath = '{.metadata.labels.olm\\.owner}' ) ; if [ ! -z \" $SUB_NAME \" ] ; then oc get csv/ $SUB_NAME --ignore-not-found -o jsonpath = '{.status.phase}' ; fi ; echo You should expect a response of Succeeded . 2.6 Deploying the Platform UI Deploying the Platform UI allows you to deploy and manage instances from a central location. Install the Platform UI Catalog source. oc apply -f catalog-sources/ ${ CP4I_VER } /03-platform-navigator-catalog-source.yaml Confirm the catalog source has been deployed successfully before moving to the next step running the following command. oc get catalogsources ibm-integration-platform-navigator-catalog -n openshift-marketplace -o jsonpath = '{.status.connectionState.lastObservedState}' ; echo You should expect a response of READY . Install the Platform UI Operator. oc apply -f subscriptions/ ${ CP4I_VER } /01-platform-navigator-subscription.yaml Confirm the operator has been deployed successfully before moving to the next step running the following command. SUB_NAME = $( oc get deployment ibm-integration-platform-navigator-operator -n openshift-operators --ignore-not-found -o jsonpath = '{.metadata.labels.olm\\.owner}' ) ; if [ ! -z \" $SUB_NAME \" ] ; then oc get csv/ $SUB_NAME --ignore-not-found -o jsonpath = '{.status.phase}' ; fi ; echo You should expect a response of Succeeded . Deploy a Platform UI instance. oc apply -f instances/ ${ CP4I_VER } /common/01-platform-navigator-instance.yaml Confirm the instance has been deployed successfully before moving to the next step running the following command. oc get platformnavigator cp4i-navigator -n tools -o jsonpath = '{.status.conditions[0].type}' ; echo You should expect a response of READY . Once the Platform UI instance is up and running, retrieve the access information by executing the following script: scripts/03b-cp4i-access-info.sh Open the Platform UI URL with a web browser and log in. Note the password is temporary and you will be required to change it. Take note of your new password, as you will use it multiple times throughout the lab. 2.7 Deploying IBM Event Streams Install the Event Streams Catalog Source. oc apply -f catalog-sources/ ${ CP4I_VER } /08-event-streams-catalog-source.yaml Confirm the catalog source has been deployed successfully before moving to the next step running the following command. oc get catalogsources ibm-eventstreams-catalog -n openshift-marketplace -o jsonpath = '{.status.connectionState.lastObservedState}' ; echo You should expect a response of READY . Install the Event Streams Operator. oc apply -f subscriptions/ ${ CP4I_VER } /05-event-streams-subscription.yaml Confirm the operator has been deployed successfully before moving to the next step running the following command. SUB_NAME = $( oc get deployment eventstreams-cluster-operator -n openshift-operators --ignore-not-found -o jsonpath = '{.metadata.labels.olm\\.owner}' ) ; if [ ! -z \" $SUB_NAME \" ] ; then oc get csv/ $SUB_NAME --ignore-not-found -o jsonpath = '{.status.phase}' ; fi ; echo You should expect a response of Succeeded . Deploy the Event Streams instance. oc apply -f instances/ ${ CP4I_VER } / ${ OCP_TYPE } /05-event-streams-instance.yaml Confirm the instance has been deployed successfully before moving to the next step running the following command. oc get eventstreams es-demo -n tools -o jsonpath = '{.status.phase}' ; echo TROUBLESHOOTING This will take few minutes, so be patient. At some point you may see errors \u2014 it is common to receive FAILED message in the first 15 minutes. Wait approximately 15 to 30 minutes. You should expect a response of Ready . Create users. oc apply -f resources/02a-es-initial-config-jgr-users.yaml 2.8 Deploying IBM Event Endpoint Management (EEM) Install the EEM Catalog Source. oc apply -f catalog-sources/ ${ CP4I_VER } /13-eem-catalog-source.yaml Confirm the catalog source has been deployed successfully before moving to the next step running the following command. oc get catalogsources ibm-eventendpointmanagement-catalog -n openshift-marketplace -o jsonpath = '{.status.connectionState.lastObservedState}' ; echo You should expect a response of READY . Install the EEM Operator. oc apply -f subscriptions/ ${ CP4I_VER } /09-eem-subscription.yaml Confirm the operator has been deployed successfully before moving to the next step running the following command. SUB_NAME = $( oc get deployment ibm-eem-operator -n openshift-operators --ignore-not-found -o jsonpath = '{.metadata.labels.olm\\.owner}' ) ; if [ ! -z \" $SUB_NAME \" ] ; then oc get csv/ $SUB_NAME --ignore-not-found -o jsonpath = '{.status.phase}' ; fi ; echo You should expect a response of Succeeded . Set passwords via environment variables (replace with your passwords). export EEM_ADMIN_PWD = <eem-admin-pwd> export EEM_USER_PWD = <eem-user-pwd> Deploy EEM Manager instance: scripts/19a-eem-manager-inst-deploy.sh TROUBLESHOOTING The current version may introduce an issue and instead of a positive response at the end of the script you may get the following message: Something is wrong! If that is the case, then run the following commands to fix the problem: oc exec -it -n tools eem-demo-mgr-ibm-eem-manager-0 -- rmdir /opt/storage/lost+found oc delete pod -n tools eem-demo-mgr-ibm-eem-manager-0 Run the following command to confirm the pod has been restarted. oc get pods -n tools | grep eem-demo-mgr Confirm you get the following response before proceeding to the next step. eem-demo-mgr-ibm-eem-manager-0 1 /1 Running 0 51s Now re-run the script and you should get the expected result. scripts/19a-eem-manager-inst-deploy.sh Confirm the instance has been deployed successfully before moving to the next step running the following command: oc get eventendpointmanagement eem-demo-mgr -n tools -o jsonpath = '{.status.phase}' ; echo Wait a few minutes. You should expect a response of Running . Deploy the EEM Gateway instance. scripts/19b-eem-gateway-inst-deploy.sh Confirm the instance has been deployed successfully before moving to the next step running the following command. oc get eventgateway eem-demo-gw -n tools -o jsonpath = '{.status.phase}' ; echo You should expect a response of Running . Integrate EEM and ES instances. scripts/19f-eem-es-config.sh Get a token for post-deployment configuration. Log in to your Event Endpoint Management UI from a supported web browser (get the login URL for your Event Endpoint Management instance running the command below). oc get eem -n tools Click the user icon in the header and then click Profile to open the page Within the Profile page, click Create token to open the dialog Read the message and note that API access tokens expire Provide a token description that can be used to identify your token and then click Create Once the new token is generated, click Copy token Click Close to exit the dialog Set environment variable for token by replacing <my-eem-token> with your own token value, then execute the following: export EEM_TOKEN = <my-eem-token> Afterwards, record your token into a notepad as you will need to reference it later in the demo script. 2.9 Deploying the Enterprise Messaging (MQ) service Install the MQ Catalog Source. oc apply -f catalog-sources/ ${ CP4I_VER } /09-mq-catalog-source.yaml Confirm the catalog source has been deployed successfully before moving to the next step running the following command. oc get catalogsources ibmmq-operator-catalogsource -n openshift-marketplace -o jsonpath = '{.status.connectionState.lastObservedState}' ; echo You should expect a response of READY . Install the MQ Operator. oc apply -f subscriptions/ ${ CP4I_VER } /06-mq-subscription.yaml Confirm the operator has been deployed successfully before moving to the next step running the following command. SUB_NAME = $( oc get deployment ibm-mq-operator -n openshift-operators --ignore-not-found -o jsonpath = '{.metadata.labels.olm\\.owner}' ) ; if [ ! -z \" $SUB_NAME \" ] ; then oc get csv/ $SUB_NAME --ignore-not-found -o jsonpath = '{.status.phase}' ; fi ; echo You should expect a response of Succeeded . Set the MQ namespace environment variable. export MQ_NAMESPACE = cp4i-mq Create certificates and extra route. scripts/10a-qmgr-pre-config.sh Create configmap with MQ configuration. oc apply -f resources/03c-qmgr-mqsc-config.yaml Deploy the MQ Queue Manager instance. scripts/10b-qmgr-inst-deploy.sh Confirm the instance has been deployed successfully before moving to the next step running the following command. oc get queuemanager qmgr-demo -n tools -o jsonpath = '{.status.phase}' ; echo Wait a few minutes. You should expect a response of Running . Create CCDT . scripts/10c-qmgr-post-config.sh Deploy the MQ Source Connector. oc apply -f resources/02b-es-mq-source.yaml Deploy the MQ Sink Connector. oc apply -f resources/02c-es-mq-sink.yaml 2.10 Deploy the Event Processing (EP) service Install Apache Flink Catalog Source. oc apply -f catalog-sources/ ${ CP4I_VER } /14-ea-flink-catalog-source.yaml Confirm the catalog source has been deployed successfully before moving to the next step running the following command. oc get catalogsources ibm-eventautomation-flink-catalog -n openshift-marketplace -o jsonpath = '{.status.connectionState.lastObservedState}' ; echo You should expect a response of READY . Install Apache Flink Operator. oc apply -f subscriptions/ ${ CP4I_VER } /10-ea-flink-subscription.yaml Confirm the operator has been deployed successfully before moving to the next step running the following command. SUB_NAME = $( oc get deployment flink-kubernetes-operator -n openshift-operators --ignore-not-found -o jsonpath = '{.metadata.labels.olm\\.owner}' ) ; if [ ! -z \" $SUB_NAME \" ] ; then oc get csv/ $SUB_NAME --ignore-not-found -o jsonpath = '{.status.phase}' ; fi ; echo You should expect a response of Succeeded . Prepare TrustStore for Event Automation. scripts/20d-ea-truststore-config.sh Deploy the Apache Flink instance. oc apply -f instances/ ${ CP4I_VER } /common/21-ea-flink-instance.yaml Confirm the operator has been deployed successfully before moving to the next step running the following command. oc get flinkdeployment ea-flink-demo -n tools -o jsonpath = '{.status.jobManagerDeploymentStatus}' ; echo You should expect a response of READY . Install the Event Processing Catalog Source. oc apply -f catalog-sources/ ${ CP4I_VER } /15-event-processing-catalog-source.yaml Confirm the catalog source has been deployed successfully before moving to the next step running the following command. oc get catalogsources ibm-eventprocessing-catalog -n openshift-marketplace -o jsonpath = '{.status.connectionState.lastObservedState}' ; echo You should expect a response of READY . Install the Event Processing Operator. oc apply -f subscriptions/ ${ CP4I_VER } /11-event-processing-subscription.yaml Confirm the operator has been deployed successfully before moving to the next step running the following command. SUB_NAME = $( oc get deployment ibm-ep-operator -n openshift-operators --ignore-not-found -o jsonpath = '{.metadata.labels.olm\\.owner}' ) ; if [ ! -z \" $SUB_NAME \" ] ; then oc get csv/ $SUB_NAME --ignore-not-found -o jsonpath = '{.status.phase}' ; fi ; echo You should expect a response of Succeeded . Set password via environment variables (substitute <ep-admin-pwd> with your own password). export EP_ADMIN_PWD = <ep-admin-pwd> Deploy the Event Processing instance. scripts/20b-ea-ep-inst-deploy.sh Confirm the instance has been deployed successfully before moving to the next step running the following command. oc get eventprocessing ep-demo -n tools -o jsonpath = '{.status.phase}' ; echo You should expect a response of Running . Install the PGSQL Operator. oc apply -f resources/12a-pgsql-subscription.yaml Confirm the instance has been deployed successfully before moving to the next step running the following command. SUB_NAME = $( oc get deployment pgo -n openshift-operators --ignore-not-found -o jsonpath = '{.metadata.labels.olm\\.owner}' ) ; if [ ! -z \" $SUB_NAME \" ] ; then oc get csv/ $SUB_NAME --ignore-not-found -o jsonpath = '{.status.phase}' ; fi ; echo You should expect a response of Succeeded . Create configmap with database configuration. oc apply -f resources/12b-pgsql-config.yaml Deploy a PGSQL database instance: oc apply -f resources/12c-pgsql-db.yaml Confirm the instance has been deployed successfully before moving to the next step running the following command. oc get pods -l \"postgres-operator.crunchydata.com/role=master\" -n tools -o jsonpath = '{.items[0].status.conditions[1].status}' ; echo Wait a few minutes. You should expect a response of True . Retrieve the connection details for accessing the EA instances. Record that information into a notepad for future reference. scripts/20c-ea-access-info.sh 2.11 Deploying the demo \"helpers\" Additional supporting services are needed to generate results and data for the hands-on lab experience. Open a Terminal window and execute the following command to clone (via git ) the demo repository to your local machine. gh repo clone ibm-integration/eventautomationL3 Navigate to the eventautomationL3 directory. cd eventautomationL3 Execute the script below to install the \"demo helper\" services. ./deploy-helpers.sh Enable the Kafka Connect base. scripts/08c-event-streams-kafka-connect-config.sh Confirm the instance has been deployed successfully before moving to the next step running the following command. oc get kafkaconnects jgr-connect-cluster -n tools -o jsonpath = '{.status.conditions[0].type}' ; echo You should expect a response of Ready . Enable the Kafka Connector Datagen. scripts/08e-event-streams-kafka-connector-datagen-config.sh Confirm the instance has been deployed successfully before moving to the next step running the following command. oc get kafkaconnector -n tools After several minutes, you should receive the following response: NAME CLUSTER CONNECTOR CLASS MAX TASKS READY kafka-datagen jgr-connect-cluster com.ibm.eventautomation.demos.acme.DatagenSourceConnector 1 True mq-sink jgr-connect-cluster com.ibm.eventstreams.connect.mqsink.MQSinkConnector 1 True mq-source jgr-connect-cluster com.ibm.eventstreams.connect.mqsource.MQSourceConnector 1 True 3. Next steps Congratulations \u2014 you have successfully deployed IBM Event Automation and IBM Cloud Pak for Integration , as well as the necessary services to run the hands-on lab. At this stage, use your web browser to open a new tab for each of the services: IBM Cloud Pak for Integration (CPI) Event Streams Event Endpoint Management Event Processing IBM MQ Keep these tabs open. You will be switching between them frequently. The following module will outline the Level 3 evaluation criteria for IBM technical sellers and business partners. Follow the accreditation steps appropriate to your job role.","title":"Setup"},{"location":"setup/#_1","text":"","title":""},{"location":"setup/#1-prerequisites","text":"This product is being developed and continuously updated in an agile manner. In addition to adding new capabilities, upgrades to the product are likely to result in changes to the user interface over time. As such, the screenshots used in this lab may not always look exactly like what you see in the latest version of product. These differences should not affect how the lab is executed, and the authoring team will do their best to keep things up to date, but be aware that screenshots or steps may differ slightly with your environment. Before reserving an environment with ITZ, first ensure that all of the following services and frameworks are available on your local machine. These will be needed to begin deployment and installation of the EA and CP4I suite on top of the OCP cluster. oc CLI v4.6+ GitHub CLI Java v8+ keytool openssl mailtrap Instructions and resources for prerequisites setup are provided in each of the hyperlinks.","title":"1. Prerequisites"},{"location":"setup/#_2","text":"","title":""},{"location":"setup/#2-deploying-the-lab-environment","text":"You will require access to the ITZ in order to reserve your environment and complete the lab. If you do not yet have access or an account with the ITZ, you will need to register for one . The hands-on environment\u2014 a combination of services from IBM Cloud Pak for Integration (CP4I) and IBM Event Automation (EA) \u2014will be running entirely atop of Red Hat OpenShift , which will orchestrate containerized deployments of CPI and EA across the OpenShift cluster. If you require more details about the CP4I installation process, please check the product documentation .","title":"2. Deploying the lab environment"},{"location":"setup/#21-deploying-an-openshift-container-platform-ocp-cluster","text":"Reserve a Red Hat OpenShift cluster via the IBM Technology Zone pre-defined template. URL: https://techzone.ibm.com/my/reservations/create/63a3a25a3a4689001740dbb3 From the Single environment reservation options list, select Reserve now [A] . FULLSCREEN IMAGES Click on any of the screenshots within this documentation to enlarge the image. Configure the following fields in the reservation template to the values as described in the table below. All other fields from the ITZ template, if not listed in the table below, should be kept at their default values. Field Value Purpose If reserving for L3 or L4 training, select Education . If delivering a PoC, select Pilot and provide a Sales opportunity number. Purpose Description If reserving for L3 or L4 training, enter Event-led Integration Training . If delivering a PoC, enter the PoC and client details. Preferred Geography Select your preferred geography. OpenShift Version 4.15 Storage ODF \u2013 2TB Worker Node Count 5 Worker Node Flavor 16 vCPU x 64 GB - 100 GB ephemeral storage When satisfied, verify that you agree to the Terms and Conditions for the environment and finalize your reservation request by clicking Submit . You will receive an ITZ email confirming that the instance is provisioning. PROVISIONING TIMES Reservations take approximately 30 to 60 minutes to complete from the time that you click submit. If you navigate to the My Reservations tab of the ITZ, you can monitor the progress of the reservation. Wait for the ITZ reservation to be marked as \" Ready \" before attempting to start the lab \u2014 accessing it too soon will lead to issues (or an outright failure) when connecting to OpenShift. You will receive a second email once the environment is provisioned and the reservation status will now show as Ready . Click on Reservation ID to open the details page for the new OCP environment. Save for reference the Cluster Admin Username [A] , Cluster Admin Password [B] , and OCP Console [C] link.","title":"2.1 Deploying an OpenShift Container Platform (OCP) cluster"},{"location":"setup/#22-accessing-the-ocp-cluster","text":"In this section, access the OpenShift cluster and install the OpenShift command line tool (CLI). In a web browser, open the OCP Console link and paste the Cluster Admin Username [A] and Password [B] from the previous step and click Log in [C] . THIS CONNECTION IS NOT PRIVATE Depending on your web browser, you may receive a warning about This connection is not private or a similarly phrased message. Disregard these warnings and continue to the intended destination by clicking \"visit this website.\" Copy the login command to access the cluster by CLI. In the top right, click your username [A] and select Copy login command [B] . Click Display Token . Copy the Log in with this token command and paste into a Terminal window. Now you are ready for the Cloud Pak for Integration (CP4I) installation.","title":"2.2 Accessing the OCP cluster"},{"location":"setup/#23-cloning-repositories","text":"You will need to clone the demo repo to your workstation. Open a terminal window and run the command below: gh repo clone github.ibm.com/joel-gomez/cp4i-demo Open the cp4i-demo folder: cd cp4i-demo","title":"2.3 Cloning repositories"},{"location":"setup/#24-set-context","text":"Based on where you have deployed your OCP cluster you will need to set up some environment variables to inform the installation script about your environment. Define an environment variable to set the CP4I version: export CP4I_VER = 16 .1.0 Set the OCP type based on the storage classes in your cluster. If using a ROKS cluster, use export OCP_TYPE=ROKS instead. export OCP_TYPE = ODF Set mailtap credentials, substituting <my-mailtrap-user> and <my-mailtrap-pwd> with the variables you set during the 1. Prerequisites section. export MAILTRAP_USER = <my-mailtrap-user> export MAILTRAP_PWD = <my-mailtrap-pwd> If you have provisioned your OCP cluster in Tech Zone you can use the following script to set the proper default storage class. scripts/99-odf-tkz-set-scs.sh Your cluster will also need access to pull the container software required to deploy the Cloud Pak. Your Entitlement Key is used to grant access. You can download your entitlement key from My IBM . Click Copy . The IBM Entitled Registry contains software images for the capabilities in IBM Cloud Pak for Integration. To allow the Cloud Pak for Integration operators to automatically pull those software images, you must first obtain your entitlement key, then add your entitlement key in a pull secret. Replace <my-key> with your IBM entitlement key and then execute: export ENT_KEY = <my-key> Now, you need to define a namespace where you will deploy the different capabilities. scripts/02a-cp4i-ns-key-config.sh","title":"2.4 Set context"},{"location":"setup/#25-installing-the-ibm-cloud-pak-foundational-services","text":"The IBM Cloud Pak foundational services operator is no longer installed automatically. Install this operator manually if you need to create an instance that uses identity and access management (IAM). The following steps will walk you through this process. First you need a certificate manager, otherwise the IBM Cloud Pak foundational services operator installation will not complete. oc apply -f resources/00-cert-manager-namespace.yaml oc apply -f resources/00-cert-manager-operatorgroup.yaml oc apply -f resources/00-cert-manager-subscription.yaml Confirm the subscription has been completed successfully before moving to the next step running the following command. SUB_NAME = $( oc get deployment cert-manager-operator-controller-manager -n cert-manager-operator --ignore-not-found -o jsonpath = '{.metadata.labels.olm\\.owner}' ) ; if [ ! -z \" $SUB_NAME \" ] ; then oc get csv/ $SUB_NAME -n cert-manager-operator --ignore-not-found -o jsonpath = '{.status.phase}' ; fi ; echo You should expect a response of Succeeded . Install the Postgress SQL Catalog source. oc apply -f catalog-sources/ ${ CP4I_VER } /01-postgress-sql-catalog-source.yaml Confirm the catalog source has been deployed successfully before moving to the next step running the following command. oc get catalogsources cloud-native-postgresql-catalog -n openshift-marketplace -o jsonpath = '{.status.connectionState.lastObservedState}' ; echo You should expect a response of READY . Install the Common Services Catalog source. oc apply -f catalog-sources/ ${ CP4I_VER } /02-common-services-catalog-source.yaml Confirm the catalog source has been deployed successfully before moving to the next step running the following command. oc get catalogsources opencloud-operators -n openshift-marketplace -o jsonpath = '{.status.connectionState.lastObservedState}' ; echo You should expect a response of READY . Create the Common Services namespace. oc new-project ibm-common-services Install the Foundational Services operator (former Common Services). oc apply -f subscriptions/ ${ CP4I_VER } /00-common-service-subscription.yaml Confirm the operator has been deployed successfully before moving to the next step running the following command. SUB_NAME = $( oc get deployment/ibm-common-service-operator -n openshift-operators --ignore-not-found -o jsonpath = '{.metadata.labels.olm\\.owner}' ) ; if [ ! -z \" $SUB_NAME \" ] ; then oc get csv/ $SUB_NAME --ignore-not-found -o jsonpath = '{.status.phase}' ; fi ; echo You should expect a response of Succeeded .","title":"2.5 Installing the IBM Cloud Pak foundational services"},{"location":"setup/#26-deploying-the-platform-ui","text":"Deploying the Platform UI allows you to deploy and manage instances from a central location. Install the Platform UI Catalog source. oc apply -f catalog-sources/ ${ CP4I_VER } /03-platform-navigator-catalog-source.yaml Confirm the catalog source has been deployed successfully before moving to the next step running the following command. oc get catalogsources ibm-integration-platform-navigator-catalog -n openshift-marketplace -o jsonpath = '{.status.connectionState.lastObservedState}' ; echo You should expect a response of READY . Install the Platform UI Operator. oc apply -f subscriptions/ ${ CP4I_VER } /01-platform-navigator-subscription.yaml Confirm the operator has been deployed successfully before moving to the next step running the following command. SUB_NAME = $( oc get deployment ibm-integration-platform-navigator-operator -n openshift-operators --ignore-not-found -o jsonpath = '{.metadata.labels.olm\\.owner}' ) ; if [ ! -z \" $SUB_NAME \" ] ; then oc get csv/ $SUB_NAME --ignore-not-found -o jsonpath = '{.status.phase}' ; fi ; echo You should expect a response of Succeeded . Deploy a Platform UI instance. oc apply -f instances/ ${ CP4I_VER } /common/01-platform-navigator-instance.yaml Confirm the instance has been deployed successfully before moving to the next step running the following command. oc get platformnavigator cp4i-navigator -n tools -o jsonpath = '{.status.conditions[0].type}' ; echo You should expect a response of READY . Once the Platform UI instance is up and running, retrieve the access information by executing the following script: scripts/03b-cp4i-access-info.sh Open the Platform UI URL with a web browser and log in. Note the password is temporary and you will be required to change it. Take note of your new password, as you will use it multiple times throughout the lab.","title":"2.6 Deploying the Platform UI"},{"location":"setup/#27-deploying-ibm-event-streams","text":"Install the Event Streams Catalog Source. oc apply -f catalog-sources/ ${ CP4I_VER } /08-event-streams-catalog-source.yaml Confirm the catalog source has been deployed successfully before moving to the next step running the following command. oc get catalogsources ibm-eventstreams-catalog -n openshift-marketplace -o jsonpath = '{.status.connectionState.lastObservedState}' ; echo You should expect a response of READY . Install the Event Streams Operator. oc apply -f subscriptions/ ${ CP4I_VER } /05-event-streams-subscription.yaml Confirm the operator has been deployed successfully before moving to the next step running the following command. SUB_NAME = $( oc get deployment eventstreams-cluster-operator -n openshift-operators --ignore-not-found -o jsonpath = '{.metadata.labels.olm\\.owner}' ) ; if [ ! -z \" $SUB_NAME \" ] ; then oc get csv/ $SUB_NAME --ignore-not-found -o jsonpath = '{.status.phase}' ; fi ; echo You should expect a response of Succeeded . Deploy the Event Streams instance. oc apply -f instances/ ${ CP4I_VER } / ${ OCP_TYPE } /05-event-streams-instance.yaml Confirm the instance has been deployed successfully before moving to the next step running the following command. oc get eventstreams es-demo -n tools -o jsonpath = '{.status.phase}' ; echo TROUBLESHOOTING This will take few minutes, so be patient. At some point you may see errors \u2014 it is common to receive FAILED message in the first 15 minutes. Wait approximately 15 to 30 minutes. You should expect a response of Ready . Create users. oc apply -f resources/02a-es-initial-config-jgr-users.yaml","title":"2.7 Deploying IBM Event Streams"},{"location":"setup/#28-deploying-ibm-event-endpoint-management-eem","text":"Install the EEM Catalog Source. oc apply -f catalog-sources/ ${ CP4I_VER } /13-eem-catalog-source.yaml Confirm the catalog source has been deployed successfully before moving to the next step running the following command. oc get catalogsources ibm-eventendpointmanagement-catalog -n openshift-marketplace -o jsonpath = '{.status.connectionState.lastObservedState}' ; echo You should expect a response of READY . Install the EEM Operator. oc apply -f subscriptions/ ${ CP4I_VER } /09-eem-subscription.yaml Confirm the operator has been deployed successfully before moving to the next step running the following command. SUB_NAME = $( oc get deployment ibm-eem-operator -n openshift-operators --ignore-not-found -o jsonpath = '{.metadata.labels.olm\\.owner}' ) ; if [ ! -z \" $SUB_NAME \" ] ; then oc get csv/ $SUB_NAME --ignore-not-found -o jsonpath = '{.status.phase}' ; fi ; echo You should expect a response of Succeeded . Set passwords via environment variables (replace with your passwords). export EEM_ADMIN_PWD = <eem-admin-pwd> export EEM_USER_PWD = <eem-user-pwd> Deploy EEM Manager instance: scripts/19a-eem-manager-inst-deploy.sh TROUBLESHOOTING The current version may introduce an issue and instead of a positive response at the end of the script you may get the following message: Something is wrong! If that is the case, then run the following commands to fix the problem: oc exec -it -n tools eem-demo-mgr-ibm-eem-manager-0 -- rmdir /opt/storage/lost+found oc delete pod -n tools eem-demo-mgr-ibm-eem-manager-0 Run the following command to confirm the pod has been restarted. oc get pods -n tools | grep eem-demo-mgr Confirm you get the following response before proceeding to the next step. eem-demo-mgr-ibm-eem-manager-0 1 /1 Running 0 51s Now re-run the script and you should get the expected result. scripts/19a-eem-manager-inst-deploy.sh Confirm the instance has been deployed successfully before moving to the next step running the following command: oc get eventendpointmanagement eem-demo-mgr -n tools -o jsonpath = '{.status.phase}' ; echo Wait a few minutes. You should expect a response of Running . Deploy the EEM Gateway instance. scripts/19b-eem-gateway-inst-deploy.sh Confirm the instance has been deployed successfully before moving to the next step running the following command. oc get eventgateway eem-demo-gw -n tools -o jsonpath = '{.status.phase}' ; echo You should expect a response of Running . Integrate EEM and ES instances. scripts/19f-eem-es-config.sh Get a token for post-deployment configuration. Log in to your Event Endpoint Management UI from a supported web browser (get the login URL for your Event Endpoint Management instance running the command below). oc get eem -n tools Click the user icon in the header and then click Profile to open the page Within the Profile page, click Create token to open the dialog Read the message and note that API access tokens expire Provide a token description that can be used to identify your token and then click Create Once the new token is generated, click Copy token Click Close to exit the dialog Set environment variable for token by replacing <my-eem-token> with your own token value, then execute the following: export EEM_TOKEN = <my-eem-token> Afterwards, record your token into a notepad as you will need to reference it later in the demo script.","title":"2.8 Deploying IBM Event Endpoint Management (EEM)"},{"location":"setup/#29-deploying-the-enterprise-messaging-mq-service","text":"Install the MQ Catalog Source. oc apply -f catalog-sources/ ${ CP4I_VER } /09-mq-catalog-source.yaml Confirm the catalog source has been deployed successfully before moving to the next step running the following command. oc get catalogsources ibmmq-operator-catalogsource -n openshift-marketplace -o jsonpath = '{.status.connectionState.lastObservedState}' ; echo You should expect a response of READY . Install the MQ Operator. oc apply -f subscriptions/ ${ CP4I_VER } /06-mq-subscription.yaml Confirm the operator has been deployed successfully before moving to the next step running the following command. SUB_NAME = $( oc get deployment ibm-mq-operator -n openshift-operators --ignore-not-found -o jsonpath = '{.metadata.labels.olm\\.owner}' ) ; if [ ! -z \" $SUB_NAME \" ] ; then oc get csv/ $SUB_NAME --ignore-not-found -o jsonpath = '{.status.phase}' ; fi ; echo You should expect a response of Succeeded . Set the MQ namespace environment variable. export MQ_NAMESPACE = cp4i-mq Create certificates and extra route. scripts/10a-qmgr-pre-config.sh Create configmap with MQ configuration. oc apply -f resources/03c-qmgr-mqsc-config.yaml Deploy the MQ Queue Manager instance. scripts/10b-qmgr-inst-deploy.sh Confirm the instance has been deployed successfully before moving to the next step running the following command. oc get queuemanager qmgr-demo -n tools -o jsonpath = '{.status.phase}' ; echo Wait a few minutes. You should expect a response of Running . Create CCDT . scripts/10c-qmgr-post-config.sh Deploy the MQ Source Connector. oc apply -f resources/02b-es-mq-source.yaml Deploy the MQ Sink Connector. oc apply -f resources/02c-es-mq-sink.yaml","title":"2.9 Deploying the Enterprise Messaging (MQ) service"},{"location":"setup/#210-deploy-the-event-processing-ep-service","text":"Install Apache Flink Catalog Source. oc apply -f catalog-sources/ ${ CP4I_VER } /14-ea-flink-catalog-source.yaml Confirm the catalog source has been deployed successfully before moving to the next step running the following command. oc get catalogsources ibm-eventautomation-flink-catalog -n openshift-marketplace -o jsonpath = '{.status.connectionState.lastObservedState}' ; echo You should expect a response of READY . Install Apache Flink Operator. oc apply -f subscriptions/ ${ CP4I_VER } /10-ea-flink-subscription.yaml Confirm the operator has been deployed successfully before moving to the next step running the following command. SUB_NAME = $( oc get deployment flink-kubernetes-operator -n openshift-operators --ignore-not-found -o jsonpath = '{.metadata.labels.olm\\.owner}' ) ; if [ ! -z \" $SUB_NAME \" ] ; then oc get csv/ $SUB_NAME --ignore-not-found -o jsonpath = '{.status.phase}' ; fi ; echo You should expect a response of Succeeded . Prepare TrustStore for Event Automation. scripts/20d-ea-truststore-config.sh Deploy the Apache Flink instance. oc apply -f instances/ ${ CP4I_VER } /common/21-ea-flink-instance.yaml Confirm the operator has been deployed successfully before moving to the next step running the following command. oc get flinkdeployment ea-flink-demo -n tools -o jsonpath = '{.status.jobManagerDeploymentStatus}' ; echo You should expect a response of READY . Install the Event Processing Catalog Source. oc apply -f catalog-sources/ ${ CP4I_VER } /15-event-processing-catalog-source.yaml Confirm the catalog source has been deployed successfully before moving to the next step running the following command. oc get catalogsources ibm-eventprocessing-catalog -n openshift-marketplace -o jsonpath = '{.status.connectionState.lastObservedState}' ; echo You should expect a response of READY . Install the Event Processing Operator. oc apply -f subscriptions/ ${ CP4I_VER } /11-event-processing-subscription.yaml Confirm the operator has been deployed successfully before moving to the next step running the following command. SUB_NAME = $( oc get deployment ibm-ep-operator -n openshift-operators --ignore-not-found -o jsonpath = '{.metadata.labels.olm\\.owner}' ) ; if [ ! -z \" $SUB_NAME \" ] ; then oc get csv/ $SUB_NAME --ignore-not-found -o jsonpath = '{.status.phase}' ; fi ; echo You should expect a response of Succeeded . Set password via environment variables (substitute <ep-admin-pwd> with your own password). export EP_ADMIN_PWD = <ep-admin-pwd> Deploy the Event Processing instance. scripts/20b-ea-ep-inst-deploy.sh Confirm the instance has been deployed successfully before moving to the next step running the following command. oc get eventprocessing ep-demo -n tools -o jsonpath = '{.status.phase}' ; echo You should expect a response of Running . Install the PGSQL Operator. oc apply -f resources/12a-pgsql-subscription.yaml Confirm the instance has been deployed successfully before moving to the next step running the following command. SUB_NAME = $( oc get deployment pgo -n openshift-operators --ignore-not-found -o jsonpath = '{.metadata.labels.olm\\.owner}' ) ; if [ ! -z \" $SUB_NAME \" ] ; then oc get csv/ $SUB_NAME --ignore-not-found -o jsonpath = '{.status.phase}' ; fi ; echo You should expect a response of Succeeded . Create configmap with database configuration. oc apply -f resources/12b-pgsql-config.yaml Deploy a PGSQL database instance: oc apply -f resources/12c-pgsql-db.yaml Confirm the instance has been deployed successfully before moving to the next step running the following command. oc get pods -l \"postgres-operator.crunchydata.com/role=master\" -n tools -o jsonpath = '{.items[0].status.conditions[1].status}' ; echo Wait a few minutes. You should expect a response of True . Retrieve the connection details for accessing the EA instances. Record that information into a notepad for future reference. scripts/20c-ea-access-info.sh","title":"2.10 Deploy the Event Processing (EP) service"},{"location":"setup/#211-deploying-the-demo-helpers","text":"Additional supporting services are needed to generate results and data for the hands-on lab experience. Open a Terminal window and execute the following command to clone (via git ) the demo repository to your local machine. gh repo clone ibm-integration/eventautomationL3 Navigate to the eventautomationL3 directory. cd eventautomationL3 Execute the script below to install the \"demo helper\" services. ./deploy-helpers.sh Enable the Kafka Connect base. scripts/08c-event-streams-kafka-connect-config.sh Confirm the instance has been deployed successfully before moving to the next step running the following command. oc get kafkaconnects jgr-connect-cluster -n tools -o jsonpath = '{.status.conditions[0].type}' ; echo You should expect a response of Ready . Enable the Kafka Connector Datagen. scripts/08e-event-streams-kafka-connector-datagen-config.sh Confirm the instance has been deployed successfully before moving to the next step running the following command. oc get kafkaconnector -n tools After several minutes, you should receive the following response: NAME CLUSTER CONNECTOR CLASS MAX TASKS READY kafka-datagen jgr-connect-cluster com.ibm.eventautomation.demos.acme.DatagenSourceConnector 1 True mq-sink jgr-connect-cluster com.ibm.eventstreams.connect.mqsink.MQSinkConnector 1 True mq-source jgr-connect-cluster com.ibm.eventstreams.connect.mqsource.MQSourceConnector 1 True","title":"2.11 Deploying the demo \"helpers\""},{"location":"setup/#_3","text":"","title":""},{"location":"setup/#3-next-steps","text":"Congratulations \u2014 you have successfully deployed IBM Event Automation and IBM Cloud Pak for Integration , as well as the necessary services to run the hands-on lab. At this stage, use your web browser to open a new tab for each of the services: IBM Cloud Pak for Integration (CPI) Event Streams Event Endpoint Management Event Processing IBM MQ Keep these tabs open. You will be switching between them frequently. The following module will outline the Level 3 evaluation criteria for IBM technical sellers and business partners. Follow the accreditation steps appropriate to your job role.","title":"3. Next steps"}]}