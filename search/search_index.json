{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Christopher Bienko (Principal, Learning Content Development, IBM Application Modernization) introduces the objectives and curriculum for IBM Event Automation for Technical Sales Level 3 . Special thanks to Callum Jackson for permission to adapt content from Using event automation to create Kafka streams from IBM MQ for the purposes of this lab. Additional ways to watch: Seismic replay available for download. [4 minutes] Integration that weaves together both applications and systems is essential to the success of a modern, digital enterprise. In 2024 and beyond, the enterprise organizations that outpace and outcompete in their marketplace will be those that are the most responsive, agile, and adaptable to an ever-shifting and increasingly-connected IT landscape. The success of these organizations will depend on their ability to securely connect, automate, and transform their business at scale \u2014 made possible through the integration of real-time messaging, events, and processes. Conceptually, it's useful to examine these requirements across three domain areas (which so happen to map to the three pillars of the IBM Integration solution portfolio): Event-led integration : For enterprise to discover what is happening within in their business and quickly respond to emergent events. API-led integration : For enterprise that needs to access and share information, both securely and rapidly. Messaging and connectivity : For enterprise to securely and reliable access or update Systems of Record, at scale. Organizations of every size, across all industries and verticals, face a shared set of challenges in their adoption of event-driven architectures and enterprise-wide integration. Enterprise organizations (and many IBM clients) are at different stages in their event-driven journey. For some, they have only recently embarked on scoping and putting into place an event messaging distribution layer. Others have already have in place the foundations for an Apache Kafka-based event distribution layer, and are looking to add capabilities that boost security, enable governed self-service access, and incorporate event processing. Despite the range of industries that must confront the challenges of event-driven architectures and integration, there are four common themes that regularly emerge in conversations with prospective clients. Real-time access for meeting customer demands and taking action in the moment. Event-driven enterprises must be optimized, timely, adaptable, and proactive. Event-driven integration and automation provides continuous awareness for a business about the state of their systems and services. It drives automation that responds to events in any given situation and enables a more adaptable approach to situations evolving in real-time. EVENT-LED INTEGRATION SOLUTIONS FROM IBM IBM Event Automation puts business events to work by making it possible for users to detect unfolding situations and making automated decisions in response in real-time. The offering consists of three core, composable capabilities: Event Streams : \"Event distribution.\" Collect real-time business events data streams using an enterprise-grade Apache Kafka distribution. Event sources can include Kafka Connect (MQ, database), REST APIs, and direct integrations (IBM App Connect, Change Data Capture, and more). Event Endpoint Management : \"Event discovery.\" Build a self-service catalog of event sources for other users within the business to securely browse and leverage. Event Processing : \"Event processing.\" Define business situations requiring automated responses ahead of time, using intuitive authoring canvas, that can be invoked in real-time to automate decision making and take decisive actions. To take raw events and make them contextually relevant to business users, Event Processing can: filter and transform event properties; combine events to identify novel patterns over continuous time windows; and aggregate events to analyze trends or detect anomalies. Once clients have set up a distribution layer and they\u2019ve started to share events around the enterprise, they want to work with those events to understand their relevance. Identifying and acting on these situations in the moment. Multiplicity of data sources, all of which a business may need access to. The ever-increasing number of data sources that an enterprise organization connects to provides an expanding surface area for malicious agents to exploit. Moreover, maintaining and managing these data sources becomes increasingly complex as more endpoints are connected to an enterprise's IT estate. + Secure access across the organization. Every enterprise has a plethora of unique APIs that are continuously updating. APIs are a specific vector that attackers can exploit for gaining unauthorized access to systems and data. Two-thirds of cloud incidents are due to misconfigured API endpoints. API-LED INTEGRATION SOLUTIONS FROM IBM The combined challenges of managing across a multiplicity of data sources and securing access to that data is addressed by the core competencies of IBM App Connect and IBM App Connect Enterprise . IBM App Connect allows clients to quickly build and test integration flows with frequently-used authoring tools. It securely connects with hundreds of cloud and on-premises applications via pre-built smart connectors and templates. App Connect's high performance transformation engine virtually supports any-to-any data formats (augmented with AI tooling to assist non-specialists.) It is used to easily deploy integrations with visibility into flow health and performance, for managing across hybrid cloud environments. IBM App Connect supports over 200 OOTB connectors and templates for jumpstarting integration. Automation Explorer provides a community for connectors and templates (explorer.automation.ibm.com). Clients can build their own bespoke systems connectors using an accelerated Connector Development Kit . Once connections have been established to the endpoints across an enterprise, that API-fed data needs to be shared (externally or internally) in order to be acted upon. From the moment these APIs are created, they must be managed and secured by a governance and control framework; it is here that API management plays a critical role. API-led integration is the cutting edge way to integrate applications and data by leveraging reusable REST APIs. Via IBM App Connect, the API-led integration approach brings together the disciplines of API management and API integration, across on-premises and cloud. First, clients build an integration flow for their APIs using an OpenAPI editor for authoring gateway policies that will govern OpenAPI documents. IBM's all-in-one flow editor provides a unified API authoring experience that brings together \"application integration\" and \"API management\" into a single tool. By contrast, other integration products require different tools, with differing experiences, to be performed by different users. IBM App Connect makes it possible to define an OpenAPI, build an integration flow, secure it with gateway policies, as well as manage and publish it to a Developer Portal for re-use \u2014 all within a unified editor. IBM App Connect's partnership with Noname Security helps identify undiscovered API exposure and API Connect's integration with IBM DataPower Gateway provides real-time threat and anomaly detection using AI. Clients can utilize StepZen to rapidly federate a variety of back-end data sources using highly responsive GraphQL APIs (declaratively defined, developer-friendly). Access to a diverse range of development and runtime execution platforms. Endpoints requiring access may include cloud-native application frameworks (containers and microservices), public cloud infrastructure, software-as-a-service (SaaS) applications, software-defined or hyperconverged infrastructure configuration, on-premises implementations of public cloud services, or consumption-billed infrastructure resources. This is not an exhaustive list. MESSAGING AND CONNECTIVITY SOLUTIONS FROM IBM The challenges of integrating across a diverse range of messaging and connectivity endpoints is addressed by a range of IBM technologies: IBM Cloud Pak for Integration , IBM App Connect + API Connect software, and IBM App Connect + API Connect as a Service (on IBM Cloud.) IBM Cloud Pak for Integration runs atop Red Hat OpenShift: a vendor-agnostic, containerization platform that is truly hybrid cloud \u2014 deployable on nearly every on-premises and cloud infrastructure topology. The offering packages together: API management, application integration, end-to-end security, enterprise messaging, event streaming, and high speed data transfers. It provides a unified place for developers, lines of business, and IT operators to catalog, document, and manage services + events through a single portal. Likewise, all of these services are bundled and licensed together; there is no need to decide what capabilities are required up-front and only pay for how much integration is deployed. \"Integration Assemblies\" simplify and automate the deployment of new integrations using the IBM Cloud Pak for Integration. Users can create and manage multiple integration instances as a single object. These instances can be immediately granted high availability (HA) if required. Declarative deployments reduce the complexity of manual integration tasks and rapidly increase the rate at which a business can make new API deployments. \"Operator-deployed container images\" take advantage of Red Hat OpenShift's powerful Operators, reducing the complexity of deploying containerized applications, updating images without disruption, and increasing the resiliency of live containers. Using IBM Event Automation to capitalize on time-sensitive revenue opportunities. Focus Corporation , a hypothetical retail company engaged with IBM, is highly motivated to derive more revenue from first-time customers to their online storefronts. Focus' marketing department believes that by sending high-value promotions to new customers immediately after they make a large purchase can significantly boost the company's revenue streams. The challenge is finding a way to capitalize on these time-sensitive revenue opportunities using the real-time transaction data already available (but underutilized) to Focus' IT department. Focus is currently using IBM MQ to exchange customer orders (placed online) between two endpoints: an order management system and the online payment gateway. As part of the hands-on work to be performed, IBM and Focus integration teams will need to tap into IBM MQ communications and publish each order as a Kafka-based \"event stream\". This will enable business-critical communications originating over MQ to be captured as events within Kafka topics, which can later be shared across the enterprise for fueling more responsive applications. Ultimately, Focus' marketing department will be able to use a self-service catalog for inspecting Order and New Customer topics as needed. From there, they will leverage no-code editors to build automated process flows that detect new customer orders totaling over $100 and issue high-value promotions to the customer's inbox immediately following the purchase. Next Steps In the following module, you will receive the necessary pre-requisites and setup instructions for getting started with IBM Event Automation and IBM Cloud Pak for Integration environments.","title":"Introduction"},{"location":"#_1","text":"","title":""},{"location":"#integration-that-weaves-together-both-applications-and-systems-is-essential-to-the-success-of-a-modern-digital-enterprise","text":"In 2024 and beyond, the enterprise organizations that outpace and outcompete in their marketplace will be those that are the most responsive, agile, and adaptable to an ever-shifting and increasingly-connected IT landscape. The success of these organizations will depend on their ability to securely connect, automate, and transform their business at scale \u2014 made possible through the integration of real-time messaging, events, and processes. Conceptually, it's useful to examine these requirements across three domain areas (which so happen to map to the three pillars of the IBM Integration solution portfolio): Event-led integration : For enterprise to discover what is happening within in their business and quickly respond to emergent events. API-led integration : For enterprise that needs to access and share information, both securely and rapidly. Messaging and connectivity : For enterprise to securely and reliable access or update Systems of Record, at scale.","title":"Integration that weaves together both applications and systems is essential to the success of a modern, digital enterprise."},{"location":"#_2","text":"","title":""},{"location":"#organizations-of-every-size-across-all-industries-and-verticals-face-a-shared-set-of-challenges-in-their-adoption-of-event-driven-architectures-and-enterprise-wide-integration","text":"Enterprise organizations (and many IBM clients) are at different stages in their event-driven journey. For some, they have only recently embarked on scoping and putting into place an event messaging distribution layer. Others have already have in place the foundations for an Apache Kafka-based event distribution layer, and are looking to add capabilities that boost security, enable governed self-service access, and incorporate event processing. Despite the range of industries that must confront the challenges of event-driven architectures and integration, there are four common themes that regularly emerge in conversations with prospective clients. Real-time access for meeting customer demands and taking action in the moment. Event-driven enterprises must be optimized, timely, adaptable, and proactive. Event-driven integration and automation provides continuous awareness for a business about the state of their systems and services. It drives automation that responds to events in any given situation and enables a more adaptable approach to situations evolving in real-time. EVENT-LED INTEGRATION SOLUTIONS FROM IBM IBM Event Automation puts business events to work by making it possible for users to detect unfolding situations and making automated decisions in response in real-time. The offering consists of three core, composable capabilities: Event Streams : \"Event distribution.\" Collect real-time business events data streams using an enterprise-grade Apache Kafka distribution. Event sources can include Kafka Connect (MQ, database), REST APIs, and direct integrations (IBM App Connect, Change Data Capture, and more). Event Endpoint Management : \"Event discovery.\" Build a self-service catalog of event sources for other users within the business to securely browse and leverage. Event Processing : \"Event processing.\" Define business situations requiring automated responses ahead of time, using intuitive authoring canvas, that can be invoked in real-time to automate decision making and take decisive actions. To take raw events and make them contextually relevant to business users, Event Processing can: filter and transform event properties; combine events to identify novel patterns over continuous time windows; and aggregate events to analyze trends or detect anomalies. Once clients have set up a distribution layer and they\u2019ve started to share events around the enterprise, they want to work with those events to understand their relevance. Identifying and acting on these situations in the moment. Multiplicity of data sources, all of which a business may need access to. The ever-increasing number of data sources that an enterprise organization connects to provides an expanding surface area for malicious agents to exploit. Moreover, maintaining and managing these data sources becomes increasingly complex as more endpoints are connected to an enterprise's IT estate. + Secure access across the organization. Every enterprise has a plethora of unique APIs that are continuously updating. APIs are a specific vector that attackers can exploit for gaining unauthorized access to systems and data. Two-thirds of cloud incidents are due to misconfigured API endpoints. API-LED INTEGRATION SOLUTIONS FROM IBM The combined challenges of managing across a multiplicity of data sources and securing access to that data is addressed by the core competencies of IBM App Connect and IBM App Connect Enterprise . IBM App Connect allows clients to quickly build and test integration flows with frequently-used authoring tools. It securely connects with hundreds of cloud and on-premises applications via pre-built smart connectors and templates. App Connect's high performance transformation engine virtually supports any-to-any data formats (augmented with AI tooling to assist non-specialists.) It is used to easily deploy integrations with visibility into flow health and performance, for managing across hybrid cloud environments. IBM App Connect supports over 200 OOTB connectors and templates for jumpstarting integration. Automation Explorer provides a community for connectors and templates (explorer.automation.ibm.com). Clients can build their own bespoke systems connectors using an accelerated Connector Development Kit . Once connections have been established to the endpoints across an enterprise, that API-fed data needs to be shared (externally or internally) in order to be acted upon. From the moment these APIs are created, they must be managed and secured by a governance and control framework; it is here that API management plays a critical role. API-led integration is the cutting edge way to integrate applications and data by leveraging reusable REST APIs. Via IBM App Connect, the API-led integration approach brings together the disciplines of API management and API integration, across on-premises and cloud. First, clients build an integration flow for their APIs using an OpenAPI editor for authoring gateway policies that will govern OpenAPI documents. IBM's all-in-one flow editor provides a unified API authoring experience that brings together \"application integration\" and \"API management\" into a single tool. By contrast, other integration products require different tools, with differing experiences, to be performed by different users. IBM App Connect makes it possible to define an OpenAPI, build an integration flow, secure it with gateway policies, as well as manage and publish it to a Developer Portal for re-use \u2014 all within a unified editor. IBM App Connect's partnership with Noname Security helps identify undiscovered API exposure and API Connect's integration with IBM DataPower Gateway provides real-time threat and anomaly detection using AI. Clients can utilize StepZen to rapidly federate a variety of back-end data sources using highly responsive GraphQL APIs (declaratively defined, developer-friendly). Access to a diverse range of development and runtime execution platforms. Endpoints requiring access may include cloud-native application frameworks (containers and microservices), public cloud infrastructure, software-as-a-service (SaaS) applications, software-defined or hyperconverged infrastructure configuration, on-premises implementations of public cloud services, or consumption-billed infrastructure resources. This is not an exhaustive list. MESSAGING AND CONNECTIVITY SOLUTIONS FROM IBM The challenges of integrating across a diverse range of messaging and connectivity endpoints is addressed by a range of IBM technologies: IBM Cloud Pak for Integration , IBM App Connect + API Connect software, and IBM App Connect + API Connect as a Service (on IBM Cloud.) IBM Cloud Pak for Integration runs atop Red Hat OpenShift: a vendor-agnostic, containerization platform that is truly hybrid cloud \u2014 deployable on nearly every on-premises and cloud infrastructure topology. The offering packages together: API management, application integration, end-to-end security, enterprise messaging, event streaming, and high speed data transfers. It provides a unified place for developers, lines of business, and IT operators to catalog, document, and manage services + events through a single portal. Likewise, all of these services are bundled and licensed together; there is no need to decide what capabilities are required up-front and only pay for how much integration is deployed. \"Integration Assemblies\" simplify and automate the deployment of new integrations using the IBM Cloud Pak for Integration. Users can create and manage multiple integration instances as a single object. These instances can be immediately granted high availability (HA) if required. Declarative deployments reduce the complexity of manual integration tasks and rapidly increase the rate at which a business can make new API deployments. \"Operator-deployed container images\" take advantage of Red Hat OpenShift's powerful Operators, reducing the complexity of deploying containerized applications, updating images without disruption, and increasing the resiliency of live containers.","title":"Organizations of every size, across all industries and verticals, face a shared set of challenges in their adoption of event-driven architectures and enterprise-wide integration."},{"location":"#_3","text":"","title":""},{"location":"#using-ibm-event-automation-to-capitalize-on-time-sensitive-revenue-opportunities","text":"Focus Corporation , a hypothetical retail company engaged with IBM, is highly motivated to derive more revenue from first-time customers to their online storefronts. Focus' marketing department believes that by sending high-value promotions to new customers immediately after they make a large purchase can significantly boost the company's revenue streams. The challenge is finding a way to capitalize on these time-sensitive revenue opportunities using the real-time transaction data already available (but underutilized) to Focus' IT department. Focus is currently using IBM MQ to exchange customer orders (placed online) between two endpoints: an order management system and the online payment gateway. As part of the hands-on work to be performed, IBM and Focus integration teams will need to tap into IBM MQ communications and publish each order as a Kafka-based \"event stream\". This will enable business-critical communications originating over MQ to be captured as events within Kafka topics, which can later be shared across the enterprise for fueling more responsive applications. Ultimately, Focus' marketing department will be able to use a self-service catalog for inspecting Order and New Customer topics as needed. From there, they will leverage no-code editors to build automated process flows that detect new customer orders totaling over $100 and issue high-value promotions to the customer's inbox immediately following the purchase.","title":"Using IBM Event Automation to capitalize on time-sensitive revenue opportunities."},{"location":"#_4","text":"","title":""},{"location":"#next-steps","text":"In the following module, you will receive the necessary pre-requisites and setup instructions for getting started with IBM Event Automation and IBM Cloud Pak for Integration environments.","title":"Next Steps"},{"location":"1/","text":"Creating an event stream from an IBM MQ message queue WATCH THIS DEMONSTRATION Christopher Bienko (Principal, Learning Content Development, IBM Application Modernization) provides a hands-on demonstration of the module contents. Additional ways to watch: Seismic replay available for download. [8 minutes] To take advantage of the message queue data available from IBM MQ, Focus Corporation's integration team will need to make IBM MQ's enterprise data available to a broader set of APIs. This will allow Focus' application developers to subscribe to the data without any risk or impact to the core back-end systems that support the data. Risks can be lowered and the application development process can be decoupled from data retention processes. To do so, Focus' integration team will need to expose the IBM MQ enterprise data using \"event streams.\" Specifically, the integration team (and application developers) will need access to customer order information. This data will be vital for the marketing team's ambitions of making high value promotions to newly-acquired customers in a timely manner. Configuring IBM MQ to clone customer order data Both the order management system and its payment gateway system are currently using IBM MQ message queues for exchanging customer order data. The integration team will need to tap into this message exchange, clone each of the orders being handled across IBM MQ, and then publish those messages into a new event stream. Open the IBM MQ dashboard with a web browser. From the left-hand side interface, drill down into the Manage [A] tab. Select the Queues [A] tab along the top of the page. Scroll down the page until you reach a table, which at this time should show several queues that are already a part of the environment. Create a new queue by clicking the blue Create [A] icon in the top-right corner of the table. Wait for the Queue creation wizard to load, then select Local [A] from Choose queue type . The Quick create [A] option should be selected by default. Under the Queue name [B] field, enter TO.KAFKA (all uppercase). Leave all other settings configured to their default values. When ready, click Create [C] . The web browser will refresh back to the Manage > Queues perspective. From the Queues table, confirm that the TO.KAFKA queue is now available. Next, you need to configure IBM MQ to clone all customer order data that it is exchanging between Focus Corporation's order management system and payment gateway. Messages must be reviewed to verify that they contain the expected payload information. From the Manage > Queues table, locate the queue named PAYMENT.REQ and click the Name [A] to inspect it in more detail. The queue will already have been populated with multiple Messages . Click on any of the Timestamp [A] names available in the table (something similar to Feb 21, 2024 at 5:12:00 PM ) to pull open a Message Details panel. Scroll down the Message Details panel until you reach Application Data [B] Inspect the contents of the packet, which is a series of key-value pairs. You should see details about the order id , customer , customerid , description , price , quantity , region , and ordertime . Close [C] the Message Details panel by clicking the X in the top-right corner or the grey Close button. Back on the PAYMENT.REQ orders summary table, locate the Actions button in the top-right corner of the page. Click to open a drop-down menu and then select View configuration [A] . Multiple attributes of the PAYMENT.REQ order queue can be configured from this page. Click the grey Edit [A] button to the right side of the General page. From the tabs on the left side of the page, drill down into Storage [A] . Scroll down until you reach the Streaming queue name [B] field and change the value to TO.KAFKA . This will direct IBM MQ to clone messages from the PAYMENT.QUE queue into the TO.KAFKA streaming queue created in Step 5. When satisfied, click the blue Save [C] button in the top-right of the page to confirm the configuration changes. Once confirmed, the new configuration will immediately take effect and cloned order messages will promptly begin filling the TO.KAFKA queue. Scroll back up to the top of the page and locate the blue Manage [A] text in the top-left corner of the screen. Click the text to return back to the Manage page for Orders . From the tabs along the top of the page, click the Queues tab. From the table of queues, drill down into the TO.KAFKA queue. Take note of the customer order messages that are now populating the TO.KAFKA message queue table. You can click the circular refresh icon (to the left of the blue Create button) to reload the queue contents. Cloning order queues with IBM Event Streams Focus Corporation's integration team will now need to create an event stream called Orders using IBM Event Streams . This will serve as the repository where messages, cloned from IBM MQ, are published and made available to other parts of the organization. MQ-KAFKA CONNECTOR IBM MQ allows applications, systems, services, and files to request and coordinate processing tasks \u2014 sending and receiving message data via messaging queues. IBM Event Automation 's Kafka integrations makes it possible to capture a continuous stream of events, representing state changes across one or multiple environments, and makes those events persistently available for retrieval. In combination, IBM MQ and Event Automation enable business-critical communications originating over MQ to be captured as events within Kafka topics, which can later be shared across the enterprise for fueling more responsive applications. IBM MQ-Kafka connectors support bi-directional connectivity between these two architectures. Clients of IBM Event Automation are fully supported for using MQ-Kafka connectors with the Event Streams capability of the platform. Support for the MQ-Kafka connector is also available for IBM clients with MQ Advanced entitlements. The integration team will need to make decisions about how to replicate the data (with or without modification) and also determine the appropriate retention settings for this data. Given the governance policies in place at Focus Corporation, they will need to retain data for up to 1 week and replicate entries for high availability. Open the IBM Event Streams tab using your web browser. From the IBM Event Streams dashboard, click the Create a topic [A] tile. The team must first decide on a Topic Name [A] . Set the value to ORDERS and then click the blue Next [B] button to continue. Under the Patitions tab, accept the default recommendation of 1 by clicking Next [A] . Under the Message Retention tab, accept the default recommendation of A week [A] and click Next [B] to continue. Under the Replicas tab, accept the default recommendation of Replication factor: 3 [A] and confirm your selections by clicking the Create Topic [B] button. Configuring a message bridge between IBM MQ and IBM Event Streams Using the Apache Kafka connector framework, Focus Corporation's integration team will now need to configure an \"event bridge\" using Red Hat OpenShift. The task can be performed programmatically via the OpenShift console. The bridge configuration will include connectivity details for accessing both IBM MQ and IBM Event Streams. Once configured and deployed, the bridge will utilize the Apache Kafka connector framework to read messages from the TO.KAFKA message queue and then publish those to the newly-created Orders event stream . Return to the OpenShift container platform dashboard. From the Home page, click the + [A] icon located in the top-right corner of the interface. The interface will load an Import YAML configuration tool, with a black canvas awaiting input. Here you can supply YAML (Yet Another Markup Language) or JSON files to define new deployments on the OpenShift cluster. The YAML definition of the Apache Kafka connector \"bridge\" has been prepared ahead of time. Copy and paste the following YAML exactly as written into the Import YAML canvas: apiVersion : eventstreams.ibm.com/v1beta2 kind : KafkaConnector metadata : name : mq-connector namespace : cp4i labels : eventstreams.ibm.com/cluster : kafka-connect-cluster spec : class : com.ibm.eventstreams.connect.mqsource.MQSourceConnector tasksMax : 1 config : # the Kafka topic to produce to topic : ORDERS # the MQ queue to get messages from mq.queue : TO.KAFKA # connection details for the queue manager mq.queue.manager : orders mq.connection.name.list : orders-ibm-mq(1414) mq.channel.name : SYSTEM.DEF.SVRCONN # format of the messages to transfer mq.message.body.jms : true mq.record.builder : com.ibm.eventstreams.connect.mqsource.builders.JsonRecordBuilder key.converter : org.apache.kafka.connect.storage.StringConverter value.converter : org.apache.kafka.connect.json.JsonConverter # whether to send the schema with the messages key.converter.schemas.enable : false value.converter.schemas.enable : false When ready, click Create [A] . Full deployment should only take a moment. Switch over to the IBM Event Streams tab with your web browser. Having configured the streaming queue in IBM MQ (earlier in Step 18), Focus Corporation's integration team now wants to view the orders that have been generated so far. From the home dashboard of the IBM Event Streams service, click the Topic [A] tab (left-hand side) and then click on the name ORDERS from the table. Granular details about the ORDERS topic will be loaded within the browser. From this page, you can inspect all of the messages (orders) generated from the time you set up the IBM MQ streaming queue configuration earlier. Click any one of the orders to pull up additional details on the payload and its contents. Note that you can inspect details about the quantity , price , customerid , description , id , region , ordertime , and customer . These fields will be valuable later for the marketing team as they look to perform outreach on customers meeting certain criteria. Next Steps In the following module, the integration team will use IBM Event Endpoint Management to import the event stream data and curate a self-service catalog for the marketing department.","title":"1. Creating an event stream from MQ data using Event Streams"},{"location":"1/#creating-an-event-stream-from-an-ibm-mq-message-queue","text":"WATCH THIS DEMONSTRATION Christopher Bienko (Principal, Learning Content Development, IBM Application Modernization) provides a hands-on demonstration of the module contents. Additional ways to watch: Seismic replay available for download. [8 minutes] To take advantage of the message queue data available from IBM MQ, Focus Corporation's integration team will need to make IBM MQ's enterprise data available to a broader set of APIs. This will allow Focus' application developers to subscribe to the data without any risk or impact to the core back-end systems that support the data. Risks can be lowered and the application development process can be decoupled from data retention processes. To do so, Focus' integration team will need to expose the IBM MQ enterprise data using \"event streams.\" Specifically, the integration team (and application developers) will need access to customer order information. This data will be vital for the marketing team's ambitions of making high value promotions to newly-acquired customers in a timely manner.","title":"Creating an event stream from an IBM MQ message queue"},{"location":"1/#_1","text":"","title":""},{"location":"1/#configuring-ibm-mq-to-clone-customer-order-data","text":"Both the order management system and its payment gateway system are currently using IBM MQ message queues for exchanging customer order data. The integration team will need to tap into this message exchange, clone each of the orders being handled across IBM MQ, and then publish those messages into a new event stream. Open the IBM MQ dashboard with a web browser. From the left-hand side interface, drill down into the Manage [A] tab. Select the Queues [A] tab along the top of the page. Scroll down the page until you reach a table, which at this time should show several queues that are already a part of the environment. Create a new queue by clicking the blue Create [A] icon in the top-right corner of the table. Wait for the Queue creation wizard to load, then select Local [A] from Choose queue type . The Quick create [A] option should be selected by default. Under the Queue name [B] field, enter TO.KAFKA (all uppercase). Leave all other settings configured to their default values. When ready, click Create [C] . The web browser will refresh back to the Manage > Queues perspective. From the Queues table, confirm that the TO.KAFKA queue is now available. Next, you need to configure IBM MQ to clone all customer order data that it is exchanging between Focus Corporation's order management system and payment gateway. Messages must be reviewed to verify that they contain the expected payload information. From the Manage > Queues table, locate the queue named PAYMENT.REQ and click the Name [A] to inspect it in more detail. The queue will already have been populated with multiple Messages . Click on any of the Timestamp [A] names available in the table (something similar to Feb 21, 2024 at 5:12:00 PM ) to pull open a Message Details panel. Scroll down the Message Details panel until you reach Application Data [B] Inspect the contents of the packet, which is a series of key-value pairs. You should see details about the order id , customer , customerid , description , price , quantity , region , and ordertime . Close [C] the Message Details panel by clicking the X in the top-right corner or the grey Close button. Back on the PAYMENT.REQ orders summary table, locate the Actions button in the top-right corner of the page. Click to open a drop-down menu and then select View configuration [A] . Multiple attributes of the PAYMENT.REQ order queue can be configured from this page. Click the grey Edit [A] button to the right side of the General page. From the tabs on the left side of the page, drill down into Storage [A] . Scroll down until you reach the Streaming queue name [B] field and change the value to TO.KAFKA . This will direct IBM MQ to clone messages from the PAYMENT.QUE queue into the TO.KAFKA streaming queue created in Step 5. When satisfied, click the blue Save [C] button in the top-right of the page to confirm the configuration changes. Once confirmed, the new configuration will immediately take effect and cloned order messages will promptly begin filling the TO.KAFKA queue. Scroll back up to the top of the page and locate the blue Manage [A] text in the top-left corner of the screen. Click the text to return back to the Manage page for Orders . From the tabs along the top of the page, click the Queues tab. From the table of queues, drill down into the TO.KAFKA queue. Take note of the customer order messages that are now populating the TO.KAFKA message queue table. You can click the circular refresh icon (to the left of the blue Create button) to reload the queue contents.","title":"Configuring IBM MQ to clone customer order data"},{"location":"1/#_2","text":"","title":""},{"location":"1/#cloning-order-queues-with-ibm-event-streams","text":"Focus Corporation's integration team will now need to create an event stream called Orders using IBM Event Streams . This will serve as the repository where messages, cloned from IBM MQ, are published and made available to other parts of the organization. MQ-KAFKA CONNECTOR IBM MQ allows applications, systems, services, and files to request and coordinate processing tasks \u2014 sending and receiving message data via messaging queues. IBM Event Automation 's Kafka integrations makes it possible to capture a continuous stream of events, representing state changes across one or multiple environments, and makes those events persistently available for retrieval. In combination, IBM MQ and Event Automation enable business-critical communications originating over MQ to be captured as events within Kafka topics, which can later be shared across the enterprise for fueling more responsive applications. IBM MQ-Kafka connectors support bi-directional connectivity between these two architectures. Clients of IBM Event Automation are fully supported for using MQ-Kafka connectors with the Event Streams capability of the platform. Support for the MQ-Kafka connector is also available for IBM clients with MQ Advanced entitlements. The integration team will need to make decisions about how to replicate the data (with or without modification) and also determine the appropriate retention settings for this data. Given the governance policies in place at Focus Corporation, they will need to retain data for up to 1 week and replicate entries for high availability. Open the IBM Event Streams tab using your web browser. From the IBM Event Streams dashboard, click the Create a topic [A] tile. The team must first decide on a Topic Name [A] . Set the value to ORDERS and then click the blue Next [B] button to continue. Under the Patitions tab, accept the default recommendation of 1 by clicking Next [A] . Under the Message Retention tab, accept the default recommendation of A week [A] and click Next [B] to continue. Under the Replicas tab, accept the default recommendation of Replication factor: 3 [A] and confirm your selections by clicking the Create Topic [B] button.","title":"Cloning order queues with IBM Event Streams"},{"location":"1/#_3","text":"","title":""},{"location":"1/#configuring-a-message-bridge-between-ibm-mq-and-ibm-event-streams","text":"Using the Apache Kafka connector framework, Focus Corporation's integration team will now need to configure an \"event bridge\" using Red Hat OpenShift. The task can be performed programmatically via the OpenShift console. The bridge configuration will include connectivity details for accessing both IBM MQ and IBM Event Streams. Once configured and deployed, the bridge will utilize the Apache Kafka connector framework to read messages from the TO.KAFKA message queue and then publish those to the newly-created Orders event stream . Return to the OpenShift container platform dashboard. From the Home page, click the + [A] icon located in the top-right corner of the interface. The interface will load an Import YAML configuration tool, with a black canvas awaiting input. Here you can supply YAML (Yet Another Markup Language) or JSON files to define new deployments on the OpenShift cluster. The YAML definition of the Apache Kafka connector \"bridge\" has been prepared ahead of time. Copy and paste the following YAML exactly as written into the Import YAML canvas: apiVersion : eventstreams.ibm.com/v1beta2 kind : KafkaConnector metadata : name : mq-connector namespace : cp4i labels : eventstreams.ibm.com/cluster : kafka-connect-cluster spec : class : com.ibm.eventstreams.connect.mqsource.MQSourceConnector tasksMax : 1 config : # the Kafka topic to produce to topic : ORDERS # the MQ queue to get messages from mq.queue : TO.KAFKA # connection details for the queue manager mq.queue.manager : orders mq.connection.name.list : orders-ibm-mq(1414) mq.channel.name : SYSTEM.DEF.SVRCONN # format of the messages to transfer mq.message.body.jms : true mq.record.builder : com.ibm.eventstreams.connect.mqsource.builders.JsonRecordBuilder key.converter : org.apache.kafka.connect.storage.StringConverter value.converter : org.apache.kafka.connect.json.JsonConverter # whether to send the schema with the messages key.converter.schemas.enable : false value.converter.schemas.enable : false When ready, click Create [A] . Full deployment should only take a moment. Switch over to the IBM Event Streams tab with your web browser. Having configured the streaming queue in IBM MQ (earlier in Step 18), Focus Corporation's integration team now wants to view the orders that have been generated so far. From the home dashboard of the IBM Event Streams service, click the Topic [A] tab (left-hand side) and then click on the name ORDERS from the table. Granular details about the ORDERS topic will be loaded within the browser. From this page, you can inspect all of the messages (orders) generated from the time you set up the IBM MQ streaming queue configuration earlier. Click any one of the orders to pull up additional details on the payload and its contents. Note that you can inspect details about the quantity , price , customerid , description , id , region , ordertime , and customer . These fields will be valuable later for the marketing team as they look to perform outreach on customers meeting certain criteria.","title":"Configuring a message bridge between IBM MQ and IBM Event Streams"},{"location":"1/#_4","text":"","title":""},{"location":"1/#next-steps","text":"In the following module, the integration team will use IBM Event Endpoint Management to import the event stream data and curate a self-service catalog for the marketing department.","title":"Next Steps"},{"location":"2/","text":"Building a self-service event catalog with IBM Event Endpoint Management WATCH THIS DEMONSTRATION Christopher Bienko (Principal, Learning Content Development, IBM Application Modernization) provides a hands-on demonstration of the module contents. Additional ways to watch: Seismic replay available for download. [9 minutes] Having successfully deployed the IBM MQ-Apacha Kafka connector framework, Focus Corporation's integration team have created a bridge between MQ and Event Streams. As new customer order data is processed by the IBM MQ message queue, it is simultaneously cloned into the Orders event stream \u2014 awaiting further analysis and action by other users downstream. In this module, Focus' integration team will utilize IBM Event Endpoint Management (EEM) to import the newly-created event stream into a self-service catalog. EEM supports two use cases: the first is allowing teams (like the integration squad) to publish new event streams; the second is for consumers (such as marketing) to browse the contents of those event streams, without requiring direct intervention from the integration team. WHAT IS THE DIFFERENCE BETWEEN OpenAPI and AsyncAPI? OpenAPI and AsyncAPI both represent formalized specifications for describing APIs, but the data source feeding these APIs and the style in which that data is communicated differs between the two. This is important in the context of IBM Event Endpoint Management. OpenAPI is designed for describing RESTful API endpoints, focusing on synchronous client-server communication: a client sends a request and waits for a direct response from a server. Therefore, OpenAPI is well suited for standard web services where a client interacts with a server in a request-response model. AsyncAPI is built for working with event-driven, asynchronous APIs that rely on asynchronous communication patterns: publish and subscribe, event streaming, and message-driven APIs. It is well suited for Internet of Things (IoT) applications, real-time data processing use cases, or other scenarios where communication doesn't follow the typical direct request-response (synchronous and RESTful) API pattern. Put simply, the key difference between the two is their differing communication patterns. OpenAPI is designed for synchronous messaging (following the request\u2014response pattern) and AsyncAPI is intended for asynchronous communication (event-driven) patterns. IBM Event Endpoint Management\u2014 a composable service within IBM Event Automation \u2014uses AsyncAPI as the standardized way for describing event-driven messages. It allows clients to quickly \"discover\" events across their ecosystem, publish those events across the organization according to governance rules that ensure compliance, and create enforceable policies through event gateways. Like enterprises do today with API gateways, an event gateway provides isolation, abstraction, policy enforcement, and decoupling to asynchronous (event-driven) data sources. Defining new Topics within IBM Event Streams Before the event stream data can be browsed as part of a self-service catalog, the integration team must first import into EEM the ORDER and CUSTOMER streams via Topics created earlier within IBM Event Streams. Since this is the first time that the integration team has needed to import Topics from IBM Event Streams, they must first register the cluster within EEM. Switch over to the EEM tab within your web browser. Click the Hide Welcome [A] text to collapse the tutorials at the top of the page. From the left-hand side of the interface, click Topic [B] . From the right side of the interface, click the blue Add Topic button. A configuration tool for importing a Topic into EEM will load. First, define details regarding the Cluster connection to the IBM Event Streams environment. Under the Add a new cluster header, click the Add new cluster [A] button. Set the Cluster Name [A] variable to IBM Event Streams and click Next [B] to continue. Additional details about the connection are required: the endpoint address, certificates required for secure communication with the cluster, and authentication credentials. Set the Servers [A] variable to ademo-es-kafka-bootstrap.cp4i.svc:9095 Click Next [B] to continue Click (to add a checkmark) on the Accept all certificates [A] toggle, then click Next [B] . Next, configure details regarding Credentials . The Security Protocol field should remain at the default value of SCRAM-SHA-512 Username [A] should be set to es-admin Password [B] must match the IBM Event Streams password generated in the Setup module. This password is unique to your particular environment. When ready, click Add Cluster [C] The web browser will return to the Add Topic configuration page from Step 2. Under the Cluster connection [A] header, select the IBM Event Streams connection defined earlier When ready, click Next [B] Now select the Topics that will be made available for browsing within the Event Endpoint Management catalog. Toggle (select) both the CUSTOMERS [A] and ORDERS [B] topics. You may keep the alias values as their recommended defaults. When ready, click Add Topic [C] Defining a data schema for CUSTOMERS With the connections to IBM Event Streams defined and the Kafka Topics selected for replication, Focus' integration team is ready to create self-service event stream catalogs within EEM. The team must provide descriptions of the event streams, including an example of how the data is structured and formatted. This description will be beneficial to other individuals within the company (such as the marketing department), who will want to know at-a-glance whether a particular Topic catalog fits their needs. From the EEM dashboad, drill down into Topics [A] as before and click the name of the CUSTOMERS [B] topic. Click the blue Edit information [A] on the right-hand side of the page. Within the Overview information tab: Scroll down until you reach the Description [A] field and enter the following text: Events generated by the customer management system. A new event is created for each new user registration. Scroll further down until you reach the Tags [B] field and enter customer For the Contact Email [C] field, enter customerservice@focus.corp Click the Event Information [D] tab from the left side of the interface DO NOT CLICK SAVE Within the Event information [A] tab, scroll down until you locate the Sample message [B] field. Here you will provide a representation of a typical message on the CUSTOMERS event stream. Copy and paste the following JSON into the Sample message field: { \"customerid\" : \"acb3eb65-98a1-45c2-84d4-f5df157862b4\" , \"customername\" : \"Emilio Quitzon\" , \"registered\" : \"2023-10-24 19:20:35.638\" } When ready, click Save [C] to finalize both the Overview Information and Event Information configuration settings. The web browser will return to the CUSTOMERS Topic overview. From the three tabs at the top of the screen, click Options [A] Click the blue Create Options [B] button Within the Details tab, provide the following customizations: Order name [A] : Customer Access Alias [B] : CUSTOMERS Description [C] : Self-service access to the customer event stream When ready, click Next [D] . Leave the Controls tab empty ( No data ) and click Next [A] to proceed. The Publish option tab will give you a final opportunity to review the CUSTOMERS topic before publishing for other users of EEM. Within the blue Option Status field, click the Publish [A] button Further down the panel, under the Available gateways [B] field, click (toggle on) production When ready, click Save [C] to finalize publication of the CUSTOMERS topic Defining a data schema for ORDERS The process of publishing an event streams Topic within EEM (Steps 9\u201316) must now be repeated\u2014 with some slight modifications \u2014for the ORDERS topic. From the EEM dashboard, drill down into Topics [A] and then click the name ORDERS [B] . Click on the Edit information button. Within the Overview information tab: Scroll down until you reach the Description [A] field and enter the following text: Events generated by the customer management system. An event will be emitted for every new order that is made. Scroll further down until you reach the Tags [B] field and enter orders For the Contact Email [C] field, enter orders@focus.corp Click the Event Information [D] tab from the left side of the interface DO NOT CLICK SAVE Within the Event information tab, scroll down until you locate the Sample message [A] field. Here you will provide a representation of a typical message on the ORDERS event stream. Copy and paste the following JSON into the field: { \"quantity\" : 9 , \"price\" : 197.09 , \"customerid\" : \"a7d1586b-ced1-462f-9e44-14e9e5013540\" , \"description\" : \"Composite Oversize 28in Tennis Racket\" , \"id\" : \"1eba7af9-b748-4754-b750-3459e589dccf\" , \"region\" : \"EMEA\" , \"ordertime\" : \"2023-10-24 19:26:04.839\" , \"customer\" : \"Reed McKenzie DDS\" } When ready, click Save [B] to finalize both the Overview Information and Event Information configuration settings. The web browser will return to the ORDERS Topic overview. From the three tabs at the top of the screen, click Options [A] Click the blue Create Options [B] button Within the Details tab, provide the following customizations: Order name [A] : Order Access Alias [B] : ORDERS Description [C] : Self-service access to the order event stream When ready, click Next [D] . Leave the Controls tab empty ( No data ) and click Next to proceed. The Publish option tab will give you a final opportunity to review the ORDERS topic before publishing for other users of EEM. Within the blue Option Status field, click the Publish [A] button Further down the panel, under the Available gateways [B] field, click (toggle on) production When ready, click Save [C] to finalize publication of the ORDERS topic Validating the configuration changes and generating access credentials Before handing access over to Focus Corporation's marketing team, the integration squad must first validate that the catalogued event streams are performing as expected. If no issues are detected, they can generate access credentials for users to securely access the catalog. Return to the EEM dashboard and drill down into Catalog [A] , then click the name CUSTOMERS [B] from the table. Click the Generate access credentials [A] button in the top-right corner of the page. Provide an email address for the Contact details field: marketing@focus.corp When ready, click Generate . Access credentials for CUSTOMERS will be generated immediately and displayed within the web browser. Record the Username [A] and Password [B] values to a notepad for reference later. Alternatively, save the credentials as a JSON file (using the Download as JSON button) on your local machine. RECORD THE ACCESS CREDENTIALS Don't proceed with the lab guide instructions before recording the CUSTOMERS access credentials. These values cannot be referenced again once this window is closed. You can always generate new credentials again later, but it will require repeating the previous steps. Generate access credentials for the ORDERS catalog in the same way done for CUSTOMERS : Return to the EEM dashboard Drill down into Catalog and then click ORDERS Click the Generate access credentials button. Provide an email address for the Contact details field: marketing@focus.corp When ready, click Generate . Access credentials for ORDERS will be generated immediately within the web browser: Record the Username and Password values to a notepad for reference later. Alternatively, save the credentials as a JSON file (using the Download as JSON button) on your local machine. At this stage, responsibility can shift towards Focus Corporation's marketing team. Thanks to the hard work of the integration squad, the marketing department now has a fully configured, self-service catalog of event stream data available within EEM. Next Steps In the following module, Focus' marketing team will use IBM Event Processing to correlate newly-created customer accounts with orders totaling over $100 within a 24-hour time window.","title":"2. Building a self-service catalog of events with Event Endpoint Management"},{"location":"2/#building-a-self-service-event-catalog-with-ibm-event-endpoint-management","text":"WATCH THIS DEMONSTRATION Christopher Bienko (Principal, Learning Content Development, IBM Application Modernization) provides a hands-on demonstration of the module contents. Additional ways to watch: Seismic replay available for download. [9 minutes] Having successfully deployed the IBM MQ-Apacha Kafka connector framework, Focus Corporation's integration team have created a bridge between MQ and Event Streams. As new customer order data is processed by the IBM MQ message queue, it is simultaneously cloned into the Orders event stream \u2014 awaiting further analysis and action by other users downstream. In this module, Focus' integration team will utilize IBM Event Endpoint Management (EEM) to import the newly-created event stream into a self-service catalog. EEM supports two use cases: the first is allowing teams (like the integration squad) to publish new event streams; the second is for consumers (such as marketing) to browse the contents of those event streams, without requiring direct intervention from the integration team. WHAT IS THE DIFFERENCE BETWEEN OpenAPI and AsyncAPI? OpenAPI and AsyncAPI both represent formalized specifications for describing APIs, but the data source feeding these APIs and the style in which that data is communicated differs between the two. This is important in the context of IBM Event Endpoint Management. OpenAPI is designed for describing RESTful API endpoints, focusing on synchronous client-server communication: a client sends a request and waits for a direct response from a server. Therefore, OpenAPI is well suited for standard web services where a client interacts with a server in a request-response model. AsyncAPI is built for working with event-driven, asynchronous APIs that rely on asynchronous communication patterns: publish and subscribe, event streaming, and message-driven APIs. It is well suited for Internet of Things (IoT) applications, real-time data processing use cases, or other scenarios where communication doesn't follow the typical direct request-response (synchronous and RESTful) API pattern. Put simply, the key difference between the two is their differing communication patterns. OpenAPI is designed for synchronous messaging (following the request\u2014response pattern) and AsyncAPI is intended for asynchronous communication (event-driven) patterns. IBM Event Endpoint Management\u2014 a composable service within IBM Event Automation \u2014uses AsyncAPI as the standardized way for describing event-driven messages. It allows clients to quickly \"discover\" events across their ecosystem, publish those events across the organization according to governance rules that ensure compliance, and create enforceable policies through event gateways. Like enterprises do today with API gateways, an event gateway provides isolation, abstraction, policy enforcement, and decoupling to asynchronous (event-driven) data sources.","title":"Building a self-service event catalog with IBM Event Endpoint Management"},{"location":"2/#_1","text":"","title":""},{"location":"2/#defining-new-topics-within-ibm-event-streams","text":"Before the event stream data can be browsed as part of a self-service catalog, the integration team must first import into EEM the ORDER and CUSTOMER streams via Topics created earlier within IBM Event Streams. Since this is the first time that the integration team has needed to import Topics from IBM Event Streams, they must first register the cluster within EEM. Switch over to the EEM tab within your web browser. Click the Hide Welcome [A] text to collapse the tutorials at the top of the page. From the left-hand side of the interface, click Topic [B] . From the right side of the interface, click the blue Add Topic button. A configuration tool for importing a Topic into EEM will load. First, define details regarding the Cluster connection to the IBM Event Streams environment. Under the Add a new cluster header, click the Add new cluster [A] button. Set the Cluster Name [A] variable to IBM Event Streams and click Next [B] to continue. Additional details about the connection are required: the endpoint address, certificates required for secure communication with the cluster, and authentication credentials. Set the Servers [A] variable to ademo-es-kafka-bootstrap.cp4i.svc:9095 Click Next [B] to continue Click (to add a checkmark) on the Accept all certificates [A] toggle, then click Next [B] . Next, configure details regarding Credentials . The Security Protocol field should remain at the default value of SCRAM-SHA-512 Username [A] should be set to es-admin Password [B] must match the IBM Event Streams password generated in the Setup module. This password is unique to your particular environment. When ready, click Add Cluster [C] The web browser will return to the Add Topic configuration page from Step 2. Under the Cluster connection [A] header, select the IBM Event Streams connection defined earlier When ready, click Next [B] Now select the Topics that will be made available for browsing within the Event Endpoint Management catalog. Toggle (select) both the CUSTOMERS [A] and ORDERS [B] topics. You may keep the alias values as their recommended defaults. When ready, click Add Topic [C]","title":"Defining new Topics within IBM Event Streams"},{"location":"2/#_2","text":"","title":""},{"location":"2/#defining-a-data-schema-for-customers","text":"With the connections to IBM Event Streams defined and the Kafka Topics selected for replication, Focus' integration team is ready to create self-service event stream catalogs within EEM. The team must provide descriptions of the event streams, including an example of how the data is structured and formatted. This description will be beneficial to other individuals within the company (such as the marketing department), who will want to know at-a-glance whether a particular Topic catalog fits their needs. From the EEM dashboad, drill down into Topics [A] as before and click the name of the CUSTOMERS [B] topic. Click the blue Edit information [A] on the right-hand side of the page. Within the Overview information tab: Scroll down until you reach the Description [A] field and enter the following text: Events generated by the customer management system. A new event is created for each new user registration. Scroll further down until you reach the Tags [B] field and enter customer For the Contact Email [C] field, enter customerservice@focus.corp Click the Event Information [D] tab from the left side of the interface DO NOT CLICK SAVE Within the Event information [A] tab, scroll down until you locate the Sample message [B] field. Here you will provide a representation of a typical message on the CUSTOMERS event stream. Copy and paste the following JSON into the Sample message field: { \"customerid\" : \"acb3eb65-98a1-45c2-84d4-f5df157862b4\" , \"customername\" : \"Emilio Quitzon\" , \"registered\" : \"2023-10-24 19:20:35.638\" } When ready, click Save [C] to finalize both the Overview Information and Event Information configuration settings. The web browser will return to the CUSTOMERS Topic overview. From the three tabs at the top of the screen, click Options [A] Click the blue Create Options [B] button Within the Details tab, provide the following customizations: Order name [A] : Customer Access Alias [B] : CUSTOMERS Description [C] : Self-service access to the customer event stream When ready, click Next [D] . Leave the Controls tab empty ( No data ) and click Next [A] to proceed. The Publish option tab will give you a final opportunity to review the CUSTOMERS topic before publishing for other users of EEM. Within the blue Option Status field, click the Publish [A] button Further down the panel, under the Available gateways [B] field, click (toggle on) production When ready, click Save [C] to finalize publication of the CUSTOMERS topic","title":"Defining a data schema for CUSTOMERS"},{"location":"2/#_3","text":"","title":""},{"location":"2/#defining-a-data-schema-for-orders","text":"The process of publishing an event streams Topic within EEM (Steps 9\u201316) must now be repeated\u2014 with some slight modifications \u2014for the ORDERS topic. From the EEM dashboard, drill down into Topics [A] and then click the name ORDERS [B] . Click on the Edit information button. Within the Overview information tab: Scroll down until you reach the Description [A] field and enter the following text: Events generated by the customer management system. An event will be emitted for every new order that is made. Scroll further down until you reach the Tags [B] field and enter orders For the Contact Email [C] field, enter orders@focus.corp Click the Event Information [D] tab from the left side of the interface DO NOT CLICK SAVE Within the Event information tab, scroll down until you locate the Sample message [A] field. Here you will provide a representation of a typical message on the ORDERS event stream. Copy and paste the following JSON into the field: { \"quantity\" : 9 , \"price\" : 197.09 , \"customerid\" : \"a7d1586b-ced1-462f-9e44-14e9e5013540\" , \"description\" : \"Composite Oversize 28in Tennis Racket\" , \"id\" : \"1eba7af9-b748-4754-b750-3459e589dccf\" , \"region\" : \"EMEA\" , \"ordertime\" : \"2023-10-24 19:26:04.839\" , \"customer\" : \"Reed McKenzie DDS\" } When ready, click Save [B] to finalize both the Overview Information and Event Information configuration settings. The web browser will return to the ORDERS Topic overview. From the three tabs at the top of the screen, click Options [A] Click the blue Create Options [B] button Within the Details tab, provide the following customizations: Order name [A] : Order Access Alias [B] : ORDERS Description [C] : Self-service access to the order event stream When ready, click Next [D] . Leave the Controls tab empty ( No data ) and click Next to proceed. The Publish option tab will give you a final opportunity to review the ORDERS topic before publishing for other users of EEM. Within the blue Option Status field, click the Publish [A] button Further down the panel, under the Available gateways [B] field, click (toggle on) production When ready, click Save [C] to finalize publication of the ORDERS topic","title":"Defining a data schema for ORDERS"},{"location":"2/#_4","text":"","title":""},{"location":"2/#validating-the-configuration-changes-and-generating-access-credentials","text":"Before handing access over to Focus Corporation's marketing team, the integration squad must first validate that the catalogued event streams are performing as expected. If no issues are detected, they can generate access credentials for users to securely access the catalog. Return to the EEM dashboard and drill down into Catalog [A] , then click the name CUSTOMERS [B] from the table. Click the Generate access credentials [A] button in the top-right corner of the page. Provide an email address for the Contact details field: marketing@focus.corp When ready, click Generate . Access credentials for CUSTOMERS will be generated immediately and displayed within the web browser. Record the Username [A] and Password [B] values to a notepad for reference later. Alternatively, save the credentials as a JSON file (using the Download as JSON button) on your local machine. RECORD THE ACCESS CREDENTIALS Don't proceed with the lab guide instructions before recording the CUSTOMERS access credentials. These values cannot be referenced again once this window is closed. You can always generate new credentials again later, but it will require repeating the previous steps. Generate access credentials for the ORDERS catalog in the same way done for CUSTOMERS : Return to the EEM dashboard Drill down into Catalog and then click ORDERS Click the Generate access credentials button. Provide an email address for the Contact details field: marketing@focus.corp When ready, click Generate . Access credentials for ORDERS will be generated immediately within the web browser: Record the Username and Password values to a notepad for reference later. Alternatively, save the credentials as a JSON file (using the Download as JSON button) on your local machine. At this stage, responsibility can shift towards Focus Corporation's marketing team. Thanks to the hard work of the integration squad, the marketing department now has a fully configured, self-service catalog of event stream data available within EEM.","title":"Validating the configuration changes and generating access credentials"},{"location":"2/#_5","text":"","title":""},{"location":"2/#next-steps","text":"In the following module, Focus' marketing team will use IBM Event Processing to correlate newly-created customer accounts with orders totaling over $100 within a 24-hour time window.","title":"Next Steps"},{"location":"3/","text":"Using IBM Event Processing no-code editors to configure the solution WATCH THIS DEMONSTRATION Christopher Bienko (Principal, Learning Content Development, IBM Application Modernization) provides a hands-on demonstration of the module contents. Additional ways to watch: Seismic replay available for download. [14 minutes] Focus Corporation's marketing department now has a fully configured, self-service catalog of event stream data available within IBM Event Endpoint Management (EEM) . At this stage, work will transition from the integration team to the domain of the marketing team. Focus' marketing team will utilize IBM Event Processing (EP) to correlate any newly-created customer accounts with first-time purchase orders that total over $100 USD in value. Furthermore, these correlated transactions will need to be identified within a 24-hour window of when they occur. If these conditions can be met, the marketing team will be able to create an actionable plan for putting high-value promotional offers in front of first-time customers in a timely manner \u2014 which should translate to more sales revenue through Focus' digital storefront. With your web browser, access the IBM Event Processing dashboard. Dismiss any welcome screens or tutorials that are prompted after first logging in by clicking the Hide Welcome [A] text in the bottom-left corner of the interface. In the bottom-right corner of the page, click the blue Create [B] to begin defining a new flow . EVENT PROCESSING FLOWS An event processing flow ingests a stream of events, analyzes that input, and takes automatic actions (defined by the user) in response to those conditions. A flow begins with one or multiple event sources which represent inbound events. For the purposes of this demonstration, you will leverage the ORDERS event stream that was defined in the previous module. You will do so using EP's intuitive, no-code authoring canvas. For Name [A] , specify NewCustomerLargeOrder and then click Create [B] . Configuring an event source for Orders Before the inbound events can be captured, connectivity details for the event source node ( ORDERS ) must be configured within the authoring canvas. Wait until the page has refreshed to display the no-code environment. Along the left side are tools and actions that can be dragged into the authoring canvas on the right Within the canvas, you will see an Event source [A] node with default name of source_1 With your cursor, hover over the source_1 node and then click the Edit [B] (pencil) icon in the top-right corner of the tile to begin modifying the node The Configure event source wizard will load within your browser. Under the Add an event source or pick a recent one header, select Add event source [A] Click Next [B] Switch back to the Event Endpoint Management tab and click Catalog from the left-hand interface. Drill down into the ORDERS [A] topic by clicking on the name Scroll down the topic summary page until you reach the Access Information section Just below, within the Servers table, is the full address of the ORDERS event topic Copy the full address ( ademo-event-gw-ibm... ) [B] to your notepad for reference later When ready, switch back to the IBM Event Processing tab with your web browser Back within EP: the first tab, Details , needs additional information to configure the node. Scroll down to the Node name field and enter Orders Further down, under the Connect to Kafka cluster header, paste the full address copied in Step 6 into the Server [A] field Click Next The event source must be configured to allow certificates used by the event stream. Accept certificates [B] (by clicking the toggle button as shown in the screenshot below) Click Next [C] The second tab, Access credentials , requires authentication details for the EEM event source. Keep the Select security mechanism variable as its default value: PLAIN For Username [A] and Password [B] , supply the credentials for the ORDERS topic (Step 32 of Module 2) Click Next [C] The third tab, Topic selection , will prompt the user on which topic(s) to ingest from the event source. Available topics will already be displayed within the configuration tool Under the Topic selection header, click the button next to the ORDERS [A] topic to select it Click Next [B] The fourth tab, Define event structure , requires a data schema or sample message in order to understand the expected structure of events ingested from the event source. Under the No defined structure header, click the blue Upload a schema or sample message [A] button to pull open an editor Users can input schemas in either Avro or JSON data formats: click the JSON [B] tab Copy and paste the following JSON into the sample message field. Note that this is the same sample schema you supplied when configuring the event source within EEM in Module 2. When ready, click Done [C] to proceed { \"quantity\" : 9 , \"price\" : 197.09 , \"customerid\" : \"a7d1586b-ced1-462f-9e44-14e9e5013540\" , \"description\" : \"Composite Oversize 28in Tennis Racket\" , \"id\" : \"1eba7af9-b748-4754-b750-3459e589dccf\" , \"region\" : \"EMEA\" , \"ordertime\" : \"2023-10-24 19:26:04.839\" , \"customer\" : \"Reed McKenzie DDS\" } You will have a final opportunity to review and confirm the event source configuration. Click Configure [A] to finalize the configuration of the Orders event source. Configuring an event source for New Customers The configurations performed for the ORDERS topic now (Steps 4 \u2013 12) must now be repeated for the CUSTOMERS topic. Return to the NewCustomerLargeOrder [A] canvas and drag a new Event Source [B] (from the left-hand interface) into the canvas, placing it just below the Orders source. With your cursor, hover over the new source_1 node and then click the Edit (pencil) icon in the top-right corner of the tile to begin modifying the node. The Configure event source wizard will load within your browser. Under the Add an event source or pick a recent one header, select Add event source [A] Click Next [B] Switch back to the Event Endpoint Management tab and click Catalog from the left-hand interface. Drill down into the CUSTOMERS topic by clicking on the name Scroll down the topic summary page until you reach the Access Information section Just below, within the Servers table, is the full address of the CUSTOMERS event topic Copy the full address ( ademo-event-gw-ibm... ) to your notepad for reference later When ready, switch back to the IBM Event Processing tab with your web browser Back within EP: the first tab, Details , needs additional information to configure the node. Scroll down to the Node name [A] field and enter New Customers Further down, under the Connect to Kafka cluster header, paste the full address copied in Step 15 into the Server [B] field Click Next [C] The event source must be configured to allow certificates used by the event stream. Accept certificates (by clicking the toggle button as shown in the screenshot below) Click Next The second tab, Access credentials , requires authentication details for the EEM event source. Keep the Select security mechanism variable as its default value: PLAIN For Username and Password , supply the credentials for the CUSTOMERS topic (Step 28 of Module 2) Click Next The third tab, Topic selection , will prompt the user on which topic(s) to ingest from the event source. Available topics will already be displayed within the configuration tool Under the Topic selection header, click the button next to the CUSTOMERS [A] topic to select it Click Next [B] The fourth tab, Define event structure , requires a data schema or sample message in order to understand the expected structure of events ingested from the event source. Under the No defined structure header, click the blue Upload a schema or sample message button to pull open an editor Users can input schemas in either Avro or JSON data formats: click the JSON [A] tab Copy and paste the following JSON into the sample message field. Note that this is the same sample schema you supplied when configuring the event source within EEM in Module 2. When ready, click Done [B] to proceed { \"customerid\" : \"acb3eb65-98a1-45c2-84d4-f5df157862b4\" , \"customername\" : \"Emilio Quitzon\" , \"registered\" : \"2023-10-24 19:20:35.638\" } You will have a final opportunity to review and confirm the event source configuration. Click Configure [A] to finalize the configuration of the New Customers event source. Filtering event sources to match specific criteria With the event source fully configured within EP, you can now begin tailoring the event processing flow to filter on orders where the value exceeds $100 USD. Scroll down within the list of nodes and actions on the left-side interface until you locate the Filter node. Click and hold the Filter [A] node, then drag it to the authoring canvas on the right Position it to the right side of the Orders node The output terminal for Orders needs to be connected as input to the filter_1 node. With your mouse pointer, hover over the Orders node and take note of the grey dot (labelled Output Port [B] ) at the right edge of the tile Click and hold the Output Port (right-most) of Orders and drag it to the Input Port [C] (left-most edge) of filter_1 , then release your hold to complete the circuit With the Orders event source node connected to the filter_1 operation node, you must utilize the Filter node's expression builder to remove orders that are less than $100 USD in value. This will allow the marketing team's application to target only first-time customers with spending patterns of $100 USD or more. Hover over the filter_1 node with your mouse pointer and click the Edit [D] (pencil) icon. Within the Details tab, specify the value of Node name as FilterLargeOrders When ready, click Next . The Define filter tab will load next. To the right of the Filter expression field, click the Assistant [A] drop-down menu to expose additional options. Event Processing's expression builder will guide you in defining the filter. Property to filter on: price Condition: Is greater than Value: 100 Click the Add to expression button The expression builder will automatically translate your statements into a syntactically-correct filter statement: 'price' > 100 Click the Configure [B] button at the bottom of the panel to finalize the filter configuration Performing a JOIN across two event sources The marketing team needs to \"JOIN\" the filtered Order stream data to records of New Customers in order to correctly identify first-time purchasers that placed an order for over $100 USD in the last 24 hours. Use EP's authoring canvas to carry out the necessary JOIN operation. Back on the canvas for NewCustomerLargeOrders , scroll down along the left-hand interface until you locate the Interval Join [A] node. Drag and drop this node [B] at the far right-hand edge of the authoring canvas. With your cursor, hover over the New Customers output terminal and locate the Output Port (grey) button. Click and hold the New Customers Output Port [A] edge and drag this to the Input Port [B] edge of the newly-created intervalJoin_1 node Release your cursor to finalize the connection Repeat the process, this time connecting the FilterLargeOrders node's Output Port [A] to the intervalJoin_1 node's Input Port [B] . At this stage, the input of intervalJoin_1 should be connected to the outputs of both the FilterLargeOrders and New Customers nodes. The JOIN node must now be configured to correlate events based on a shared customerid field within the two event streams ( New Customers and pre-filtered Orders ). With your cursor, hover over the intervalJoin_1 node and click the Edit (pencil) icon. Within the Details tab, set Node name [A] equal to DetectNewCustomerLargeOrders When ready, click Next [B] . Within the Join conditions tab, activate the expression builder by click the Assistant [A] drop-down menu. Set the following properties: Specify property from 'New Customers': customerid Specify property from 'FilterLargeOrders': customerid When ready, click Add to expression [B] The syntactically-correct JOIN expression will be expressed under the Define events field: 'New Customers'.'customerid' = 'FilterLargeOrders'.'customerid' After you have reviewed the JOIN condition, click Next [C] TRIGGERING EVENTS AND DETECTED EVENTS Two concepts are important to understand at this time: Triggering events and Detected events . Since the New Customer sign-up event must logically occur before a purchase, it is considered a triggering event Therefore, when a signal for an Order purchase of over $100 (the detected event ) is received, the JOIN logic condition will be triggered for the two event streams Under the Time window condition , you can define the time interval \"window\" where detected events are considered viable for a promotional offer from the marketing team. To meet the criteria, a purchase of over $100 USD must be made by a first-time customer within a 24 hour window of creating an account. Look for the Event to detect [A] field and select the FilterLargeOrders (event_time) option from the drop-down menu. A Preview visualization of the time window interval is rendered within the panel. For now, with no adjustments made, this will appear as a vertical bar at 0 (X-axis). Scroll down further to reveal additional attributes to edit. Event to set the time window: New Customers (event-time) Offset from event to start the time window: 0 HOUR(S) Offset from event to end the time window [B] : 24 HOUR(S) When ready, click Next [C] The JOIN node's output going forward will be a combination of fields from both the filtered Orders and New Customers events streams. Before finalizing the interval join, you need to clean up the output so that duplicate fields (like customerid and event_time ) are not included from the JOIN operation. You have the option of renaming or removing the duplicate fields \u2014 but for the sake of this demonstration, you will be removing the fields. Within the Output properties tab, locate the customerid [A] field. There are two (duplicate) copies of the same field at this time. Your objective will be to delete ONE of the two duplicate records, where the Source is labelled as New Customers . Preserve the record where Source is labelled as FilterLargeOrders . ONLY REMOVE DUPLICATES WHERE SOURCE == New Customers When reviewing Output properties for duplicate fields, take note of the table's Source column. Duplicates will be produced by both the New Customers and FilterLargeOrders nodes. Only remove duplicates where the Source is listed as New Customers . Click the round \u2014 sign to the left of the first customerid row, where the Source is labelled as \"New Customers\", to remove the duplicate. The result will be only one copy of the customerid field in the table. The red text ( Must be unique ) underlining the field will disappear from the table. Repeat the procedure for the event_time field, which is also duplicated by the JOIN: Delete the event_time row where the Source column is labelled as \"New Customers\". With all duplicates now removed from the table, the Configure [A] button will now be highlighted in blue and selectable. Click the Configure button to finalize the interval join configuration. Next Steps With filtering and processing steps now in place for incoming event streams, Focus Corporation's marketing team is able to detect valid promotional opportunities and emit those detected events to their customer loyalty application. In the following section, you will create those linkages and validate its effectiveness using \"historical\" and \"live\" data feeds.","title":"3. Configuring the solution with Event Processing no-code editors"},{"location":"3/#using-ibm-event-processing-no-code-editors-to-configure-the-solution","text":"WATCH THIS DEMONSTRATION Christopher Bienko (Principal, Learning Content Development, IBM Application Modernization) provides a hands-on demonstration of the module contents. Additional ways to watch: Seismic replay available for download. [14 minutes] Focus Corporation's marketing department now has a fully configured, self-service catalog of event stream data available within IBM Event Endpoint Management (EEM) . At this stage, work will transition from the integration team to the domain of the marketing team. Focus' marketing team will utilize IBM Event Processing (EP) to correlate any newly-created customer accounts with first-time purchase orders that total over $100 USD in value. Furthermore, these correlated transactions will need to be identified within a 24-hour window of when they occur. If these conditions can be met, the marketing team will be able to create an actionable plan for putting high-value promotional offers in front of first-time customers in a timely manner \u2014 which should translate to more sales revenue through Focus' digital storefront. With your web browser, access the IBM Event Processing dashboard. Dismiss any welcome screens or tutorials that are prompted after first logging in by clicking the Hide Welcome [A] text in the bottom-left corner of the interface. In the bottom-right corner of the page, click the blue Create [B] to begin defining a new flow . EVENT PROCESSING FLOWS An event processing flow ingests a stream of events, analyzes that input, and takes automatic actions (defined by the user) in response to those conditions. A flow begins with one or multiple event sources which represent inbound events. For the purposes of this demonstration, you will leverage the ORDERS event stream that was defined in the previous module. You will do so using EP's intuitive, no-code authoring canvas. For Name [A] , specify NewCustomerLargeOrder and then click Create [B] .","title":"Using IBM Event Processing no-code editors to configure the solution"},{"location":"3/#_1","text":"","title":""},{"location":"3/#configuring-an-event-source-for-orders","text":"Before the inbound events can be captured, connectivity details for the event source node ( ORDERS ) must be configured within the authoring canvas. Wait until the page has refreshed to display the no-code environment. Along the left side are tools and actions that can be dragged into the authoring canvas on the right Within the canvas, you will see an Event source [A] node with default name of source_1 With your cursor, hover over the source_1 node and then click the Edit [B] (pencil) icon in the top-right corner of the tile to begin modifying the node The Configure event source wizard will load within your browser. Under the Add an event source or pick a recent one header, select Add event source [A] Click Next [B] Switch back to the Event Endpoint Management tab and click Catalog from the left-hand interface. Drill down into the ORDERS [A] topic by clicking on the name Scroll down the topic summary page until you reach the Access Information section Just below, within the Servers table, is the full address of the ORDERS event topic Copy the full address ( ademo-event-gw-ibm... ) [B] to your notepad for reference later When ready, switch back to the IBM Event Processing tab with your web browser Back within EP: the first tab, Details , needs additional information to configure the node. Scroll down to the Node name field and enter Orders Further down, under the Connect to Kafka cluster header, paste the full address copied in Step 6 into the Server [A] field Click Next The event source must be configured to allow certificates used by the event stream. Accept certificates [B] (by clicking the toggle button as shown in the screenshot below) Click Next [C] The second tab, Access credentials , requires authentication details for the EEM event source. Keep the Select security mechanism variable as its default value: PLAIN For Username [A] and Password [B] , supply the credentials for the ORDERS topic (Step 32 of Module 2) Click Next [C] The third tab, Topic selection , will prompt the user on which topic(s) to ingest from the event source. Available topics will already be displayed within the configuration tool Under the Topic selection header, click the button next to the ORDERS [A] topic to select it Click Next [B] The fourth tab, Define event structure , requires a data schema or sample message in order to understand the expected structure of events ingested from the event source. Under the No defined structure header, click the blue Upload a schema or sample message [A] button to pull open an editor Users can input schemas in either Avro or JSON data formats: click the JSON [B] tab Copy and paste the following JSON into the sample message field. Note that this is the same sample schema you supplied when configuring the event source within EEM in Module 2. When ready, click Done [C] to proceed { \"quantity\" : 9 , \"price\" : 197.09 , \"customerid\" : \"a7d1586b-ced1-462f-9e44-14e9e5013540\" , \"description\" : \"Composite Oversize 28in Tennis Racket\" , \"id\" : \"1eba7af9-b748-4754-b750-3459e589dccf\" , \"region\" : \"EMEA\" , \"ordertime\" : \"2023-10-24 19:26:04.839\" , \"customer\" : \"Reed McKenzie DDS\" } You will have a final opportunity to review and confirm the event source configuration. Click Configure [A] to finalize the configuration of the Orders event source.","title":"Configuring an event source for Orders"},{"location":"3/#_2","text":"","title":""},{"location":"3/#configuring-an-event-source-for-new-customers","text":"The configurations performed for the ORDERS topic now (Steps 4 \u2013 12) must now be repeated for the CUSTOMERS topic. Return to the NewCustomerLargeOrder [A] canvas and drag a new Event Source [B] (from the left-hand interface) into the canvas, placing it just below the Orders source. With your cursor, hover over the new source_1 node and then click the Edit (pencil) icon in the top-right corner of the tile to begin modifying the node. The Configure event source wizard will load within your browser. Under the Add an event source or pick a recent one header, select Add event source [A] Click Next [B] Switch back to the Event Endpoint Management tab and click Catalog from the left-hand interface. Drill down into the CUSTOMERS topic by clicking on the name Scroll down the topic summary page until you reach the Access Information section Just below, within the Servers table, is the full address of the CUSTOMERS event topic Copy the full address ( ademo-event-gw-ibm... ) to your notepad for reference later When ready, switch back to the IBM Event Processing tab with your web browser Back within EP: the first tab, Details , needs additional information to configure the node. Scroll down to the Node name [A] field and enter New Customers Further down, under the Connect to Kafka cluster header, paste the full address copied in Step 15 into the Server [B] field Click Next [C] The event source must be configured to allow certificates used by the event stream. Accept certificates (by clicking the toggle button as shown in the screenshot below) Click Next The second tab, Access credentials , requires authentication details for the EEM event source. Keep the Select security mechanism variable as its default value: PLAIN For Username and Password , supply the credentials for the CUSTOMERS topic (Step 28 of Module 2) Click Next The third tab, Topic selection , will prompt the user on which topic(s) to ingest from the event source. Available topics will already be displayed within the configuration tool Under the Topic selection header, click the button next to the CUSTOMERS [A] topic to select it Click Next [B] The fourth tab, Define event structure , requires a data schema or sample message in order to understand the expected structure of events ingested from the event source. Under the No defined structure header, click the blue Upload a schema or sample message button to pull open an editor Users can input schemas in either Avro or JSON data formats: click the JSON [A] tab Copy and paste the following JSON into the sample message field. Note that this is the same sample schema you supplied when configuring the event source within EEM in Module 2. When ready, click Done [B] to proceed { \"customerid\" : \"acb3eb65-98a1-45c2-84d4-f5df157862b4\" , \"customername\" : \"Emilio Quitzon\" , \"registered\" : \"2023-10-24 19:20:35.638\" } You will have a final opportunity to review and confirm the event source configuration. Click Configure [A] to finalize the configuration of the New Customers event source.","title":"Configuring an event source for New Customers"},{"location":"3/#_3","text":"","title":""},{"location":"3/#filtering-event-sources-to-match-specific-criteria","text":"With the event source fully configured within EP, you can now begin tailoring the event processing flow to filter on orders where the value exceeds $100 USD. Scroll down within the list of nodes and actions on the left-side interface until you locate the Filter node. Click and hold the Filter [A] node, then drag it to the authoring canvas on the right Position it to the right side of the Orders node The output terminal for Orders needs to be connected as input to the filter_1 node. With your mouse pointer, hover over the Orders node and take note of the grey dot (labelled Output Port [B] ) at the right edge of the tile Click and hold the Output Port (right-most) of Orders and drag it to the Input Port [C] (left-most edge) of filter_1 , then release your hold to complete the circuit With the Orders event source node connected to the filter_1 operation node, you must utilize the Filter node's expression builder to remove orders that are less than $100 USD in value. This will allow the marketing team's application to target only first-time customers with spending patterns of $100 USD or more. Hover over the filter_1 node with your mouse pointer and click the Edit [D] (pencil) icon. Within the Details tab, specify the value of Node name as FilterLargeOrders When ready, click Next . The Define filter tab will load next. To the right of the Filter expression field, click the Assistant [A] drop-down menu to expose additional options. Event Processing's expression builder will guide you in defining the filter. Property to filter on: price Condition: Is greater than Value: 100 Click the Add to expression button The expression builder will automatically translate your statements into a syntactically-correct filter statement: 'price' > 100 Click the Configure [B] button at the bottom of the panel to finalize the filter configuration","title":"Filtering event sources to match specific criteria"},{"location":"3/#_4","text":"","title":""},{"location":"3/#performing-a-join-across-two-event-sources","text":"The marketing team needs to \"JOIN\" the filtered Order stream data to records of New Customers in order to correctly identify first-time purchasers that placed an order for over $100 USD in the last 24 hours. Use EP's authoring canvas to carry out the necessary JOIN operation. Back on the canvas for NewCustomerLargeOrders , scroll down along the left-hand interface until you locate the Interval Join [A] node. Drag and drop this node [B] at the far right-hand edge of the authoring canvas. With your cursor, hover over the New Customers output terminal and locate the Output Port (grey) button. Click and hold the New Customers Output Port [A] edge and drag this to the Input Port [B] edge of the newly-created intervalJoin_1 node Release your cursor to finalize the connection Repeat the process, this time connecting the FilterLargeOrders node's Output Port [A] to the intervalJoin_1 node's Input Port [B] . At this stage, the input of intervalJoin_1 should be connected to the outputs of both the FilterLargeOrders and New Customers nodes. The JOIN node must now be configured to correlate events based on a shared customerid field within the two event streams ( New Customers and pre-filtered Orders ). With your cursor, hover over the intervalJoin_1 node and click the Edit (pencil) icon. Within the Details tab, set Node name [A] equal to DetectNewCustomerLargeOrders When ready, click Next [B] . Within the Join conditions tab, activate the expression builder by click the Assistant [A] drop-down menu. Set the following properties: Specify property from 'New Customers': customerid Specify property from 'FilterLargeOrders': customerid When ready, click Add to expression [B] The syntactically-correct JOIN expression will be expressed under the Define events field: 'New Customers'.'customerid' = 'FilterLargeOrders'.'customerid' After you have reviewed the JOIN condition, click Next [C] TRIGGERING EVENTS AND DETECTED EVENTS Two concepts are important to understand at this time: Triggering events and Detected events . Since the New Customer sign-up event must logically occur before a purchase, it is considered a triggering event Therefore, when a signal for an Order purchase of over $100 (the detected event ) is received, the JOIN logic condition will be triggered for the two event streams Under the Time window condition , you can define the time interval \"window\" where detected events are considered viable for a promotional offer from the marketing team. To meet the criteria, a purchase of over $100 USD must be made by a first-time customer within a 24 hour window of creating an account. Look for the Event to detect [A] field and select the FilterLargeOrders (event_time) option from the drop-down menu. A Preview visualization of the time window interval is rendered within the panel. For now, with no adjustments made, this will appear as a vertical bar at 0 (X-axis). Scroll down further to reveal additional attributes to edit. Event to set the time window: New Customers (event-time) Offset from event to start the time window: 0 HOUR(S) Offset from event to end the time window [B] : 24 HOUR(S) When ready, click Next [C] The JOIN node's output going forward will be a combination of fields from both the filtered Orders and New Customers events streams. Before finalizing the interval join, you need to clean up the output so that duplicate fields (like customerid and event_time ) are not included from the JOIN operation. You have the option of renaming or removing the duplicate fields \u2014 but for the sake of this demonstration, you will be removing the fields. Within the Output properties tab, locate the customerid [A] field. There are two (duplicate) copies of the same field at this time. Your objective will be to delete ONE of the two duplicate records, where the Source is labelled as New Customers . Preserve the record where Source is labelled as FilterLargeOrders . ONLY REMOVE DUPLICATES WHERE SOURCE == New Customers When reviewing Output properties for duplicate fields, take note of the table's Source column. Duplicates will be produced by both the New Customers and FilterLargeOrders nodes. Only remove duplicates where the Source is listed as New Customers . Click the round \u2014 sign to the left of the first customerid row, where the Source is labelled as \"New Customers\", to remove the duplicate. The result will be only one copy of the customerid field in the table. The red text ( Must be unique ) underlining the field will disappear from the table. Repeat the procedure for the event_time field, which is also duplicated by the JOIN: Delete the event_time row where the Source column is labelled as \"New Customers\". With all duplicates now removed from the table, the Configure [A] button will now be highlighted in blue and selectable. Click the Configure button to finalize the interval join configuration.","title":"Performing a JOIN across two event sources"},{"location":"3/#_5","text":"","title":""},{"location":"3/#next-steps","text":"With filtering and processing steps now in place for incoming event streams, Focus Corporation's marketing team is able to detect valid promotional opportunities and emit those detected events to their customer loyalty application. In the following section, you will create those linkages and validate its effectiveness using \"historical\" and \"live\" data feeds.","title":"Next Steps"},{"location":"4/","text":"Connecting real-time events to a promotional marketing application WATCH THIS DEMONSTRATION Christopher Bienko (Principal, Learning Content Development, IBM Application Modernization) provides a hands-on demonstration of the module contents. Additional ways to watch: Seismic replay available for download. [4 minutes] It's finally time to unify the hard work of Focus Corporation's integration and marketing teams. With a rich feed of filtered and processed event streams, which joins together New Customers and Orders data feeds, the marketing department can begin supplying their customer loyalty application with input. When new events are detected which match the marketing team's promotional criteria, the application will automatically trigger an upsell activity for the new customer. Return to the authoring canvas dashboard for NewCustomerLargeOrders within the IBM Event Processing (EP) tab. From the list of nodes along the left-side interface, drag and drop the Event destination [A] node into the canvas. Place it far to the right of the other nodes within the table [B] . You can use your mouse wheel to resize the table if necessary. The new node sink_1 represents a \"resource\", a concept from Apache Kafka. These resources can receive incoming events \u2014 thereby serving as a destination . With your cursor, hover over the DetectNewCustomerLargeOrders node that was created in the previous module. Click and drag the node's Output Port edge, connecting it to the Input Port edge of the sink_1 node Release the mouse button to establish the connection between the two nodes Hover over the sink_1 node and click the Edit [A] (pencil) icon to configure the destination. Within the Details tab, adjust the following variables: Node name [A] : OutputToMarketingApp Server [B] : ademo-es-kafka-bootstrap.cp4i.svc:9095 The server has been pre-configured as part of the hands-on lab Setup module, generating the Apache Kafka events that will simulate both \"historical\" and \"live\" customer interactions with Focus' online storefront When ready, click Next [C] Select the Accept certificates [B] toggle when asked if you wish to trust all security certificates issued by the event stream source. When ready, click Next [C] . Within the Access credentials tab, modify the following variables: Username [A] : es-admin Password [B] : the password unique to your Event Streams instance When ready, click Next [C] Within the Topic selection tab, you will be asked to specify which of the available endpoints (topics available to the credentials you provided in Step 7) should serve as a destination \"sink.\" In this situation, the goal is to send all valid customer interactions (which meet the purchasing criteria) to the marketing department's customer loyalty application. Select LOYALTY.APP [A] Click Configure [B] to finalize the event destination You are now ready to begin testing the end-to-end flow using historical event data. This will allow the marketing and integration teams to see the results of the flow and validate the result. If successful, several new customers who are eligible for the promotional discount should be detected and flagged within the loyalty application. Return to the authoring canvas dashboard for NewCustomerLargeOrders within EP. In the top-right corner of the interface, click the Run drop-down menu From the list of options, select Include historical [A] Detected events will begin automatically populating the Output Events panel, along the bottom of the page. It will take a few moments for the events to begin populating the table Continue to verify the detected events until satisfied with the historical event data results When ready, stop the flow by clicking the Stop button in the top-right corner of the page UNABLE TO CONNECT TO RESULTS You may experience a pop-up window in the top-right corner of the authoring canvas after initiating a Run execution. This is expected given that not all of the Kafka topic endpoints were configured \u2014 you only configured the LOYALTY.APP endpoint, as well as the ORDERS and CUSTOMERS event streams. You can safely ignore this warning and click the X icon to dismiss it. Having successfully tested the flow, the marketing team is ready to reconfigure the event destination sink, allowing them to supply live event stream data into Focus' production environment. The event flow processing can then easily be put into production and immediately begin generating actionable promotions within Focus Corporation's customer loyalty application. Congratulations on a successful demonstration! This concludes the hands-on material. At this time, you should be ready to record a Stand & Deliver presentation or complete the Level 3 accreditation quiz \u2014 follow the Evaluation steps appropriate to your job role.","title":"4. Connecting real-time events to a promotional marketing app"},{"location":"4/#connecting-real-time-events-to-a-promotional-marketing-application","text":"WATCH THIS DEMONSTRATION Christopher Bienko (Principal, Learning Content Development, IBM Application Modernization) provides a hands-on demonstration of the module contents. Additional ways to watch: Seismic replay available for download. [4 minutes] It's finally time to unify the hard work of Focus Corporation's integration and marketing teams. With a rich feed of filtered and processed event streams, which joins together New Customers and Orders data feeds, the marketing department can begin supplying their customer loyalty application with input. When new events are detected which match the marketing team's promotional criteria, the application will automatically trigger an upsell activity for the new customer. Return to the authoring canvas dashboard for NewCustomerLargeOrders within the IBM Event Processing (EP) tab. From the list of nodes along the left-side interface, drag and drop the Event destination [A] node into the canvas. Place it far to the right of the other nodes within the table [B] . You can use your mouse wheel to resize the table if necessary. The new node sink_1 represents a \"resource\", a concept from Apache Kafka. These resources can receive incoming events \u2014 thereby serving as a destination . With your cursor, hover over the DetectNewCustomerLargeOrders node that was created in the previous module. Click and drag the node's Output Port edge, connecting it to the Input Port edge of the sink_1 node Release the mouse button to establish the connection between the two nodes Hover over the sink_1 node and click the Edit [A] (pencil) icon to configure the destination. Within the Details tab, adjust the following variables: Node name [A] : OutputToMarketingApp Server [B] : ademo-es-kafka-bootstrap.cp4i.svc:9095 The server has been pre-configured as part of the hands-on lab Setup module, generating the Apache Kafka events that will simulate both \"historical\" and \"live\" customer interactions with Focus' online storefront When ready, click Next [C] Select the Accept certificates [B] toggle when asked if you wish to trust all security certificates issued by the event stream source. When ready, click Next [C] . Within the Access credentials tab, modify the following variables: Username [A] : es-admin Password [B] : the password unique to your Event Streams instance When ready, click Next [C] Within the Topic selection tab, you will be asked to specify which of the available endpoints (topics available to the credentials you provided in Step 7) should serve as a destination \"sink.\" In this situation, the goal is to send all valid customer interactions (which meet the purchasing criteria) to the marketing department's customer loyalty application. Select LOYALTY.APP [A] Click Configure [B] to finalize the event destination You are now ready to begin testing the end-to-end flow using historical event data. This will allow the marketing and integration teams to see the results of the flow and validate the result. If successful, several new customers who are eligible for the promotional discount should be detected and flagged within the loyalty application. Return to the authoring canvas dashboard for NewCustomerLargeOrders within EP. In the top-right corner of the interface, click the Run drop-down menu From the list of options, select Include historical [A] Detected events will begin automatically populating the Output Events panel, along the bottom of the page. It will take a few moments for the events to begin populating the table Continue to verify the detected events until satisfied with the historical event data results When ready, stop the flow by clicking the Stop button in the top-right corner of the page UNABLE TO CONNECT TO RESULTS You may experience a pop-up window in the top-right corner of the authoring canvas after initiating a Run execution. This is expected given that not all of the Kafka topic endpoints were configured \u2014 you only configured the LOYALTY.APP endpoint, as well as the ORDERS and CUSTOMERS event streams. You can safely ignore this warning and click the X icon to dismiss it. Having successfully tested the flow, the marketing team is ready to reconfigure the event destination sink, allowing them to supply live event stream data into Focus' production environment. The event flow processing can then easily be put into production and immediately begin generating actionable promotions within Focus Corporation's customer loyalty application. Congratulations on a successful demonstration! This concludes the hands-on material. At this time, you should be ready to record a Stand & Deliver presentation or complete the Level 3 accreditation quiz \u2014 follow the Evaluation steps appropriate to your job role.","title":"Connecting real-time events to a promotional marketing application"},{"location":"evaluation/","text":"Evaluation Criteria for IBM Technical Sellers and Business Partners To receive a Level 3 accreditation ( IBM Event Automation for Technical Sales Intermediate ), IBMers and Business Partners must demonstrate mastery of the skills learned throughout the various modules of these hands-on labs and coursework. Level 3 accreditation requirements\u2014 and the way participants will be evaluated before receiving accreditation \u2014differs depending on job role. IBM TECHNICAL SELLERS IBM technical sellers must develop and record a Stand & Deliver presentation, which will be uploaded to the IBM Stand & Deliver platform for evaluation. This video is intended to simulate your delivery of a \u201clive\u201d demo in front of a client \u2014 on camera. IBMers will have flexibility in defining a hypothetical client, the pain points that customer has, and the goals they aspire to. The recording will then cover the seller\u2019s hands-on demonstration and pitch to the client of the value of the IBM solution using the environments and techniques of this lab. BUSINESS PARTNERS Business partners must pass an accreditation quiz after completing the hands-on portion of the course. The quiz consists of multiple choice questions, with four possible responses (and only one correct answer) for each question. The quiz questions will ask you about on-screen text or descriptions that come up as you work through the lab guide. Answers to quiz questions can only be determined by carefully following the instructions of the hands-on lab. You will have three attempts in total to obtain a passing grade. Take note that any materials marked as OPTIONAL are not evaluated for Level 3 accreditation. IBM technical sellers are not required to include optional sections in their Stand & Deliver Business partners are not tested on the contents of optional sections IBMer Stand & Deliver Assessment IBMers \u2014 SUBMIT RECORDINGS HERE Submit your Stand & Deliver recording online using IBM YourLearning: https://yourlearning.ibm.com/activity/ISND-salesenablement-2310181600 The evaluation criteria described below only applies to IBMers , who must record a Stand & Deliver to receive accreditation for this Level 3 course. By default, IBMers will be evaluated by their First Line Manager (FLM) \u2014 although they may request another manager to evaluate their Stand & Deliver, if appropriate. Instructions on how to submit a Stand & Deliver are included below. Read these carefully before starting your Stand & Deliver recording. IBM Technical Sellers need to include all seven of the following elements in their Stand & Deliver recording to receive Level 3 accreditation: Seller articulated their client's pain point(s) and the value proposition of using IBM Event Automation and IBM Cloud Pak for Integration . Seller highlighted use cases for IBM Event Automation . It is recommended that the seller address the challenges of real-time access for meeting customer demands and taking action in the moment \u2014 and how event-led integration solutions from IBM can overcome these obstacles. Seller must demonstrate how to take existing IBM MQ message queues and publish those as Kafka-based event streams using IBM Cloud Pak for Integration and IBM Event Streams . Seller must utilize IBM Event Endpoint Management to import the newly-created event streams topics (3.) and show how the service provides a self-service catalog to view and manage those topics. Seller must show how IBM Event Processing can correlate any newly-created customer accounts with first-time purchase orders that total over $100 USD in value. They should include in this demonstration an explanation about how to define event sources, destinations, filters, and JOIN interval operations. Seller must connect historical data to the LOYALTY.APP application endpoint to validate the end-to-end configurations made so far and simulate how \"live\" event data is being processed by the IBM Event Automation suite. Sellers should use the Run > Include Historical test execution inside IBM Event Processing to demonstrate this. Seller closed the demonstration with a call to action for their client that could include: a workshop, a deeper dive into the product meeting, or Proof of Experience (PoX) engagement. Business Partner Quiz Assessment PARTNERS \u2014 COMPLETE ASSESSMENT HERE Complete your assessment online using IBM Training: https://learn.ibm.com/mod/subcourse/view.php?id=284786 The accreditation quiz consists of multiple choice questions, with four possible responses (and only one correct answer) for each question. The quiz questions will ask you about on-screen text or descriptions that come up as you work through the lab guide. Answers to the quiz can only be determined by carefully following the instructions of the hands-on lab. Sections of the lab marked as OPTIONAL will not be tested on as part of the quiz. You will have three attempts in total to obtain a passing grade. Next Steps In the following module, you will create an event stream from a message queue using IBM MQ .","title":"Evaluation"},{"location":"evaluation/#evaluation-criteria-for-ibm-technical-sellers-and-business-partners","text":"To receive a Level 3 accreditation ( IBM Event Automation for Technical Sales Intermediate ), IBMers and Business Partners must demonstrate mastery of the skills learned throughout the various modules of these hands-on labs and coursework. Level 3 accreditation requirements\u2014 and the way participants will be evaluated before receiving accreditation \u2014differs depending on job role. IBM TECHNICAL SELLERS IBM technical sellers must develop and record a Stand & Deliver presentation, which will be uploaded to the IBM Stand & Deliver platform for evaluation. This video is intended to simulate your delivery of a \u201clive\u201d demo in front of a client \u2014 on camera. IBMers will have flexibility in defining a hypothetical client, the pain points that customer has, and the goals they aspire to. The recording will then cover the seller\u2019s hands-on demonstration and pitch to the client of the value of the IBM solution using the environments and techniques of this lab. BUSINESS PARTNERS Business partners must pass an accreditation quiz after completing the hands-on portion of the course. The quiz consists of multiple choice questions, with four possible responses (and only one correct answer) for each question. The quiz questions will ask you about on-screen text or descriptions that come up as you work through the lab guide. Answers to quiz questions can only be determined by carefully following the instructions of the hands-on lab. You will have three attempts in total to obtain a passing grade. Take note that any materials marked as OPTIONAL are not evaluated for Level 3 accreditation. IBM technical sellers are not required to include optional sections in their Stand & Deliver Business partners are not tested on the contents of optional sections","title":"Evaluation Criteria for IBM Technical Sellers and Business Partners"},{"location":"evaluation/#_1","text":"","title":""},{"location":"evaluation/#ibmer-stand-deliver-assessment","text":"IBMers \u2014 SUBMIT RECORDINGS HERE Submit your Stand & Deliver recording online using IBM YourLearning: https://yourlearning.ibm.com/activity/ISND-salesenablement-2310181600 The evaluation criteria described below only applies to IBMers , who must record a Stand & Deliver to receive accreditation for this Level 3 course. By default, IBMers will be evaluated by their First Line Manager (FLM) \u2014 although they may request another manager to evaluate their Stand & Deliver, if appropriate. Instructions on how to submit a Stand & Deliver are included below. Read these carefully before starting your Stand & Deliver recording. IBM Technical Sellers need to include all seven of the following elements in their Stand & Deliver recording to receive Level 3 accreditation: Seller articulated their client's pain point(s) and the value proposition of using IBM Event Automation and IBM Cloud Pak for Integration . Seller highlighted use cases for IBM Event Automation . It is recommended that the seller address the challenges of real-time access for meeting customer demands and taking action in the moment \u2014 and how event-led integration solutions from IBM can overcome these obstacles. Seller must demonstrate how to take existing IBM MQ message queues and publish those as Kafka-based event streams using IBM Cloud Pak for Integration and IBM Event Streams . Seller must utilize IBM Event Endpoint Management to import the newly-created event streams topics (3.) and show how the service provides a self-service catalog to view and manage those topics. Seller must show how IBM Event Processing can correlate any newly-created customer accounts with first-time purchase orders that total over $100 USD in value. They should include in this demonstration an explanation about how to define event sources, destinations, filters, and JOIN interval operations. Seller must connect historical data to the LOYALTY.APP application endpoint to validate the end-to-end configurations made so far and simulate how \"live\" event data is being processed by the IBM Event Automation suite. Sellers should use the Run > Include Historical test execution inside IBM Event Processing to demonstrate this. Seller closed the demonstration with a call to action for their client that could include: a workshop, a deeper dive into the product meeting, or Proof of Experience (PoX) engagement.","title":"IBMer Stand &amp; Deliver Assessment"},{"location":"evaluation/#_2","text":"","title":""},{"location":"evaluation/#business-partner-quiz-assessment","text":"PARTNERS \u2014 COMPLETE ASSESSMENT HERE Complete your assessment online using IBM Training: https://learn.ibm.com/mod/subcourse/view.php?id=284786 The accreditation quiz consists of multiple choice questions, with four possible responses (and only one correct answer) for each question. The quiz questions will ask you about on-screen text or descriptions that come up as you work through the lab guide. Answers to the quiz can only be determined by carefully following the instructions of the hands-on lab. Sections of the lab marked as OPTIONAL will not be tested on as part of the quiz. You will have three attempts in total to obtain a passing grade.","title":"Business Partner Quiz Assessment"},{"location":"evaluation/#_3","text":"","title":""},{"location":"evaluation/#next-steps","text":"In the following module, you will create an event stream from a message queue using IBM MQ .","title":"Next Steps"},{"location":"setup/","text":"Prerequisites and Setup SETUP TIME ESTIMATES The following section will take approximately 90-120 minutes to complete. Much of the setup work is fully automated and will allow you to perform other tasks or errands while it is underway. Budget your time accordingly. SUPPORT AND TROUBLESHOOTING If you require assistance or run into issues with the hands-on lab, help is available. Environment issues : The lab environment is managed by IBM Technology Zone. Opening a support case ticket is recommended for issues related to the hands-on environment (provisioning, running, and so on.) Documentation issues : If there is an error in the lab documentation, or if you require additional support in completing the material, open a thread on the #l3-support-app-modernization Slack channel. The hands-on material covered within this lab will focus on event-led integration solutions, specifically the IBM Event Automation suite \u2014 as well as elements of the IBM Cloud Pak for Integration (MQ-Kafka connector.) IBM Event Automation provides an open, composable set of capabilities that interoperate with the services and tools that are already part of a client's ecosystem. A fundamental component to IBM Event Automation's architecture is composability, which will be highlighted through the lab as you incrementally add more capabilities and elements to the demo environment. Event Endpoint Management : to facilitate secured exposure of event streams and unite all event streams (including MQ and Kafka) into a single, comprehensive view. Focus' marketing department can discover, subscribe to, and generate security credentials for colleagues using the self-service catalog. Manual processes will be automated and onboarding of users can be expedited. Event Processing : a no-code editor for allowing developers (of any skill level) to construct processing flows, detect situations across multiple events inside a configurable time-window, and validate flow behavior prior to deploying into production (using historical data.) Event Streams + Kafka-MQ connector : part of the IBM Cloud Pak for Integration suite, this connector allows IBM MQ messages to be published into a Kafka-compatible event stream. It provides assurances that all MQ messages are delivered once-and-only-once, without requiring complex or custom code. Environment templates (images) have been defined ahead of time with the assistance of IBM Technology Zone (ITZ), which can be reserved for learning and client demonstration purposes at no-charge. Some further configuration of these environments will be required after deployment. Request an IBM Technology Zone Environment You will require access to the ITZ in order to reserve your environment and complete the lab. If you do not yet have access or an account with the ITZ, you will need to register for one . The hands-on environment\u2014 a combination of services from IBM Cloud Pak for Integration (CPI) and IBM Event Automation (EA) \u2014will be running entirely atop of Red Hat OpenShift , which will orchestrate containerized deployments of CPI and EA across the OpenShift cluster. Reserve a Red Hat OpenShift cluster via the IBM Technology Zone pre-defined template. URL: https://techzone.ibm.com/my/reservations/create/63a3a25a3a4689001740dbb3 From the Single environment reservation options list, select Reserve now [A] . Supply additional details about your ITZ reservation request: Name [A] : Give your reservation a unique name. Purpose [B] : Set to Practice / Self-Education and affirm that customer data will not be used with the environment. If you are replicating this hands-on demonstration with a client, you must select Customer Demo and supply a sales opportunity number. Purpose Description [C] : Provide a brief summary of how the environment will be used. Preferred Geography [D] : Select the data center region that is closest to your location. End Date & Time [E] : Select a time and date for when the reservation will expire. The recommended amount is 2 days, although it is possible to finish the hands-on demonstration within a few hours. Additional time extensions are available. OCP/Kubernetes Cluster Network [F] : Default value of 10.128.0.0/14 Enable FIPS Security : Default value of No OpenShift Version [G] : Set to 4.14 Storage [H] : Set to ODF - 2 TB OCP/Kubernetes Service Network [I] : Default value of 172.30.0.0/16 Worker Node Count : Default value of 3 Worker Node Flavor [J] : Set to 16 vCPU x 64 GB - 100 GB ephemeral storage When satisfied, verify that you agree to the Terms and Conditions for the environment and finalize your reservation request by clicking Submit [K] . PROVISIONING TIMES Reservations take approximately 60 minutes to complete from the time that you click submit. If you navigate to the My Reservations tab of the ITZ, you can monitor the progress of your reservation. Wait for the ITZ reservation to be marked as \" Ready \" before attempting to start the lab \u2014 accessing it too soon will lead to issues (or an outright failure) when connecting to the OpenShift cluster. When the ITZ environment is ready to go, you will receive an email to your inbox (similar to the screenshot below). Click the blue View My Reservations [A] at the bottom of the email to open a page with more details about the environment. Alternatively, you can click the My Reservations tab from the ITZ home page to drill down into your environment's details. Details about your OpenShift cluster, including login and authentication details, are summarized on the page. Record the Username [A] ( kubeadmin ) and Password [B] (unique to your environment), as well as the Desktop URL just above those two items, to a notepad for reference later Click the blue Open your IBM Cloud environment [C] icon to access OpenShift's dashboard THIS CONNECTION IS NOT PRIVATE Depending on your web browser, you may receive a warning about This connection is not private or a similarly phrased message. Disregard these warnings and continue to the intended destination by clicking \"visit this website.\" You will be prompted to provide the Username and Password recorded in Step 5 . Click Log in [A] to proceed Once authenticated, the web browser will redirect to the main dashboard for the OpenShift Container Platform web interface You are now ready to proceed with the lab environment customization for IBM Event Automation and IBM Cloud Pak for Integration Customization of the Lab Environment In order to deploy containerized instances of IBM Event Automation and IBM Cloud Pak for Integration on top of OpenShift, you must first download and set up your local machine with the OpenShift Command Line Interface (CLI) . In the top-right corner of the OpenShift dashboard, click the question mark [A] ( ? ) icon as shown in the screenshot below. This will open a drop-down menu with several options. Click the Command line tools [B] option from the drop-down menu. COMMAND LINE ACCESS For parts of the hands-on lab, you will be required to establish connections over SSH to remote infrastructure endpoints. It is recommended that users do so via Terminal (Mac) or PuTTY (Windows). For detailed instructions on how to connect with PuTTY, or if you are using Windows, reference the linked material . From the oc - OpenShift Command Line Interface (CLI) options, click the download link that matches your local machine's operating system (Mac, Windows, or Linux). Once downloaded, execute the installer and proceed with the lab guide once oc is installed. Return to the OpenShift dashboard landing page. In the far right-hand corner of the interface, click the kube:admin button to open another drop-down menu From the options, select Copy login command [A] to open a new tab or window with details about remotely accessing the OpenShift cluster A mostly-blank page will load, with Display Token written in blue text in the top-left of the page. Click this link to proceed. Details about a newly-generated API token will load on screen. Record the full command listed under Log in with this token [A] (resembling something similar to oc login --token=sha256... ) and save this to your notepad for later reference. Open your command line tool of preference on your local machine, likely either Terminal or PuTTY . At this stage, the OpenShift CLI ( oc ) should be available on your machine and ready for remotely connecting to the OpenShift cluster. Within the Terminal, paste the full login command recorded in Step 11 and hit Return to commence the login. A prompt similar to the screenshot below will be displayed momentarily, indicating that the Terminal is now successfully connected (remotely) to the OpenShift cluster. To deploy containerized instances of IBM Event Automation and IBM Cloud Pak for Integration to the OpenShift cluster, you must first download those container images locally and then deploy them remotely on the OpenShift cluster. Execute the following command within your Terminal console to clone the necessary scripts and images to your local machine : git clone https://github.com/bienko/platinum-demo-code-event-automation.git Navigate to the newly cloned platinum-demo-code-event-automation repository with the following command: cd platinum-demo-code-event-automation Execute the following command to kick off the process of deploying and configuring the containerized services to the OpenShift cluster : ./deploy.sh Wait for the full deployment to complete before continuing with the lab documentation. When ready, each of the lab services\u2014 IBM Cloud Pak for Integration [A] (\"Platform Navigator\"), Event Streams [B] , Event Endpoint Management [C] , Event Processing [D] , and IBM MQ [E] \u2014will be listed to the Terminal window. Each listing will be accompanied by a URL , username , and password . Record this information to a notepad for reference later. DEPLOYMENT TIMES The deployment of IBM Event Automation and IBM Cloud Pak for Integration containers to the OpenShift cluster is entirely automated using Red Hat Ansible scripts. Full deployment and configuration should take approximately 20-45 minutes to complete. The Platform Navigator and IBM MQ services utilize the same login details. The password supplied for these two services by the Terminal console in Step 15 is a temporary , one-time-use password. Try logging into the IBM MQ environment environment by pasting the URL into a web browser. Then supply the username and temporary password recorded within your notepad. When prompted, create your own password for logging into the environment. Confirm your selection with Submit [A] and then update your password records for both the Platform Navigator and IBM MQ services. At this stage, the lab environment is fully configured and ready for hands-on work. With your web browser, open a new tab for each of the services needed for the hands-on lab: IBM Cloud Pak for Integration (CPI) Event Streams Event Endpoint Management Event Processing IBM MQ MQ & CPI AUTHENTICATION Both services utilize the same username and password credentials. However, the two URLs generated by the Terminal console in Step 15 are unique . Ensure that you are logging in to the correct dashboard (IBM MQ or CPI) when asked to do so in the subsequent chapters. Keep these tabs open. You will be switching between them frequently. Next Steps The following module will outline the Level 3 evaluation criteria for IBM technical sellers and business partners. Follow the accreditation steps appropriate to your job role.","title":"Setup"},{"location":"setup/#prerequisites-and-setup","text":"SETUP TIME ESTIMATES The following section will take approximately 90-120 minutes to complete. Much of the setup work is fully automated and will allow you to perform other tasks or errands while it is underway. Budget your time accordingly. SUPPORT AND TROUBLESHOOTING If you require assistance or run into issues with the hands-on lab, help is available. Environment issues : The lab environment is managed by IBM Technology Zone. Opening a support case ticket is recommended for issues related to the hands-on environment (provisioning, running, and so on.) Documentation issues : If there is an error in the lab documentation, or if you require additional support in completing the material, open a thread on the #l3-support-app-modernization Slack channel. The hands-on material covered within this lab will focus on event-led integration solutions, specifically the IBM Event Automation suite \u2014 as well as elements of the IBM Cloud Pak for Integration (MQ-Kafka connector.) IBM Event Automation provides an open, composable set of capabilities that interoperate with the services and tools that are already part of a client's ecosystem. A fundamental component to IBM Event Automation's architecture is composability, which will be highlighted through the lab as you incrementally add more capabilities and elements to the demo environment. Event Endpoint Management : to facilitate secured exposure of event streams and unite all event streams (including MQ and Kafka) into a single, comprehensive view. Focus' marketing department can discover, subscribe to, and generate security credentials for colleagues using the self-service catalog. Manual processes will be automated and onboarding of users can be expedited. Event Processing : a no-code editor for allowing developers (of any skill level) to construct processing flows, detect situations across multiple events inside a configurable time-window, and validate flow behavior prior to deploying into production (using historical data.) Event Streams + Kafka-MQ connector : part of the IBM Cloud Pak for Integration suite, this connector allows IBM MQ messages to be published into a Kafka-compatible event stream. It provides assurances that all MQ messages are delivered once-and-only-once, without requiring complex or custom code. Environment templates (images) have been defined ahead of time with the assistance of IBM Technology Zone (ITZ), which can be reserved for learning and client demonstration purposes at no-charge. Some further configuration of these environments will be required after deployment.","title":"Prerequisites and Setup"},{"location":"setup/#_1","text":"","title":""},{"location":"setup/#request-an-ibm-technology-zone-environment","text":"You will require access to the ITZ in order to reserve your environment and complete the lab. If you do not yet have access or an account with the ITZ, you will need to register for one . The hands-on environment\u2014 a combination of services from IBM Cloud Pak for Integration (CPI) and IBM Event Automation (EA) \u2014will be running entirely atop of Red Hat OpenShift , which will orchestrate containerized deployments of CPI and EA across the OpenShift cluster. Reserve a Red Hat OpenShift cluster via the IBM Technology Zone pre-defined template. URL: https://techzone.ibm.com/my/reservations/create/63a3a25a3a4689001740dbb3 From the Single environment reservation options list, select Reserve now [A] . Supply additional details about your ITZ reservation request: Name [A] : Give your reservation a unique name. Purpose [B] : Set to Practice / Self-Education and affirm that customer data will not be used with the environment. If you are replicating this hands-on demonstration with a client, you must select Customer Demo and supply a sales opportunity number. Purpose Description [C] : Provide a brief summary of how the environment will be used. Preferred Geography [D] : Select the data center region that is closest to your location. End Date & Time [E] : Select a time and date for when the reservation will expire. The recommended amount is 2 days, although it is possible to finish the hands-on demonstration within a few hours. Additional time extensions are available. OCP/Kubernetes Cluster Network [F] : Default value of 10.128.0.0/14 Enable FIPS Security : Default value of No OpenShift Version [G] : Set to 4.14 Storage [H] : Set to ODF - 2 TB OCP/Kubernetes Service Network [I] : Default value of 172.30.0.0/16 Worker Node Count : Default value of 3 Worker Node Flavor [J] : Set to 16 vCPU x 64 GB - 100 GB ephemeral storage When satisfied, verify that you agree to the Terms and Conditions for the environment and finalize your reservation request by clicking Submit [K] . PROVISIONING TIMES Reservations take approximately 60 minutes to complete from the time that you click submit. If you navigate to the My Reservations tab of the ITZ, you can monitor the progress of your reservation. Wait for the ITZ reservation to be marked as \" Ready \" before attempting to start the lab \u2014 accessing it too soon will lead to issues (or an outright failure) when connecting to the OpenShift cluster. When the ITZ environment is ready to go, you will receive an email to your inbox (similar to the screenshot below). Click the blue View My Reservations [A] at the bottom of the email to open a page with more details about the environment. Alternatively, you can click the My Reservations tab from the ITZ home page to drill down into your environment's details. Details about your OpenShift cluster, including login and authentication details, are summarized on the page. Record the Username [A] ( kubeadmin ) and Password [B] (unique to your environment), as well as the Desktop URL just above those two items, to a notepad for reference later Click the blue Open your IBM Cloud environment [C] icon to access OpenShift's dashboard THIS CONNECTION IS NOT PRIVATE Depending on your web browser, you may receive a warning about This connection is not private or a similarly phrased message. Disregard these warnings and continue to the intended destination by clicking \"visit this website.\" You will be prompted to provide the Username and Password recorded in Step 5 . Click Log in [A] to proceed Once authenticated, the web browser will redirect to the main dashboard for the OpenShift Container Platform web interface You are now ready to proceed with the lab environment customization for IBM Event Automation and IBM Cloud Pak for Integration","title":"Request an IBM Technology Zone Environment"},{"location":"setup/#_2","text":"","title":""},{"location":"setup/#customization-of-the-lab-environment","text":"In order to deploy containerized instances of IBM Event Automation and IBM Cloud Pak for Integration on top of OpenShift, you must first download and set up your local machine with the OpenShift Command Line Interface (CLI) . In the top-right corner of the OpenShift dashboard, click the question mark [A] ( ? ) icon as shown in the screenshot below. This will open a drop-down menu with several options. Click the Command line tools [B] option from the drop-down menu. COMMAND LINE ACCESS For parts of the hands-on lab, you will be required to establish connections over SSH to remote infrastructure endpoints. It is recommended that users do so via Terminal (Mac) or PuTTY (Windows). For detailed instructions on how to connect with PuTTY, or if you are using Windows, reference the linked material . From the oc - OpenShift Command Line Interface (CLI) options, click the download link that matches your local machine's operating system (Mac, Windows, or Linux). Once downloaded, execute the installer and proceed with the lab guide once oc is installed. Return to the OpenShift dashboard landing page. In the far right-hand corner of the interface, click the kube:admin button to open another drop-down menu From the options, select Copy login command [A] to open a new tab or window with details about remotely accessing the OpenShift cluster A mostly-blank page will load, with Display Token written in blue text in the top-left of the page. Click this link to proceed. Details about a newly-generated API token will load on screen. Record the full command listed under Log in with this token [A] (resembling something similar to oc login --token=sha256... ) and save this to your notepad for later reference. Open your command line tool of preference on your local machine, likely either Terminal or PuTTY . At this stage, the OpenShift CLI ( oc ) should be available on your machine and ready for remotely connecting to the OpenShift cluster. Within the Terminal, paste the full login command recorded in Step 11 and hit Return to commence the login. A prompt similar to the screenshot below will be displayed momentarily, indicating that the Terminal is now successfully connected (remotely) to the OpenShift cluster. To deploy containerized instances of IBM Event Automation and IBM Cloud Pak for Integration to the OpenShift cluster, you must first download those container images locally and then deploy them remotely on the OpenShift cluster. Execute the following command within your Terminal console to clone the necessary scripts and images to your local machine : git clone https://github.com/bienko/platinum-demo-code-event-automation.git Navigate to the newly cloned platinum-demo-code-event-automation repository with the following command: cd platinum-demo-code-event-automation Execute the following command to kick off the process of deploying and configuring the containerized services to the OpenShift cluster : ./deploy.sh Wait for the full deployment to complete before continuing with the lab documentation. When ready, each of the lab services\u2014 IBM Cloud Pak for Integration [A] (\"Platform Navigator\"), Event Streams [B] , Event Endpoint Management [C] , Event Processing [D] , and IBM MQ [E] \u2014will be listed to the Terminal window. Each listing will be accompanied by a URL , username , and password . Record this information to a notepad for reference later. DEPLOYMENT TIMES The deployment of IBM Event Automation and IBM Cloud Pak for Integration containers to the OpenShift cluster is entirely automated using Red Hat Ansible scripts. Full deployment and configuration should take approximately 20-45 minutes to complete. The Platform Navigator and IBM MQ services utilize the same login details. The password supplied for these two services by the Terminal console in Step 15 is a temporary , one-time-use password. Try logging into the IBM MQ environment environment by pasting the URL into a web browser. Then supply the username and temporary password recorded within your notepad. When prompted, create your own password for logging into the environment. Confirm your selection with Submit [A] and then update your password records for both the Platform Navigator and IBM MQ services. At this stage, the lab environment is fully configured and ready for hands-on work. With your web browser, open a new tab for each of the services needed for the hands-on lab: IBM Cloud Pak for Integration (CPI) Event Streams Event Endpoint Management Event Processing IBM MQ MQ & CPI AUTHENTICATION Both services utilize the same username and password credentials. However, the two URLs generated by the Terminal console in Step 15 are unique . Ensure that you are logging in to the correct dashboard (IBM MQ or CPI) when asked to do so in the subsequent chapters. Keep these tabs open. You will be switching between them frequently.","title":"Customization of the Lab Environment"},{"location":"setup/#_3","text":"","title":""},{"location":"setup/#next-steps","text":"The following module will outline the Level 3 evaluation criteria for IBM technical sellers and business partners. Follow the accreditation steps appropriate to your job role.","title":"Next Steps"}]}